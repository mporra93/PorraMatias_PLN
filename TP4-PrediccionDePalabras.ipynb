{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizar otro dataset y poner en práctica la predicción de próxima palabra"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importo librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import io\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /opt/homebrew/Caskroom/miniforge/base/envs/nlp/lib/python3.10/site-packages (1.8.0)\n"
     ]
    }
   ],
   "source": [
    "# torchsummar actualmente tiene un problema con las LSTM, por eso\n",
    "# se utiliza torchinfo, un fork del proyecto original con el bug solucionado\n",
    "!pip3 install torchinfo\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "import os\n",
    "import platform\n",
    "\n",
    "if not os.path.exists('torch_helpers.py'):\n",
    "    if platform.system() == 'Windows':\n",
    "        os.system('curl -o torch_helpers.py https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py')\n",
    "    else:\n",
    "        os.system('curl -o torch_helpers.py https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_helpers import categorical_acc\n",
    "\n",
    "def train(model, train_loader, valid_loader, optimizer, criterion, epochs=100):\n",
    "    # Defino listas para realizar graficas de los resultados\n",
    "    train_loss = []\n",
    "    train_accuracy = []\n",
    "    valid_loss = []\n",
    "    valid_accuracy = []\n",
    "\n",
    "    # Defino mi loop de entrenamiento\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        epoch_train_loss = 0.0\n",
    "        epoch_train_accuracy = 0.0\n",
    "\n",
    "        for train_data, train_target in train_loader:\n",
    "            # Seteo los gradientes en cero ya que, por defecto, PyTorch\n",
    "            # los va acumulando\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(train_data)\n",
    "\n",
    "            # Computo el error de la salida comparando contra las etiquetas\n",
    "            loss = criterion(output, train_target)\n",
    "\n",
    "            # Almaceno el error del batch para luego tener el error promedio de la epoca\n",
    "            epoch_train_loss += loss.item()\n",
    "\n",
    "            # Computo el nuevo set de gradientes a lo largo de toda la red\n",
    "            loss.backward()\n",
    "\n",
    "            # Realizo el paso de optimizacion actualizando los parametros de toda la red\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculo el accuracy del batch\n",
    "            accuracy = categorical_acc(output, train_target)\n",
    "            # Almaceno el accuracy del batch para luego tener el accuracy promedio de la epoca\n",
    "            epoch_train_accuracy += accuracy.item()\n",
    "\n",
    "        # Calculo la media de error para la epoca de entrenamiento.\n",
    "        # La longitud de train_loader es igual a la cantidad de batches dentro de una epoca.\n",
    "        epoch_train_loss = epoch_train_loss / len(train_loader)\n",
    "        train_loss.append(epoch_train_loss)\n",
    "        epoch_train_accuracy = epoch_train_accuracy / len(train_loader)        \n",
    "        train_accuracy.append(epoch_train_accuracy)\n",
    "\n",
    "        # Realizo el paso de validación computando error y accuracy, y\n",
    "        # almacenando los valores para imprimirlos y graficarlos\n",
    "        valid_data, valid_target = iter(valid_loader).__next__()\n",
    "        output = model(valid_data)\n",
    "        \n",
    "        epoch_valid_loss = criterion(output, valid_target).item()\n",
    "        valid_loss.append(epoch_valid_loss)\n",
    "\n",
    "        # Calculo el accuracy de la epoch\n",
    "        epoch_valid_accuracy = categorical_acc(output, valid_target).item()\n",
    "        valid_accuracy.append(epoch_valid_accuracy)\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}/{epochs} - Train loss {epoch_train_loss:.3f} - Train accuracy {epoch_train_accuracy:.3f} - Valid Loss {epoch_valid_loss:.3f} - Valid accuracy {epoch_valid_accuracy:.3f}\")\n",
    "\n",
    "    history = {\n",
    "        \"loss\": train_loss,\n",
    "        \"accuracy\": train_accuracy,\n",
    "        \"val_loss\": valid_loss,\n",
    "        \"val_accuracy\": valid_accuracy,\n",
    "    }\n",
    "    return history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importo Datos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mi idea para este dataset, fue leer un grupo bastante grande de archivos de python (preprocesados y pasados a .txt) con la idea de intentar hacer algo similar a un copilot para ayudar al programador en un caso real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3072\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td># \"y\" (target) se obtiene como cada dato de en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>y = [x * 15 for x in X]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>print(\"datos X:\", X)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>print(\"datos y:\", y)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td># Cada dato X lo transformarmos en una matriz ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3067</th>\n",
       "      <td>loss = model3_criterion(y_hat, test_target).it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3068</th>\n",
       "      <td>print(\"loss:\", loss)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3069</th>\n",
       "      <td>\"\"\"### 4 - ConclusiÃ³n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3070</th>\n",
       "      <td>El resultado alcanzado es bueno pero podrÃ­a m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3071</th>\n",
       "      <td>\"\"\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3012 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "60    # \"y\" (target) se obtiene como cada dato de en...\n",
       "61                              y = [x * 15 for x in X]\n",
       "62                                 print(\"datos X:\", X)\n",
       "63                                 print(\"datos y:\", y)\n",
       "64    # Cada dato X lo transformarmos en una matriz ...\n",
       "...                                                 ...\n",
       "3067  loss = model3_criterion(y_hat, test_target).it...\n",
       "3068                               print(\"loss:\", loss)\n",
       "3069                             \"\"\"### 4 - ConclusiÃ³n\n",
       "3070  El resultado alcanzado es bueno pero podrÃ­a m...\n",
       "3071                                                \"\"\"\n",
       "\n",
       "[3012 rows x 1 columns]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Folder path containing the .txt files\n",
    "folder_path = \"scriptDatasets\"\n",
    "\n",
    "# List to store the lines from the text files\n",
    "lines = []\n",
    "\n",
    "# Iterate over each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"latin-1\") as file:  # Specify the appropriate encoding\n",
    "            for line in file:\n",
    "                # Remove leading/trailing whitespace and append non-empty lines\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    lines.append(line)\n",
    "\n",
    "# Create a DataFrame from the lines\n",
    "df = pd.DataFrame({\"text\": lines})\n",
    "\n",
    "# Remove spaces at the start and end of each line\n",
    "df[\"text\"] = df[\"text\"].str.strip()\n",
    "\n",
    "print(len(df))\n",
    "df[60:]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Proceso los datos, y tomo como entrenamiento un largo de secuencia de 3 palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_helpers import Tokenizer # tool de keras equivalente a ltokenizer de nltk\n",
    "from torch_helpers import text_to_word_sequence # tool de keras equivalente a word_teokenize de nltk\n",
    "from torch_helpers import pad_sequences # tool de keras qye se utilizará para padding\n",
    "\n",
    "# largo de la secuencia, incluye seq input + word output\n",
    "train_len = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import re import numpy as np import matplotlib.pyplot as plt import seaborn as sns import torch import torch.nn.functional as F import torch.nn as nn from torch.utils.data import Dataset, DataLoader # torchsummar actualmente tiene un problema con las LSTM, por eso # se utiliza torchinfo, un fork del proyecto original con el bug solucionado !pip3 install torchinfo from torchinfo import summary import os import platform if os.access(\\'torch_helpers.py\\', os.F_OK) is False: if platform.system() == \\'Windows\\': !curl !wget https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py > torch_helpers.py else: !wget torch_helpers.py https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py def train(model, train_loader, valid_loader, optimizer, criterion, epochs=100): # Defino listas para realizar graficas de los resultados train_loss = [] valid_loss = [] # Defino mi loop de entrenamiento for epoch in range(epochs): epoch_train_loss = 0.0 epoch_train_accuracy = 0.0 for train_data, train_target in train_loader: # Seteo los gradientes en cero ya que, por defecto, PyTorch # los va acumulando optimizer.zero_grad() output = model(train_data) # Computo el error de la salida comparando contra las etiquetas loss = criterion(output, train_target) # Almaceno el error del batch para luego tener el error promedio de la epoca epoch_train_loss += loss.item() # Computo el nuevo set de gradientes a lo largo de toda la red loss.backward() # Realizo el paso de optimizacion actualizando los parametros de toda la red optimizer.step() # Calculo la media de error para la epoca de entrenamiento. # La longitud de train_loader es igual a la cantidad de batches dentro de una epoca. epoch_train_loss = epoch_train_loss / len(train_loader) train_loss.append(epoch_train_loss) # Realizo el paso de validaciÃ³n computando error y accuracy, y # almacenando los valores para imprimirlos y graficarlos valid_data, valid_target = iter(valid_loader).next() output = model(valid_data) epoch_valid_loss = criterion(output, valid_target).item() valid_loss.append(epoch_valid_loss) print(f\"Epoch: {epoch+1}/{epochs} - Train loss {epoch_train_loss:.3f} - Valid Loss {epoch_valid_loss:.3f}\") history = { \"loss\": train_loss, \"val_loss\": valid_loss, } return history # Generar datos sintÃ©ticos X = list() y = list() X = [x+1 for x in range(20)] # \"y\" (target) se obtiene como cada dato de entrada multiplicado por 15 y = [x * 15 for x in X] print(\"datos X:\", X) print(\"datos y:\", y) # Cada dato X lo transformarmos en una matriz de 1 fila 1 columna (1x1) X = np.array(X).reshape(len(X), 1, 1) print(\"datos X:\", X) # (batch size, seq_len, input_size) X.shape y = np.asanyarray(y) y.shape class Data(Dataset): def __init__(self, x, y): # Convertir los arrays de numpy a tensores. # pytorch espera en general entradas 32bits self.x = torch.from_numpy(x.astype(np.float32)) # las loss unfction esperan la salida float self.y = torch.from_numpy(y.astype(np.int32)).float().view(-1, 1) self.len = self.y.shape[0] def __getitem__(self,index): return self.x[index], self.y[index] def __len__(self): return self.len data_set = Data(X, y) input_dim = data_set.x.shape[1:] seq_length = input_dim[0] input_size = input_dim[1] print(\"Input dim\", input_dim) print(\"seq_length:\", seq_length) print(\"input_size:\", input_size) output_dim = data_set.y.shape[1] print(\"Output dim\", output_dim) data_set.x.shape data_set.y.shape torch.manual_seed(42) valid_set_size = int(data_set.len * 0.2) train_set_size = data_set.len - valid_set_size # Cuando trabajmos con una serie temporal no mezclamos (shuffle) los datos train_set = torch.utils.data.Subset(data_set, range(train_set_size)) valid_set = torch.utils.data.Subset(data_set, range(train_set_size, data_set.len)) print(\"TamaÃ±o del conjunto de entrenamiento:\", len(train_set)) print(\"TamaÃ±o del conjunto de validacion:\", len(valid_set)) train_loader = torch.utils.data.DataLoader(train_set, batch_size=len(train_set), shuffle=False) valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=len(valid_set), shuffle=False) \"\"\"### 2 - Entrenar el modelo (RNN y LSTM)\"\"\" from torch_helpers import CustomRNN class Model1(nn.Module): def __init__(self, input_size, output_dim): super().__init__() #self.rnn1 = nn.RNN(input_size=input_size, hidden_size=64, batch_first=True) # RNN layer # Utilizamos la CustomRNN ya que para series temporales suele funcionar mejor # la activacion \"relu\" en las RNN en vez de la \"tanh\", pero por defecto la # layer de Pytorch RNN no permite modificar la funcion de activacion #self.rnn1 = CustomRNN(input_size=input_size, hidden_size=64) # RNN layer self.rnn1 = CustomRNN(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # RNN layer self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer def forward(self, x): lstm_output, _ = self.rnn1(x) out = self.fc(lstm_output[:,-1,:]) # take last output (last seq) return out model1 = Model1(input_size=input_size, output_dim=output_dim) # Crear el optimizador la una funciÃ³n de error model1_optimizer = torch.optim.Adam(model1.parameters(), lr=0.01) model1_criterion = nn.MSELoss()  # mean squared error summary(model1, input_size=(1, seq_length, input_size)) history1 = train(model1, train_loader, valid_loader, model1_optimizer, model1_criterion, epochs=500 ) epoch_count = range(1, len(history1[\\'loss\\']) + 1) sns.lineplot(x=epoch_count,  y=history1[\\'loss\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history1[\\'val_loss\\'], label=\\'valid\\') plt.show() # Ensayo # x = 30 # y_test = x * 15 x_test = 30 y_test = x_test * 15 test_input = np.array([x_test]) test_input = test_input.reshape((1, seq_length, input_size)) test_input = torch.from_numpy(test_input.astype(np.float32)) test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1) y_hat = model1(test_input) print(\"y_test:\", y_test) print(\"y_hat:\", y_hat) loss = model1_criterion(y_hat, test_target).item() print(\"loss:\", loss) from torch_helpers import CustomLSTM class Model2(nn.Module): def __init__(self, input_size, output_dim): super().__init__() #self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=64, batch_first=True) # LSTM layer # Utilizamos la CustomLSTM ya que para series temporales suele funcionar mejor # la activacion \"relu\" en las LSTM en vez de la \"tanh\", pero por defecto la # layer de Pytorch LSTM no permite modificar la funcion de activacion self.lstm1 = CustomLSTM(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # LSTM layer self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer def forward(self, x): lstm_output, _ = self.lstm1(x) out = self.fc(lstm_output[:,-1,:]) # take last output (last seq) return out model2 = Model2(input_size=input_size, output_dim=output_dim) # Crear el optimizador la una funciÃ³n de error model2_optimizer = torch.optim.Adam(model2.parameters(), lr=0.01) model2_criterion = nn.MSELoss()  # mean squared error summary(model2, input_size=(1, seq_length, input_size)) history2 = train(model2, train_loader, valid_loader, model2_optimizer, model2_criterion, epochs=500 ) epoch_count = range(1, len(history2[\\'loss\\']) + 1) sns.lineplot(x=epoch_count,  y=history2[\\'loss\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history2[\\'val_loss\\'], label=\\'valid\\') plt.show() # Ensayo # x = 30 # y_test = x * 15 x_test = 30 y_test = x_test * 15 test_input = np.array([x_test]) test_input = test_input.reshape((1, seq_length, input_size)) test_input = torch.from_numpy(test_input.astype(np.float32)) test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1) y_hat = model2(test_input) print(\"y_test:\", y_test) print(\"y_hat:\", y_hat) loss = model2_criterion(y_hat, test_target).item() print(\"loss:\", loss) \"\"\"Se puede observar que para un problema tan simple como este no hay mucha diferencia entre utilizar una RNN o LSTM. La LSTM tiene muchos mÃ¡s parÃ¡metros que la RNN. ### 3 - Multi-layer LSTM \"\"\" from torch_helpers import CustomLSTM # En esta oportunidad se utilizarÃ¡n dos layer LSTM class Model3(nn.Module): def __init__(self, input_size, output_dim): super().__init__() self.lstm1 = CustomLSTM(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # LSTM layer self.lstm2 = CustomLSTM(input_size=64, hidden_size=64, activation=nn.ReLU()) # LSTM layer self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer def forward(self, x): lstm_output, _ = self.lstm1(x) lstm_output, _ = self.lstm2(lstm_output) out = self.fc(lstm_output[:,-1,:]) # take last output (last seq) return out model3 = Model3(input_size=input_size, output_dim=output_dim) # Crear el optimizador la una funciÃ³n de error model3_optimizer = torch.optim.Adam(model3.parameters(), lr=0.01) model3_criterion = nn.MSELoss()  # mean squared error summary(model3, input_size=(1, seq_length, input_size)) history3 = train(model3, train_loader, valid_loader, model3_optimizer, model3_criterion, epochs=500 ) epoch_count = range(1, len(history3[\\'loss\\']) + 1) sns.lineplot(x=epoch_count,  y=history3[\\'loss\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history3[\\'val_loss\\'], label=\\'valid\\') plt.show() # Ensayo # x = 30 # y_test = x * 15 x_test = 30 y_test = x_test * 15 test_input = np.array([x_test]) test_input = test_input.reshape((1, seq_length, input_size)) test_input = torch.from_numpy(test_input.astype(np.float32)) test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1) y_hat = model3(test_input) print(\"y_test:\", y_test) print(\"y_hat:\", y_hat) loss = model3_criterion(y_hat, test_target).item() print(\"loss:\", loss) \"\"\"### 4 - ConclusiÃ³n El resultado alcanzado es bueno pero podrÃ\\xada mejorarse agregando mÃ¡s layer LSTM o mÃ¡s layer fully connected. \"\"\" import re import numpy as np import matplotlib.pyplot as plt import seaborn as sns import torch import torch.nn.functional as F import torch.nn as nn from torch.utils.data import Dataset, DataLoader # torchsummar actualmente tiene un problema con las LSTM, por eso # se utiliza torchinfo, un fork del proyecto original con el bug solucionado !pip3 install torchinfo from torchinfo import summary import os import platform if os.access(\\'torch_helpers.py\\', os.F_OK) is False: if platform.system() == \\'Windows\\': !curl !wget https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py > torch_helpers.py else: !wget torch_helpers.py https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py def train(model, train_loader, valid_loader, optimizer, criterion, epochs=100): # Defino listas para realizar graficas de los resultados train_loss = [] valid_loss = [] # Defino mi loop de entrenamiento for epoch in range(epochs): epoch_train_loss = 0.0 epoch_train_accuracy = 0.0 for train_data, train_target in train_loader: # Seteo los gradientes en cero ya que, por defecto, PyTorch # los va acumulando optimizer.zero_grad() output = model(train_data) # Computo el error de la salida comparando contra las etiquetas loss = criterion(output, train_target) # Almaceno el error del batch para luego tener el error promedio de la epoca epoch_train_loss += loss.item() # Computo el nuevo set de gradientes a lo largo de toda la red loss.backward() # Realizo el paso de optimizacion actualizando los parametros de toda la red optimizer.step() # Calculo la media de error para la epoca de entrenamiento. # La longitud de train_loader es igual a la cantidad de batches dentro de una epoca. epoch_train_loss = epoch_train_loss / len(train_loader) train_loss.append(epoch_train_loss) # Realizo el paso de validaciÃ³n computando error y accuracy, y # almacenando los valores para imprimirlos y graficarlos valid_data, valid_target = iter(valid_loader).next() output = model(valid_data) epoch_valid_loss = criterion(output, valid_target).item() valid_loss.append(epoch_valid_loss) print(f\"Epoch: {epoch+1}/{epochs} - Train loss {epoch_train_loss:.3f} - Valid Loss {epoch_valid_loss:.3f}\") history = { \"loss\": train_loss, \"val_loss\": valid_loss, } return history # Generar datos sintÃ©ticos X = list() y = list() X = [x+1 for x in range(20)] # \"y\" (target) se obtiene como cada dato de entrada multiplicado por 15 y = [x * 15 for x in X] print(\"datos X:\", X) print(\"datos y:\", y) # Cada dato X lo transformarmos en una matriz de 1 fila 1 columna (1x1) X = np.array(X).reshape(len(X), 1, 1) print(\"datos X:\", X) # (batch size, seq_len, input_size) X.shape y = np.asanyarray(y) y.shape class Data(Dataset): def __init__(self, x, y): # Convertir los arrays de numpy a tensores. # pytorch espera en general entradas 32bits self.x = torch.from_numpy(x.astype(np.float32)) # las loss unfction esperan la salida float self.y = torch.from_numpy(y.astype(np.int32)).float().view(-1, 1) self.len = self.y.shape[0] def __getitem__(self,index): return self.x[index], self.y[index] def __len__(self): return self.len data_set = Data(X, y) input_dim = data_set.x.shape[1:] seq_length = input_dim[0] input_size = input_dim[1] print(\"Input dim\", input_dim) print(\"seq_length:\", seq_length) print(\"input_size:\", input_size) output_dim = data_set.y.shape[1] print(\"Output dim\", output_dim) data_set.x.shape data_set.y.shape torch.manual_seed(42) valid_set_size = int(data_set.len * 0.2) train_set_size = data_set.len - valid_set_size # Cuando trabajmos con una serie temporal no mezclamos (shuffle) los datos train_set = torch.utils.data.Subset(data_set, range(train_set_size)) valid_set = torch.utils.data.Subset(data_set, range(train_set_size, data_set.len)) print(\"TamaÃ±o del conjunto de entrenamiento:\", len(train_set)) print(\"TamaÃ±o del conjunto de validacion:\", len(valid_set)) train_loader = torch.utils.data.DataLoader(train_set, batch_size=len(train_set), shuffle=False) valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=len(valid_set), shuffle=False) \"\"\"### 2 - Entrenar el modelo (RNN y LSTM)\"\"\" from torch_helpers import CustomRNN class Model1(nn.Module): def __init__(self, input_size, output_dim): super().__init__() #self.rnn1 = nn.RNN(input_size=input_size, hidden_size=64, batch_first=True) # RNN layer # Utilizamos la CustomRNN ya que para series temporales suele funcionar mejor # la activacion \"relu\" en las RNN en vez de la \"tanh\", pero por defecto la # layer de Pytorch RNN no permite modificar la funcion de activacion #self.rnn1 = CustomRNN(input_size=input_size, hidden_size=64) # RNN layer self.rnn1 = CustomRNN(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # RNN layer self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer def forward(self, x): lstm_output, _ = self.rnn1(x) out = self.fc(lstm_output[:,-1,:]) # take last output (last seq) return out model1 = Model1(input_size=input_size, output_dim=output_dim) # Crear el optimizador la una funciÃ³n de error model1_optimizer = torch.optim.Adam(model1.parameters(), lr=0.01) model1_criterion = nn.MSELoss()  # mean squared error summary(model1, input_size=(1, seq_length, input_size)) history1 = train(model1, train_loader, valid_loader, model1_optimizer, model1_criterion, epochs=500 ) epoch_count = range(1, len(history1[\\'loss\\']) + 1) sns.lineplot(x=epoch_count,  y=history1[\\'loss\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history1[\\'val_loss\\'], label=\\'valid\\') plt.show() # Ensayo # x = 30 # y_test = x * 15 x_test = 30 y_test = x_test * 15 test_input = np.array([x_test]) test_input = test_input.reshape((1, seq_length, input_size)) test_input = torch.from_numpy(test_input.astype(np.float32)) test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1) y_hat = model1(test_input) print(\"y_test:\", y_test) print(\"y_hat:\", y_hat) loss = model1_criterion(y_hat, test_target).item() print(\"loss:\", loss) from torch_helpers import CustomLSTM class Model2(nn.Module): def __init__(self, input_size, output_dim): super().__init__() #self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=64, batch_first=True) # LSTM layer # Utilizamos la CustomLSTM ya que para series temporales suele funcionar mejor # la activacion \"relu\" en las LSTM en vez de la \"tanh\", pero por defecto la # layer de Pytorch LSTM no permite modificar la funcion de activacion self.lstm1 = CustomLSTM(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # LSTM layer self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer def forward(self, x): lstm_output, _ = self.lstm1(x) out = self.fc(lstm_output[:,-1,:]) # take last output (last seq) return out model2 = Model2(input_size=input_size, output_dim=output_dim) # Crear el optimizador la una funciÃ³n de error model2_optimizer = torch.optim.Adam(model2.parameters(), lr=0.01) model2_criterion = nn.MSELoss()  # mean squared error summary(model2, input_size=(1, seq_length, input_size)) history2 = train(model2, train_loader, valid_loader, model2_optimizer, model2_criterion, epochs=500 ) epoch_count = range(1, len(history2[\\'loss\\']) + 1) sns.lineplot(x=epoch_count,  y=history2[\\'loss\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history2[\\'val_loss\\'], label=\\'valid\\') plt.show() # Ensayo # x = 30 # y_test = x * 15 x_test = 30 y_test = x_test * 15 test_input = np.array([x_test]) test_input = test_input.reshape((1, seq_length, input_size)) test_input = torch.from_numpy(test_input.astype(np.float32)) test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1) y_hat = model2(test_input) print(\"y_test:\", y_test) print(\"y_hat:\", y_hat) loss = model2_criterion(y_hat, test_target).item() print(\"loss:\", loss) \"\"\"Se puede observar que para un problema tan simple como este no hay mucha diferencia entre utilizar una RNN o LSTM. La LSTM tiene muchos mÃ¡s parÃ¡metros que la RNN. ### 3 - Multi-layer LSTM \"\"\" from torch_helpers import CustomLSTM # En esta oportunidad se utilizarÃ¡n dos layer LSTM class Model3(nn.Module): def __init__(self, input_size, output_dim): super().__init__() self.lstm1 = CustomLSTM(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # LSTM layer self.lstm2 = CustomLSTM(input_size=64, hidden_size=64, activation=nn.ReLU()) # LSTM layer self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer def forward(self, x): lstm_output, _ = self.lstm1(x) lstm_output, _ = self.lstm2(lstm_output) out = self.fc(lstm_output[:,-1,:]) # take last output (last seq) return out model3 = Model3(input_size=input_size, output_dim=output_dim) # Crear el optimizador la una funciÃ³n de error model3_optimizer = torch.optim.Adam(model3.parameters(), lr=0.01) model3_criterion = nn.MSELoss()  # mean squared error summary(model3, input_size=(1, seq_length, input_size)) history3 = train(model3, train_loader, valid_loader, model3_optimizer, model3_criterion, epochs=500 ) epoch_count = range(1, len(history3[\\'loss\\']) + 1) sns.lineplot(x=epoch_count,  y=history3[\\'loss\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history3[\\'val_loss\\'], label=\\'valid\\') plt.show() # Ensayo # x = 30 # y_test = x * 15 x_test = 30 y_test = x_test * 15 test_input = np.array([x_test]) test_input = test_input.reshape((1, seq_length, input_size)) test_input = torch.from_numpy(test_input.astype(np.float32)) test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1) y_hat = model3(test_input) print(\"y_test:\", y_test) print(\"y_hat:\", y_hat) loss = model3_criterion(y_hat, test_target).item() print(\"loss:\", loss) \"\"\"### 4 - ConclusiÃ³n El resultado alcanzado es bueno pero podrÃ\\xada mejorarse agregando mÃ¡s layer LSTM o mÃ¡s layer fully connected. \"\"\" import re import numpy as np import matplotlib.pyplot as plt import seaborn as sns import torch import torch.nn.functional as F import torch.nn as nn from torch.utils.data import Dataset, DataLoader # torchsummar actualmente tiene un problema con las LSTM, por eso # se utiliza torchinfo, un fork del proyecto original con el bug solucionado !pip3 install torchinfo from torchinfo import summary import os import platform if os.access(\\'torch_helpers.py\\', os.F_OK) is False: if platform.system() == \\'Windows\\': !curl !wget https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py > torch_helpers.py else: !wget torch_helpers.py https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py def train(model, train_loader, valid_loader, optimizer, criterion, epochs=100): # Defino listas para realizar graficas de los resultados train_loss = [] valid_loss = [] # Defino mi loop de entrenamiento for epoch in range(epochs): epoch_train_loss = 0.0 epoch_train_accuracy = 0.0 for train_data, train_target in train_loader: # Seteo los gradientes en cero ya que, por defecto, PyTorch # los va acumulando optimizer.zero_grad() output = model(train_data) # Computo el error de la salida comparando contra las etiquetas loss = criterion(output, train_target) # Almaceno el error del batch para luego tener el error promedio de la epoca epoch_train_loss += loss.item() # Computo el nuevo set de gradientes a lo largo de toda la red loss.backward() # Realizo el paso de optimizacion actualizando los parametros de toda la red optimizer.step() # Calculo la media de error para la epoca de entrenamiento. # La longitud de train_loader es igual a la cantidad de batches dentro de una epoca. epoch_train_loss = epoch_train_loss / len(train_loader) train_loss.append(epoch_train_loss) # Realizo el paso de validaciÃ³n computando error y accuracy, y # almacenando los valores para imprimirlos y graficarlos valid_data, valid_target = iter(valid_loader).next() output = model(valid_data) epoch_valid_loss = criterion(output, valid_target).item() valid_loss.append(epoch_valid_loss) print(f\"Epoch: {epoch+1}/{epochs} - Train loss {epoch_train_loss:.3f} - Valid Loss {epoch_valid_loss:.3f}\") history = { \"loss\": train_loss, \"val_loss\": valid_loss, } return history # Generar datos sintÃ©ticos X = list() y = list() X = [x+1 for x in range(20)] # \"y\" (target) se obtiene como cada dato de entrada multiplicado por 15 y = [x * 15 for x in X] print(\"datos X:\", X) print(\"datos y:\", y) # Cada dato X lo transformarmos en una matriz de 1 fila 1 columna (1x1) X = np.array(X).reshape(len(X), 1, 1) print(\"datos X:\", X) # (batch size, seq_len, input_size) X.shape y = np.asanyarray(y) y.shape class Data(Dataset): def __init__(self, x, y): # Convertir los arrays de numpy a tensores. # pytorch espera en general entradas 32bits self.x = torch.from_numpy(x.astype(np.float32)) # las loss unfction esperan la salida float self.y = torch.from_numpy(y.astype(np.int32)).float().view(-1, 1) self.len = self.y.shape[0] def __getitem__(self,index): return self.x[index], self.y[index] def __len__(self): return self.len data_set = Data(X, y) input_dim = data_set.x.shape[1:] seq_length = input_dim[0] input_size = input_dim[1] print(\"Input dim\", input_dim) print(\"seq_length:\", seq_length) print(\"input_size:\", input_size) output_dim = data_set.y.shape[1] print(\"Output dim\", output_dim) data_set.x.shape data_set.y.shape torch.manual_seed(42) valid_set_size = int(data_set.len * 0.2) train_set_size = data_set.len - valid_set_size # Cuando trabajmos con una serie temporal no mezclamos (shuffle) los datos train_set = torch.utils.data.Subset(data_set, range(train_set_size)) valid_set = torch.utils.data.Subset(data_set, range(train_set_size, data_set.len)) print(\"TamaÃ±o del conjunto de entrenamiento:\", len(train_set)) print(\"TamaÃ±o del conjunto de validacion:\", len(valid_set)) train_loader = torch.utils.data.DataLoader(train_set, batch_size=len(train_set), shuffle=False) valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=len(valid_set), shuffle=False) \"\"\"### 2 - Entrenar el modelo (RNN y LSTM)\"\"\" from torch_helpers import CustomRNN class Model1(nn.Module): def __init__(self, input_size, output_dim): super().__init__() #self.rnn1 = nn.RNN(input_size=input_size, hidden_size=64, batch_first=True) # RNN layer # Utilizamos la CustomRNN ya que para series temporales suele funcionar mejor # la activacion \"relu\" en las RNN en vez de la \"tanh\", pero por defecto la # layer de Pytorch RNN no permite modificar la funcion de activacion #self.rnn1 = CustomRNN(input_size=input_size, hidden_size=64) # RNN layer self.rnn1 = CustomRNN(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # RNN layer self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer def forward(self, x): lstm_output, _ = self.rnn1(x) out = self.fc(lstm_output[:,-1,:]) # take last output (last seq) return out model1 = Model1(input_size=input_size, output_dim=output_dim) # Crear el optimizador la una funciÃ³n de error model1_optimizer = torch.optim.Adam(model1.parameters(), lr=0.01) model1_criterion = nn.MSELoss()  # mean squared error summary(model1, input_size=(1, seq_length, input_size)) history1 = train(model1, train_loader, valid_loader, model1_optimizer, model1_criterion, epochs=500 ) epoch_count = range(1, len(history1[\\'loss\\']) + 1) sns.lineplot(x=epoch_count,  y=history1[\\'loss\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history1[\\'val_loss\\'], label=\\'valid\\') plt.show() # Ensayo # x = 30 # y_test = x * 15 x_test = 30 y_test = x_test * 15 test_input = np.array([x_test]) test_input = test_input.reshape((1, seq_length, input_size)) test_input = torch.from_numpy(test_input.astype(np.float32)) test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1) y_hat = model1(test_input) print(\"y_test:\", y_test) print(\"y_hat:\", y_hat) loss = model1_criterion(y_hat, test_target).item() print(\"loss:\", loss) from torch_helpers import CustomLSTM class Model2(nn.Module): def __init__(self, input_size, output_dim): super().__init__() #self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=64, batch_first=True) # LSTM layer # Utilizamos la CustomLSTM ya que para series temporales suele funcionar mejor # la activacion \"relu\" en las LSTM en vez de la \"tanh\", pero por defecto la # layer de Pytorch LSTM no permite modificar la funcion de activacion self.lstm1 = CustomLSTM(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # LSTM layer self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer def forward(self, x): lstm_output, _ = self.lstm1(x) out = self.fc(lstm_output[:,-1,:]) # take last output (last seq) return out model2 = Model2(input_size=input_size, output_dim=output_dim) # Crear el optimizador la una funciÃ³n de error model2_optimizer = torch.optim.Adam(model2.parameters(), lr=0.01) model2_criterion = nn.MSELoss()  # mean squared error summary(model2, input_size=(1, seq_length, input_size)) history2 = train(model2, train_loader, valid_loader, model2_optimizer, model2_criterion, epochs=500 ) epoch_count = range(1, len(history2[\\'loss\\']) + 1) sns.lineplot(x=epoch_count,  y=history2[\\'loss\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history2[\\'val_loss\\'], label=\\'valid\\') plt.show() # Ensayo # x = 30 # y_test = x * 15 x_test = 30 y_test = x_test * 15 test_input = np.array([x_test]) test_input = test_input.reshape((1, seq_length, input_size)) test_input = torch.from_numpy(test_input.astype(np.float32)) test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1) y_hat = model2(test_input) print(\"y_test:\", y_test) print(\"y_hat:\", y_hat) loss = model2_criterion(y_hat, test_target).item() print(\"loss:\", loss) \"\"\"Se puede observar que para un problema tan simple como este no hay mucha diferencia entre utilizar una RNN o LSTM. La LSTM tiene muchos mÃ¡s parÃ¡metros que la RNN. ### 3 - Multi-layer LSTM \"\"\" from torch_helpers import CustomLSTM # En esta oportunidad se utilizarÃ¡n dos layer LSTM class Model3(nn.Module): def __init__(self, input_size, output_dim): super().__init__() self.lstm1 = CustomLSTM(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # LSTM layer self.lstm2 = CustomLSTM(input_size=64, hidden_size=64, activation=nn.ReLU()) # LSTM layer self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer def forward(self, x): lstm_output, _ = self.lstm1(x) lstm_output, _ = self.lstm2(lstm_output) out = self.fc(lstm_output[:,-1,:]) # take last output (last seq) return out model3 = Model3(input_size=input_size, output_dim=output_dim) # Crear el optimizador la una funciÃ³n de error model3_optimizer = torch.optim.Adam(model3.parameters(), lr=0.01) model3_criterion = nn.MSELoss()  # mean squared error summary(model3, input_size=(1, seq_length, input_size)) history3 = train(model3, train_loader, valid_loader, model3_optimizer, model3_criterion, epochs=500 ) epoch_count = range(1, len(history3[\\'loss\\']) + 1) sns.lineplot(x=epoch_count,  y=history3[\\'loss\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history3[\\'val_loss\\'], label=\\'valid\\') plt.show() # Ensayo # x = 30 # y_test = x * 15 x_test = 30 y_test = x_test * 15 test_input = np.array([x_test]) test_input = test_input.reshape((1, seq_length, input_size)) test_input = torch.from_numpy(test_input.astype(np.float32)) test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1) y_hat = model3(test_input) print(\"y_test:\", y_test) print(\"y_hat:\", y_hat) loss = model3_criterion(y_hat, test_target).item() print(\"loss:\", loss) \"\"\"### 4 - ConclusiÃ³n El resultado alcanzado es bueno pero podrÃ\\xada mejorarse agregando mÃ¡s layer LSTM o mÃ¡s layer fully connected. \"\"\" import re import numpy as np import matplotlib.pyplot as plt import seaborn as sns import torch import torch.nn.functional as F import torch.nn as nn from torch.utils.data import Dataset, DataLoader # torchsummar actualmente tiene un problema con las LSTM, por eso # se utiliza torchinfo, un fork del proyecto original con el bug solucionado !pip3 install torchinfo from torchinfo import summary import os import platform if os.access(\\'torch_helpers.py\\', os.F_OK) is False: if platform.system() == \\'Windows\\': !curl !wget https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py > torch_helpers.py else: !wget torch_helpers.py https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py def train(model, train_loader, valid_loader, optimizer, criterion, epochs=100): # Defino listas para realizar graficas de los resultados train_loss = [] valid_loss = [] # Defino mi loop de entrenamiento for epoch in range(epochs): epoch_train_loss = 0.0 epoch_train_accuracy = 0.0 for train_data, train_target in train_loader: # Seteo los gradientes en cero ya que, por defecto, PyTorch # los va acumulando optimizer.zero_grad() output = model(train_data) # Computo el error de la salida comparando contra las etiquetas loss = criterion(output, train_target) # Almaceno el error del batch para luego tener el error promedio de la epoca epoch_train_loss += loss.item() # Computo el nuevo set de gradientes a lo largo de toda la red loss.backward() # Realizo el paso de optimizacion actualizando los parametros de toda la red optimizer.step() # Calculo la media de error para la epoca de entrenamiento. # La longitud de train_loader es igual a la cantidad de batches dentro de una epoca. epoch_train_loss = epoch_train_loss / len(train_loader) train_loss.append(epoch_train_loss) # Realizo el paso de validaciÃ³n computando error y accuracy, y # almacenando los valores para imprimirlos y graficarlos valid_data, valid_target = iter(valid_loader).next() output = model(valid_data) epoch_valid_loss = criterion(output, valid_target).item() valid_loss.append(epoch_valid_loss) print(f\"Epoch: {epoch+1}/{epochs} - Train loss {epoch_train_loss:.3f} - Valid Loss {epoch_valid_loss:.3f}\") history = { \"loss\": train_loss, \"val_loss\": valid_loss, } return history # Generar datos sintÃ©ticos X = list() y = list() X = [x+1 for x in range(20)] # \"y\" (target) se obtiene como cada dato de entrada multiplicado por 15 y = [x * 15 for x in X] print(\"datos X:\", X) print(\"datos y:\", y) # Cada dato X lo transformarmos en una matriz de 1 fila 1 columna (1x1) X = np.array(X).reshape(len(X), 1, 1) print(\"datos X:\", X) # (batch size, seq_len, input_size) X.shape y = np.asanyarray(y) y.shape class Data(Dataset): def __init__(self, x, y): # Convertir los arrays de numpy a tensores. # pytorch espera en general entradas 32bits self.x = torch.from_numpy(x.astype(np.float32)) # las loss unfction esperan la salida float self.y = torch.from_numpy(y.astype(np.int32)).float().view(-1, 1) self.len = self.y.shape[0] def __getitem__(self,index): return self.x[index], self.y[index] def __len__(self): return self.len data_set = Data(X, y) input_dim = data_set.x.shape[1:] seq_length = input_dim[0] input_size = input_dim[1] print(\"Input dim\", input_dim) print(\"seq_length:\", seq_length) print(\"input_size:\", input_size) output_dim = data_set.y.shape[1] print(\"Output dim\", output_dim) data_set.x.shape data_set.y.shape torch.manual_seed(42) valid_set_size = int(data_set.len * 0.2) train_set_size = data_set.len - valid_set_size # Cuando trabajmos con una serie temporal no mezclamos (shuffle) los datos train_set = torch.utils.data.Subset(data_set, range(train_set_size)) valid_set = torch.utils.data.Subset(data_set, range(train_set_size, data_set.len)) print(\"TamaÃ±o del conjunto de entrenamiento:\", len(train_set)) print(\"TamaÃ±o del conjunto de validacion:\", len(valid_set)) train_loader = torch.utils.data.DataLoader(train_set, batch_size=len(train_set), shuffle=False) valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=len(valid_set), shuffle=False) \"\"\"### 2 - Entrenar el modelo (RNN y LSTM)\"\"\" from torch_helpers import CustomRNN class Model1(nn.Module): def __init__(self, input_size, output_dim): super().__init__() #self.rnn1 = nn.RNN(input_size=input_size, hidden_size=64, batch_first=True) # RNN layer # Utilizamos la CustomRNN ya que para series temporales suele funcionar mejor # la activacion \"relu\" en las RNN en vez de la \"tanh\", pero por defecto la # layer de Pytorch RNN no permite modificar la funcion de activacion #self.rnn1 = CustomRNN(input_size=input_size, hidden_size=64) # RNN layer self.rnn1 = CustomRNN(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # RNN layer self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer def forward(self, x): lstm_output, _ = self.rnn1(x) out = self.fc(lstm_output[:,-1,:]) # take last output (last seq) return out model1 = Model1(input_size=input_size, output_dim=output_dim) # Crear el optimizador la una funciÃ³n de error model1_optimizer = torch.optim.Adam(model1.parameters(), lr=0.01) model1_criterion = nn.MSELoss()  # mean squared error summary(model1, input_size=(1, seq_length, input_size)) history1 = train(model1, train_loader, valid_loader, model1_optimizer, model1_criterion, epochs=500 ) epoch_count = range(1, len(history1[\\'loss\\']) + 1) sns.lineplot(x=epoch_count,  y=history1[\\'loss\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history1[\\'val_loss\\'], label=\\'valid\\') plt.show() # Ensayo # x = 30 # y_test = x * 15 x_test = 30 y_test = x_test * 15 test_input = np.array([x_test]) test_input = test_input.reshape((1, seq_length, input_size)) test_input = torch.from_numpy(test_input.astype(np.float32)) test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1) y_hat = model1(test_input) print(\"y_test:\", y_test) print(\"y_hat:\", y_hat) loss = model1_criterion(y_hat, test_target).item() print(\"loss:\", loss) from torch_helpers import CustomLSTM class Model2(nn.Module): def __init__(self, input_size, output_dim): super().__init__() #self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=64, batch_first=True) # LSTM layer # Utilizamos la CustomLSTM ya que para series temporales suele funcionar mejor # la activacion \"relu\" en las LSTM en vez de la \"tanh\", pero por defecto la # layer de Pytorch LSTM no permite modificar la funcion de activacion self.lstm1 = CustomLSTM(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # LSTM layer self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer def forward(self, x): lstm_output, _ = self.lstm1(x) out = self.fc(lstm_output[:,-1,:]) # take last output (last seq) return out model2 = Model2(input_size=input_size, output_dim=output_dim) # Crear el optimizador la una funciÃ³n de error model2_optimizer = torch.optim.Adam(model2.parameters(), lr=0.01) model2_criterion = nn.MSELoss()  # mean squared error summary(model2, input_size=(1, seq_length, input_size)) history2 = train(model2, train_loader, valid_loader, model2_optimizer, model2_criterion, epochs=500 ) epoch_count = range(1, len(history2[\\'loss\\']) + 1) sns.lineplot(x=epoch_count,  y=history2[\\'loss\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history2[\\'val_loss\\'], label=\\'valid\\') plt.show() # Ensayo # x = 30 # y_test = x * 15 x_test = 30 y_test = x_test * 15 test_input = np.array([x_test]) test_input = test_input.reshape((1, seq_length, input_size)) test_input = torch.from_numpy(test_input.astype(np.float32)) test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1) y_hat = model2(test_input) print(\"y_test:\", y_test) print(\"y_hat:\", y_hat) loss = model2_criterion(y_hat, test_target).item() print(\"loss:\", loss) \"\"\"Se puede observar que para un problema tan simple como este no hay mucha diferencia entre utilizar una RNN o LSTM. La LSTM tiene muchos mÃ¡s parÃ¡metros que la RNN. ### 3 - Multi-layer LSTM \"\"\" from torch_helpers import CustomLSTM # En esta oportunidad se utilizarÃ¡n dos layer LSTM class Model3(nn.Module): def __init__(self, input_size, output_dim): super().__init__() self.lstm1 = CustomLSTM(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # LSTM layer self.lstm2 = CustomLSTM(input_size=64, hidden_size=64, activation=nn.ReLU()) # LSTM layer self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer def forward(self, x): lstm_output, _ = self.lstm1(x) lstm_output, _ = self.lstm2(lstm_output) out = self.fc(lstm_output[:,-1,:]) # take last output (last seq) return out model3 = Model3(input_size=input_size, output_dim=output_dim) # Crear el optimizador la una funciÃ³n de error model3_optimizer = torch.optim.Adam(model3.parameters(), lr=0.01) model3_criterion = nn.MSELoss()  # mean squared error summary(model3, input_size=(1, seq_length, input_size)) history3 = train(model3, train_loader, valid_loader, model3_optimizer, model3_criterion, epochs=500 ) epoch_count = range(1, len(history3[\\'loss\\']) + 1) sns.lineplot(x=epoch_count,  y=history3[\\'loss\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history3[\\'val_loss\\'], label=\\'valid\\') plt.show() # Ensayo # x = 30 # y_test = x * 15 x_test = 30 y_test = x_test * 15 test_input = np.array([x_test]) test_input = test_input.reshape((1, seq_length, input_size)) test_input = torch.from_numpy(test_input.astype(np.float32)) test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1) y_hat = model3(test_input) print(\"y_test:\", y_test) print(\"y_hat:\", y_hat) loss = model3_criterion(y_hat, test_target).item() print(\"loss:\", loss) \"\"\"### 4 - ConclusiÃ³n El resultado alcanzado es bueno pero podrÃ\\xada mejorarse agregando mÃ¡s layer LSTM o mÃ¡s layer fully connected. \"\"\" import re import numpy as np import matplotlib.pyplot as plt import seaborn as sns import torch import torch.nn.functional as F import torch.nn as nn from torch.utils.data import Dataset, DataLoader # torchsummar actualmente tiene un problema con las LSTM, por eso # se utiliza torchinfo, un fork del proyecto original con el bug solucionado !pip3 install torchinfo from torchinfo import summary import os import platform if os.access(\\'torch_helpers.py\\', os.F_OK) is False: if platform.system() == \\'Windows\\': !curl !wget https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py > torch_helpers.py else: !wget torch_helpers.py https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py def train(model, train_loader, valid_loader, optimizer, criterion, epochs=100): # Defino listas para realizar graficas de los resultados train_loss = [] valid_loss = [] # Defino mi loop de entrenamiento for epoch in range(epochs): epoch_train_loss = 0.0 epoch_train_accuracy = 0.0 for train_data, train_target in train_loader: # Seteo los gradientes en cero ya que, por defecto, PyTorch # los va acumulando optimizer.zero_grad() output = model(train_data) # Computo el error de la salida comparando contra las etiquetas loss = criterion(output, train_target) # Almaceno el error del batch para luego tener el error promedio de la epoca epoch_train_loss += loss.item() # Computo el nuevo set de gradientes a lo largo de toda la red loss.backward() # Realizo el paso de optimizacion actualizando los parametros de toda la red optimizer.step() # Calculo la media de error para la epoca de entrenamiento. # La longitud de train_loader es igual a la cantidad de batches dentro de una epoca. epoch_train_loss = epoch_train_loss / len(train_loader) train_loss.append(epoch_train_loss) # Realizo el paso de validaciÃ³n computando error y accuracy, y # almacenando los valores para imprimirlos y graficarlos valid_data, valid_target = iter(valid_loader).next() output = model(valid_data) epoch_valid_loss = criterion(output, valid_target).item() valid_loss.append(epoch_valid_loss) print(f\"Epoch: {epoch+1}/{epochs} - Train loss {epoch_train_loss:.3f} - Valid Loss {epoch_valid_loss:.3f}\") history = { \"loss\": train_loss, \"val_loss\": valid_loss, } return history # Generar datos sintÃ©ticos X = list() y = list() # X es una lista de nÃºmeros de 1 al 43 que avanzan de 3 en 3 X = [x for x in range(1, 44, 3)] # \"y\" (target) se obtiene como por cada dato de entrada se # se obtienen dos datos de salida como x+1 y x+2 y = [ [x+1, x+2] for x in X] print(\"datos X:\", X) print(\"datos y:\", y) # Cada dato X lo transformarmos en una matriz de 1 fila 1 columna (1x1) X = np.array(X).reshape(len(X), 1, 1) print(\"datos X:\", X) # (batch size, seq_len, input_size) X.shape y = np.asanyarray(y) y.shape class Data(Dataset): def __init__(self, x, y): # Convertir los arrays de numpy a tensores. # pytorch espera en general entradas 32bits self.x = torch.from_numpy(x.astype(np.float32)) # las loss unfction esperan la salida float self.y = torch.from_numpy(y.astype(np.float32)) self.len = self.y.shape[0] def __getitem__(self,index): return self.x[index], self.y[index] def __len__(self): return self.len data_set = Data(X, y) input_dim = data_set.x.shape[1:] seq_length = input_dim[0] input_size = input_dim[1] print(\"Input dim\", input_dim) print(\"seq_length:\", seq_length) print(\"input_size:\", input_size) output_dim = data_set.y.shape[1] print(\"Output dim\", output_dim) data_set.x.shape data_set.y.shape torch.manual_seed(42) valid_set_size = int(data_set.len * 0.2) train_set_size = data_set.len - valid_set_size # Cuando trabajmos con una serie temporal no mezclamos (shuffle) los datos train_set = torch.utils.data.Subset(data_set, range(train_set_size)) valid_set = torch.utils.data.Subset(data_set, range(train_set_size, data_set.len)) print(\"TamaÃ±o del conjunto de entrenamiento:\", len(train_set)) print(\"TamaÃ±o del conjunto de validacion:\", len(valid_set)) train_loader = torch.utils.data.DataLoader(train_set, batch_size=len(train_set), shuffle=False) valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=len(valid_set), shuffle=False) \"\"\"### 2 - Entrenar el modelo\"\"\" from torch_helpers import CustomLSTM class Model1(nn.Module): def __init__(self, input_size, output_dim): super().__init__() #self.rnn1 = nn.RNN(input_size=input_size, hidden_size=64, batch_first=True) # RNN layer # Utilizamos la CustomRNN ya que para series temporales suele funcionar mejor # la activacion \"relu\" en las RNN en vez de la \"tanh\", pero por defecto la # layer de Pytorch RNN no permite modificar la funcion de activacion #self.rnn1 = CustomRNN(input_size=input_size, hidden_size=64) # RNN layer self.rnn1 = CustomLSTM(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # RNN layer self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer def forward(self, x): lstm_output, _ = self.rnn1(x) out = self.fc(lstm_output[:,-1,:]) # take last output (last seq) return out model1 = Model1(input_size=input_size, output_dim=output_dim) # Crear el optimizador la una funciÃ³n de error model1_optimizer = torch.optim.Adam(model1.parameters(), lr=0.01) model1_criterion = nn.MSELoss()  # mean squared error summary(model1, input_size=(1, seq_length, input_size)) history1 = train(model1, train_loader, valid_loader, model1_optimizer, model1_criterion, epochs=500 ) epoch_count = range(1, len(history1[\\'loss\\']) + 1) sns.lineplot(x=epoch_count,  y=history1[\\'loss\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history1[\\'val_loss\\'], label=\\'valid\\') plt.show() # Ensayo x_test = 10 y_test = [x_test + 1, x_test + 2] test_input = np.array([x_test]) test_input = test_input.reshape((1, seq_length, input_size)) test_input = torch.from_numpy(test_input.astype(np.float32)) test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1) y_hat = model1(test_input) print(\"y_test:\", y_test) print(\"y_hat:\", y_hat) loss = model1_criterion(y_hat, test_target).item() print(\"loss:\", loss) \"\"\"### 3 - Multi-layer LSTM\"\"\" from torch_helpers import CustomLSTM # En esta oportunidad se utilizarÃ¡n dos layer LSTM class Model2(nn.Module): def __init__(self, input_size, output_dim): super().__init__() self.lstm1 = CustomLSTM(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # LSTM layer self.lstm2 = CustomLSTM(input_size=64, hidden_size=64, activation=nn.ReLU()) # LSTM layer self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer def forward(self, x): lstm_output, _ = self.lstm1(x) lstm_output, _ = self.lstm2(lstm_output) out = self.fc(lstm_output[:,-1,:]) # take last output (last seq) return out model2 = Model2(input_size=input_size, output_dim=output_dim) # Crear el optimizador la una funciÃ³n de error model2_optimizer = torch.optim.Adam(model2.parameters(), lr=0.01) model2_criterion = nn.MSELoss()  # mean squared error summary(model2, input_size=(1, seq_length, input_size)) history2 = train(model2, train_loader, valid_loader, model2_optimizer, model2_criterion, epochs=500 ) epoch_count = range(1, len(history2[\\'loss\\']) + 1) sns.lineplot(x=epoch_count,  y=history2[\\'loss\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history2[\\'val_loss\\'], label=\\'valid\\') plt.show() # Ensayo x_test = 10 y_test = [x_test + 1, x_test + 2] test_input = np.array([x_test]) test_input = test_input.reshape((1, seq_length, input_size)) test_input = torch.from_numpy(test_input.astype(np.float32)) test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1) y_hat = model2(test_input) print(\"y_test:\", y_test) print(\"y_hat:\", y_hat) loss = model2_criterion(y_hat, test_target).item() print(\"loss:\", loss) \"\"\"### 4 - ConclusiÃ³n La unica diferencia que se debe tener en cuenta cuando hay mÃ¡s de una salida es que la cantidad de neuronas de la Ãºltima capa debe coincidir con el tamaÃ±o de la secuencia de salida. En este ejemplo, donde el problema es mÃ¡s complejo, hubo una diferencia apreciable entre utilizar una sola capa o varias LSTM. \"\"\" import re import numpy as np import matplotlib.pyplot as plt import seaborn as sns import torch import torch.nn.functional as F import torch.nn as nn from torch.utils.data import Dataset, DataLoader # torchsummar actualmente tiene un problema con las LSTM, por eso # se utiliza torchinfo, un fork del proyecto original con el bug solucionado !pip3 install torchinfo from torchinfo import summary import os import platform if os.access(\\'torch_helpers.py\\', os.F_OK) is False: if platform.system() == \\'Windows\\': !curl !wget https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py > torch_helpers.py else: !wget torch_helpers.py https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py def train(model, train_loader, valid_loader, optimizer, criterion, epochs=100): # Defino listas para realizar graficas de los resultados train_loss = [] valid_loss = [] # Defino mi loop de entrenamiento for epoch in range(epochs): epoch_train_loss = 0.0 epoch_train_accuracy = 0.0 for train_data, train_target in train_loader: # Seteo los gradientes en cero ya que, por defecto, PyTorch # los va acumulando optimizer.zero_grad() output = model(train_data) # Computo el error de la salida comparando contra las etiquetas loss = criterion(output, train_target) # Almaceno el error del batch para luego tener el error promedio de la epoca epoch_train_loss += loss.item() # Computo el nuevo set de gradientes a lo largo de toda la red loss.backward() # Realizo el paso de optimizacion actualizando los parametros de toda la red optimizer.step() # Calculo la media de error para la epoca de entrenamiento. # La longitud de train_loader es igual a la cantidad de batches dentro de una epoca. epoch_train_loss = epoch_train_loss / len(train_loader) train_loss.append(epoch_train_loss) # Realizo el paso de validaciÃ³n computando error y accuracy, y # almacenando los valores para imprimirlos y graficarlos valid_data, valid_target = iter(valid_loader).next() output = model(valid_data) epoch_valid_loss = criterion(output, valid_target).item() valid_loss.append(epoch_valid_loss) print(f\"Epoch: {epoch+1}/{epochs} - Train loss {epoch_train_loss:.3f} - Valid Loss {epoch_valid_loss:.3f}\") history = { \"loss\": train_loss, \"val_loss\": valid_loss, } return history # Generar datos sintÃ©ticos X = list() y = list() X = [x+1 for x in range(20)] # \"y\" (target) se obtiene como cada dato de entrada multiplicado por 15 y = [x * 15 for x in X] print(\"datos X:\", X) print(\"datos y:\", y) # Cada dato X lo transformarmos en una matriz de 1 fila 1 columna (1x1) X = np.array(X).reshape(len(X), 1, 1) print(\"datos X:\", X) # (batch size, seq_len, input_size) X.shape y = np.asanyarray(y) y.shape class Data(Dataset): def __init__(self, x, y): # Convertir los arrays de numpy a tensores. # pytorch espera en general entradas 32bits self.x = torch.from_numpy(x.astype(np.float32)) # las loss unfction esperan la salida float self.y = torch.from_numpy(y.astype(np.int32)).float().view(-1, 1) self.len = self.y.shape[0] def __getitem__(self,index): return self.x[index], self.y[index] def __len__(self): return self.len data_set = Data(X, y) input_dim = data_set.x.shape[1:] seq_length = input_dim[0] input_size = input_dim[1] print(\"Input dim\", input_dim) print(\"seq_length:\", seq_length) print(\"input_size:\", input_size) output_dim = data_set.y.shape[1] print(\"Output dim\", output_dim) data_set.x.shape data_set.y.shape torch.manual_seed(42) valid_set_size = int(data_set.len * 0.2) train_set_size = data_set.len - valid_set_size # Cuando trabajmos con una serie temporal no mezclamos (shuffle) los datos train_set = torch.utils.data.Subset(data_set, range(train_set_size)) valid_set = torch.utils.data.Subset(data_set, range(train_set_size, data_set.len)) print(\"TamaÃ±o del conjunto de entrenamiento:\", len(train_set)) print(\"TamaÃ±o del conjunto de validacion:\", len(valid_set)) train_loader = torch.utils.data.DataLoader(train_set, batch_size=len(train_set), shuffle=False) valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=len(valid_set), shuffle=False) \"\"\"### 2 - Entrenar el modelo (RNN y LSTM)\"\"\" from torch_helpers import CustomRNN class Model1(nn.Module): def __init__(self, input_size, output_dim): super().__init__() #self.rnn1 = nn.RNN(input_size=input_size, hidden_size=64, batch_first=True) # RNN layer # Utilizamos la CustomRNN ya que para series temporales suele funcionar mejor # la activacion \"relu\" en las RNN en vez de la \"tanh\", pero por defecto la # layer de Pytorch RNN no permite modificar la funcion de activacion #self.rnn1 = CustomRNN(input_size=input_size, hidden_size=64) # RNN layer self.rnn1 = CustomRNN(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # RNN layer self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer def forward(self, x): lstm_output, _ = self.rnn1(x) out = self.fc(lstm_output[:,-1,:]) # take last output (last seq) return out model1 = Model1(input_size=input_size, output_dim=output_dim) # Crear el optimizador la una funciÃ³n de error model1_optimizer = torch.optim.Adam(model1.parameters(), lr=0.01) model1_criterion = nn.MSELoss()  # mean squared error summary(model1, input_size=(1, seq_length, input_size)) history1 = train(model1, train_loader, valid_loader, model1_optimizer, model1_criterion, epochs=500 ) epoch_count = range(1, len(history1[\\'loss\\']) + 1) sns.lineplot(x=epoch_count,  y=history1[\\'loss\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history1[\\'val_loss\\'], label=\\'valid\\') plt.show() # Ensayo # x = 30 # y_test = x * 15 x_test = 30 y_test = x_test * 15 test_input = np.array([x_test]) test_input = test_input.reshape((1, seq_length, input_size)) test_input = torch.from_numpy(test_input.astype(np.float32)) test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1) y_hat = model1(test_input) print(\"y_test:\", y_test) print(\"y_hat:\", y_hat) loss = model1_criterion(y_hat, test_target).item() print(\"loss:\", loss) from torch_helpers import CustomLSTM class Model2(nn.Module): def __init__(self, input_size, output_dim): super().__init__() #self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=64, batch_first=True) # LSTM layer # Utilizamos la CustomLSTM ya que para series temporales suele funcionar mejor # la activacion \"relu\" en las LSTM en vez de la \"tanh\", pero por defecto la # layer de Pytorch LSTM no permite modificar la funcion de activacion self.lstm1 = CustomLSTM(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # LSTM layer self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer def forward(self, x): lstm_output, _ = self.lstm1(x) out = self.fc(lstm_output[:,-1,:]) # take last output (last seq) return out model2 = Model2(input_size=input_size, output_dim=output_dim) # Crear el optimizador la una funciÃ³n de error model2_optimizer = torch.optim.Adam(model2.parameters(), lr=0.01) model2_criterion = nn.MSELoss()  # mean squared error summary(model2, input_size=(1, seq_length, input_size)) history2 = train(model2, train_loader, valid_loader, model2_optimizer, model2_criterion, epochs=500 ) epoch_count = range(1, len(history2[\\'loss\\']) + 1) sns.lineplot(x=epoch_count,  y=history2[\\'loss\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history2[\\'val_loss\\'], label=\\'valid\\') plt.show() # Ensayo # x = 30 # y_test = x * 15 x_test = 30 y_test = x_test * 15 test_input = np.array([x_test]) test_input = test_input.reshape((1, seq_length, input_size)) test_input = torch.from_numpy(test_input.astype(np.float32)) test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1) y_hat = model2(test_input) print(\"y_test:\", y_test) print(\"y_hat:\", y_hat) loss = model2_criterion(y_hat, test_target).item() print(\"loss:\", loss) \"\"\"Se puede observar que para un problema tan simple como este no hay mucha diferencia entre utilizar una RNN o LSTM. La LSTM tiene muchos mÃ¡s parÃ¡metros que la RNN. ### 3 - Multi-layer LSTM \"\"\" from torch_helpers import CustomLSTM # En esta oportunidad se utilizarÃ¡n dos layer LSTM class Model3(nn.Module): def __init__(self, input_size, output_dim): super().__init__() self.lstm1 = CustomLSTM(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # LSTM layer self.lstm2 = CustomLSTM(input_size=64, hidden_size=64, activation=nn.ReLU()) # LSTM layer self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer def forward(self, x): lstm_output, _ = self.lstm1(x) lstm_output, _ = self.lstm2(lstm_output) out = self.fc(lstm_output[:,-1,:]) # take last output (last seq) return out model3 = Model3(input_size=input_size, output_dim=output_dim) # Crear el optimizador la una funciÃ³n de error model3_optimizer = torch.optim.Adam(model3.parameters(), lr=0.01) model3_criterion = nn.MSELoss()  # mean squared error summary(model3, input_size=(1, seq_length, input_size)) history3 = train(model3, train_loader, valid_loader, model3_optimizer, model3_criterion, epochs=500 ) epoch_count = range(1, len(history3[\\'loss\\']) + 1) sns.lineplot(x=epoch_count,  y=history3[\\'loss\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history3[\\'val_loss\\'], label=\\'valid\\') plt.show() # Ensayo # x = 30 # y_test = x * 15 x_test = 30 y_test = x_test * 15 test_input = np.array([x_test]) test_input = test_input.reshape((1, seq_length, input_size)) test_input = torch.from_numpy(test_input.astype(np.float32)) test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1) y_hat = model3(test_input) print(\"y_test:\", y_test) print(\"y_hat:\", y_hat) loss = model3_criterion(y_hat, test_target).item() print(\"loss:\", loss) \"\"\"### 4 - ConclusiÃ³n El resultado alcanzado es bueno pero podrÃ\\xada mejorarse agregando mÃ¡s layer LSTM o mÃ¡s layer fully connected. \"\"\" import re import numpy as np import matplotlib.pyplot as plt import seaborn as sns import torch import torch.nn.functional as F import torch.nn as nn from torch.utils.data import Dataset, DataLoader # torchsummar actualmente tiene un problema con las LSTM, por eso # se utiliza torchinfo, un fork del proyecto original con el bug solucionado !pip3 install torchinfo from torchinfo import summary import os import platform if os.access(\\'torch_helpers.py\\', os.F_OK) is False: if platform.system() == \\'Windows\\': !curl !wget https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py > torch_helpers.py else: !wget torch_helpers.py https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py def train(model, train_loader, valid_loader, optimizer, criterion, epochs=100): # Defino listas para realizar graficas de los resultados train_loss = [] valid_loss = [] # Defino mi loop de entrenamiento for epoch in range(epochs): epoch_train_loss = 0.0 epoch_train_accuracy = 0.0 for train_data, train_target in train_loader: # Seteo los gradientes en cero ya que, por defecto, PyTorch # los va acumulando optimizer.zero_grad() output = model(train_data) # Computo el error de la salida comparando contra las etiquetas loss = criterion(output, train_target) # Almaceno el error del batch para luego tener el error promedio de la epoca epoch_train_loss += loss.item() # Computo el nuevo set de gradientes a lo largo de toda la red loss.backward() # Realizo el paso de optimizacion actualizando los parametros de toda la red optimizer.step() # Calculo la media de error para la epoca de entrenamiento. # La longitud de train_loader es igual a la cantidad de batches dentro de una epoca. epoch_train_loss = epoch_train_loss / len(train_loader) train_loss.append(epoch_train_loss) # Realizo el paso de validaciÃ³n computando error y accuracy, y # almacenando los valores para imprimirlos y graficarlos valid_data, valid_target = iter(valid_loader).next() output = model(valid_data) epoch_valid_loss = criterion(output, valid_target).item() valid_loss.append(epoch_valid_loss) print(f\"Epoch: {epoch+1}/{epochs} - Train loss {epoch_train_loss:.3f} - Valid Loss {epoch_valid_loss:.3f}\") history = { \"loss\": train_loss, \"val_loss\": valid_loss, } return history # Generar datos sintÃ©ticos X = list() y = list() X = [x+1 for x in range(20)] # \"y\" (target) se obtiene como cada dato de entrada multiplicado por 15 y = [x * 15 for x in X] print(\"datos X:\", X) print(\"datos y:\", y) # Cada dato X lo transformarmos en una matriz de 1 fila 1 columna (1x1) X = np.array(X).reshape(len(X), 1, 1) print(\"datos X:\", X) # (batch size, seq_len, input_size) X.shape y = np.asanyarray(y) y.shape class Data(Dataset): def __init__(self, x, y): # Convertir los arrays de numpy a tensores. # pytorch espera en general entradas 32bits self.x = torch.from_numpy(x.astype(np.float32)) # las loss unfction esperan la salida float self.y = torch.from_numpy(y.astype(np.int32)).float().view(-1, 1) self.len = self.y.shape[0] def __getitem__(self,index): return self.x[index], self.y[index] def __len__(self): return self.len data_set = Data(X, y) input_dim = data_set.x.shape[1:] seq_length = input_dim[0] input_size = input_dim[1] print(\"Input dim\", input_dim) print(\"seq_length:\", seq_length) print(\"input_size:\", input_size) output_dim = data_set.y.shape[1] print(\"Output dim\", output_dim) data_set.x.shape data_set.y.shape torch.manual_seed(42) valid_set_size = int(data_set.len * 0.2) train_set_size = data_set.len - valid_set_size # Cuando trabajmos con una serie temporal no mezclamos (shuffle) los datos train_set = torch.utils.data.Subset(data_set, range(train_set_size)) valid_set = torch.utils.data.Subset(data_set, range(train_set_size, data_set.len)) print(\"TamaÃ±o del conjunto de entrenamiento:\", len(train_set)) print(\"TamaÃ±o del conjunto de validacion:\", len(valid_set)) train_loader = torch.utils.data.DataLoader(train_set, batch_size=len(train_set), shuffle=False) valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=len(valid_set), shuffle=False) \"\"\"### 2 - Entrenar el modelo (RNN y LSTM)\"\"\" from torch_helpers import CustomRNN class Model1(nn.Module): def __init__(self, input_size, output_dim): super().__init__() #self.rnn1 = nn.RNN(input_size=input_size, hidden_size=64, batch_first=True) # RNN layer # Utilizamos la CustomRNN ya que para series temporales suele funcionar mejor # la activacion \"relu\" en las RNN en vez de la \"tanh\", pero por defecto la # layer de Pytorch RNN no permite modificar la funcion de activacion #self.rnn1 = CustomRNN(input_size=input_size, hidden_size=64) # RNN layer self.rnn1 = CustomRNN(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # RNN layer self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer def forward(self, x): lstm_output, _ = self.rnn1(x) out = self.fc(lstm_output[:,-1,:]) # take last output (last seq) return out model1 = Model1(input_size=input_size, output_dim=output_dim) # Crear el optimizador la una funciÃ³n de error model1_optimizer = torch.optim.Adam(model1.parameters(), lr=0.01) model1_criterion = nn.MSELoss()  # mean squared error summary(model1, input_size=(1, seq_length, input_size)) history1 = train(model1, train_loader, valid_loader, model1_optimizer, model1_criterion, epochs=500 ) epoch_count = range(1, len(history1[\\'loss\\']) + 1) sns.lineplot(x=epoch_count,  y=history1[\\'loss\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history1[\\'val_loss\\'], label=\\'valid\\') plt.show() # Ensayo # x = 30 # y_test = x * 15 x_test = 30 y_test = x_test * 15 test_input = np.array([x_test]) test_input = test_input.reshape((1, seq_length, input_size)) test_input = torch.from_numpy(test_input.astype(np.float32)) test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1) y_hat = model1(test_input) print(\"y_test:\", y_test) print(\"y_hat:\", y_hat) loss = model1_criterion(y_hat, test_target).item() print(\"loss:\", loss) from torch_helpers import CustomLSTM class Model2(nn.Module): def __init__(self, input_size, output_dim): super().__init__() #self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=64, batch_first=True) # LSTM layer # Utilizamos la CustomLSTM ya que para series temporales suele funcionar mejor # la activacion \"relu\" en las LSTM en vez de la \"tanh\", pero por defecto la # layer de Pytorch LSTM no permite modificar la funcion de activacion self.lstm1 = CustomLSTM(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # LSTM layer self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer def forward(self, x): lstm_output, _ = self.lstm1(x) out = self.fc(lstm_output[:,-1,:]) # take last output (last seq) return out model2 = Model2(input_size=input_size, output_dim=output_dim) # Crear el optimizador la una funciÃ³n de error model2_optimizer = torch.optim.Adam(model2.parameters(), lr=0.01) model2_criterion = nn.MSELoss()  # mean squared error summary(model2, input_size=(1, seq_length, input_size)) history2 = train(model2, train_loader, valid_loader, model2_optimizer, model2_criterion, epochs=500 ) epoch_count = range(1, len(history2[\\'loss\\']) + 1) sns.lineplot(x=epoch_count,  y=history2[\\'loss\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history2[\\'val_loss\\'], label=\\'valid\\') plt.show() # Ensayo # x = 30 # y_test = x * 15 x_test = 30 y_test = x_test * 15 test_input = np.array([x_test]) test_input = test_input.reshape((1, seq_length, input_size)) test_input = torch.from_numpy(test_input.astype(np.float32)) test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1) y_hat = model2(test_input) print(\"y_test:\", y_test) print(\"y_hat:\", y_hat) loss = model2_criterion(y_hat, test_target).item() print(\"loss:\", loss) \"\"\"Se puede observar que para un problema tan simple como este no hay mucha diferencia entre utilizar una RNN o LSTM. La LSTM tiene muchos mÃ¡s parÃ¡metros que la RNN. ### 3 - Multi-layer LSTM \"\"\" from torch_helpers import CustomLSTM # En esta oportunidad se utilizarÃ¡n dos layer LSTM class Model3(nn.Module): def __init__(self, input_size, output_dim): super().__init__() self.lstm1 = CustomLSTM(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # LSTM layer self.lstm2 = CustomLSTM(input_size=64, hidden_size=64, activation=nn.ReLU()) # LSTM layer self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer def forward(self, x): lstm_output, _ = self.lstm1(x) lstm_output, _ = self.lstm2(lstm_output) out = self.fc(lstm_output[:,-1,:]) # take last output (last seq) return out model3 = Model3(input_size=input_size, output_dim=output_dim) # Crear el optimizador la una funciÃ³n de error model3_optimizer = torch.optim.Adam(model3.parameters(), lr=0.01) model3_criterion = nn.MSELoss()  # mean squared error summary(model3, input_size=(1, seq_length, input_size)) history3 = train(model3, train_loader, valid_loader, model3_optimizer, model3_criterion, epochs=500 ) epoch_count = range(1, len(history3[\\'loss\\']) + 1) sns.lineplot(x=epoch_count,  y=history3[\\'loss\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history3[\\'val_loss\\'], label=\\'valid\\') plt.show() # Ensayo # x = 30 # y_test = x * 15 x_test = 30 y_test = x_test * 15 test_input = np.array([x_test]) test_input = test_input.reshape((1, seq_length, input_size)) test_input = torch.from_numpy(test_input.astype(np.float32)) test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1) y_hat = model3(test_input) print(\"y_test:\", y_test) print(\"y_hat:\", y_hat) loss = model3_criterion(y_hat, test_target).item() print(\"loss:\", loss) \"\"\"### 4 - ConclusiÃ³n El resultado alcanzado es bueno pero podrÃ\\xada mejorarse agregando mÃ¡s layer LSTM o mÃ¡s layer fully connected. \"\"\" import re import numpy as np import matplotlib.pyplot as plt import seaborn as sns import torch import torch.nn.functional as F import torch.nn as nn from torch.utils.data import Dataset, DataLoader # torchsummar actualmente tiene un problema con las LSTM, por eso # se utiliza torchinfo, un fork del proyecto original con el bug solucionado !pip3 install torchinfo from torchinfo import summary import os import platform if os.access(\\'torch_helpers.py\\', os.F_OK) is False: if platform.system() == \\'Windows\\': !curl !wget https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py > torch_helpers.py else: !wget torch_helpers.py https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py def train(model, train_loader, valid_loader, optimizer, criterion, epochs=100): # Defino listas para realizar graficas de los resultados train_loss = [] valid_loss = [] # Defino mi loop de entrenamiento for epoch in range(epochs): epoch_train_loss = 0.0 epoch_train_accuracy = 0.0 for train_data, train_target in train_loader: # Seteo los gradientes en cero ya que, por defecto, PyTorch # los va acumulando optimizer.zero_grad() output = model(train_data) # Computo el error de la salida comparando contra las etiquetas loss = criterion(output, train_target) # Almaceno el error del batch para luego tener el error promedio de la epoca epoch_train_loss += loss.item() # Computo el nuevo set de gradientes a lo largo de toda la red loss.backward() # Realizo el paso de optimizacion actualizando los parametros de toda la red optimizer.step() # Calculo la media de error para la epoca de entrenamiento. # La longitud de train_loader es igual a la cantidad de batches dentro de una epoca. epoch_train_loss = epoch_train_loss / len(train_loader) train_loss.append(epoch_train_loss) # Realizo el paso de validaciÃ³n computando error y accuracy, y # almacenando los valores para imprimirlos y graficarlos valid_data, valid_target = iter(valid_loader).next() output = model(valid_data) epoch_valid_loss = criterion(output, valid_target).item() valid_loss.append(epoch_valid_loss) print(f\"Epoch: {epoch+1}/{epochs} - Train loss {epoch_train_loss:.3f} - Valid Loss {epoch_valid_loss:.3f}\") history = { \"loss\": train_loss, \"val_loss\": valid_loss, } return history # Generar datos sintÃ©ticos X = list() y = list() X = [x+1 for x in range(20)] # \"y\" (target) se obtiene como cada dato de entrada multiplicado por 15 y = [x * 15 for x in X] print(\"datos X:\", X) print(\"datos y:\", y) # Cada dato X lo transformarmos en una matriz de 1 fila 1 columna (1x1) X = np.array(X).reshape(len(X), 1, 1) print(\"datos X:\", X) # (batch size, seq_len, input_size) X.shape y = np.asanyarray(y) y.shape class Data(Dataset): def __init__(self, x, y): # Convertir los arrays de numpy a tensores. # pytorch espera en general entradas 32bits self.x = torch.from_numpy(x.astype(np.float32)) # las loss unfction esperan la salida float self.y = torch.from_numpy(y.astype(np.int32)).float().view(-1, 1) self.len = self.y.shape[0] def __getitem__(self,index): return self.x[index], self.y[index] def __len__(self): return self.len data_set = Data(X, y) input_dim = data_set.x.shape[1:] seq_length = input_dim[0] input_size = input_dim[1] print(\"Input dim\", input_dim) print(\"seq_length:\", seq_length) print(\"input_size:\", input_size) output_dim = data_set.y.shape[1] print(\"Output dim\", output_dim) data_set.x.shape data_set.y.shape torch.manual_seed(42) valid_set_size = int(data_set.len * 0.2) train_set_size = data_set.len - valid_set_size # Cuando trabajmos con una serie temporal no mezclamos (shuffle) los datos train_set = torch.utils.data.Subset(data_set, range(train_set_size)) valid_set = torch.utils.data.Subset(data_set, range(train_set_size, data_set.len)) print(\"TamaÃ±o del conjunto de entrenamiento:\", len(train_set)) print(\"TamaÃ±o del conjunto de validacion:\", len(valid_set)) train_loader = torch.utils.data.DataLoader(train_set, batch_size=len(train_set), shuffle=False) valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=len(valid_set), shuffle=False) \"\"\"### 2 - Entrenar el modelo (RNN y LSTM)\"\"\" from torch_helpers import CustomRNN class Model1(nn.Module): def __init__(self, input_size, output_dim): super().__init__() #self.rnn1 = nn.RNN(input_size=input_size, hidden_size=64, batch_first=True) # RNN layer # Utilizamos la CustomRNN ya que para series temporales suele funcionar mejor # la activacion \"relu\" en las RNN en vez de la \"tanh\", pero por defecto la # layer de Pytorch RNN no permite modificar la funcion de activacion #self.rnn1 = CustomRNN(input_size=input_size, hidden_size=64) # RNN layer self.rnn1 = CustomRNN(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # RNN layer self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer def forward(self, x): lstm_output, _ = self.rnn1(x) out = self.fc(lstm_output[:,-1,:]) # take last output (last seq) return out model1 = Model1(input_size=input_size, output_dim=output_dim) # Crear el optimizador la una funciÃ³n de error model1_optimizer = torch.optim.Adam(model1.parameters(), lr=0.01) model1_criterion = nn.MSELoss()  # mean squared error summary(model1, input_size=(1, seq_length, input_size)) history1 = train(model1, train_loader, valid_loader, model1_optimizer, model1_criterion, epochs=500 ) epoch_count = range(1, len(history1[\\'loss\\']) + 1) sns.lineplot(x=epoch_count,  y=history1[\\'loss\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history1[\\'val_loss\\'], label=\\'valid\\') plt.show() # Ensayo # x = 30 # y_test = x * 15 x_test = 30 y_test = x_test * 15 test_input = np.array([x_test]) test_input = test_input.reshape((1, seq_length, input_size)) test_input = torch.from_numpy(test_input.astype(np.float32)) test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1) y_hat = model1(test_input) print(\"y_test:\", y_test) print(\"y_hat:\", y_hat) loss = model1_criterion(y_hat, test_target).item() print(\"loss:\", loss) from torch_helpers import CustomLSTM class Model2(nn.Module): def __init__(self, input_size, output_dim): super().__init__() #self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=64, batch_first=True) # LSTM layer # Utilizamos la CustomLSTM ya que para series temporales suele funcionar mejor # la activacion \"relu\" en las LSTM en vez de la \"tanh\", pero por defecto la # layer de Pytorch LSTM no permite modificar la funcion de activacion self.lstm1 = CustomLSTM(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # LSTM layer self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer def forward(self, x): lstm_output, _ = self.lstm1(x) out = self.fc(lstm_output[:,-1,:]) # take last output (last seq) return out model2 = Model2(input_size=input_size, output_dim=output_dim) # Crear el optimizador la una funciÃ³n de error model2_optimizer = torch.optim.Adam(model2.parameters(), lr=0.01) model2_criterion = nn.MSELoss()  # mean squared error summary(model2, input_size=(1, seq_length, input_size)) history2 = train(model2, train_loader, valid_loader, model2_optimizer, model2_criterion, epochs=500 ) epoch_count = range(1, len(history2[\\'loss\\']) + 1) sns.lineplot(x=epoch_count,  y=history2[\\'loss\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history2[\\'val_loss\\'], label=\\'valid\\') plt.show() # Ensayo # x = 30 # y_test = x * 15 x_test = 30 y_test = x_test * 15 test_input = np.array([x_test]) test_input = test_input.reshape((1, seq_length, input_size)) test_input = torch.from_numpy(test_input.astype(np.float32)) test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1) y_hat = model2(test_input) print(\"y_test:\", y_test) print(\"y_hat:\", y_hat) loss = model2_criterion(y_hat, test_target).item() print(\"loss:\", loss) \"\"\"Se puede observar que para un problema tan simple como este no hay mucha diferencia entre utilizar una RNN o LSTM. La LSTM tiene muchos mÃ¡s parÃ¡metros que la RNN. ### 3 - Multi-layer LSTM \"\"\" from torch_helpers import CustomLSTM # En esta oportunidad se utilizarÃ¡n dos layer LSTM class Model3(nn.Module): def __init__(self, input_size, output_dim): super().__init__() self.lstm1 = CustomLSTM(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # LSTM layer self.lstm2 = CustomLSTM(input_size=64, hidden_size=64, activation=nn.ReLU()) # LSTM layer self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer def forward(self, x): lstm_output, _ = self.lstm1(x) lstm_output, _ = self.lstm2(lstm_output) out = self.fc(lstm_output[:,-1,:]) # take last output (last seq) return out model3 = Model3(input_size=input_size, output_dim=output_dim) # Crear el optimizador la una funciÃ³n de error model3_optimizer = torch.optim.Adam(model3.parameters(), lr=0.01) model3_criterion = nn.MSELoss()  # mean squared error summary(model3, input_size=(1, seq_length, input_size)) history3 = train(model3, train_loader, valid_loader, model3_optimizer, model3_criterion, epochs=500 ) epoch_count = range(1, len(history3[\\'loss\\']) + 1) sns.lineplot(x=epoch_count,  y=history3[\\'loss\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history3[\\'val_loss\\'], label=\\'valid\\') plt.show() # Ensayo # x = 30 # y_test = x * 15 x_test = 30 y_test = x_test * 15 test_input = np.array([x_test]) test_input = test_input.reshape((1, seq_length, input_size)) test_input = torch.from_numpy(test_input.astype(np.float32)) test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1) y_hat = model3(test_input) print(\"y_test:\", y_test) print(\"y_hat:\", y_hat) loss = model3_criterion(y_hat, test_target).item() print(\"loss:\", loss) \"\"\"### 4 - ConclusiÃ³n El resultado alcanzado es bueno pero podrÃ\\xada mejorarse agregando mÃ¡s layer LSTM o mÃ¡s layer fully connected. \"\"\" import re import numpy as np import matplotlib.pyplot as plt import seaborn as sns import torch import torch.nn.functional as F import torch.nn as nn from torch.utils.data import Dataset, DataLoader # torchsummar actualmente tiene un problema con las LSTM, por eso # se utiliza torchinfo, un fork del proyecto original con el bug solucionado !pip3 install torchinfo from torchinfo import summary import os import platform if os.access(\\'torch_helpers.py\\', os.F_OK) is False: if platform.system() == \\'Windows\\': !curl !wget https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py > torch_helpers.py else: !wget torch_helpers.py https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py def train(model, train_loader, valid_loader, optimizer, criterion, epochs=100): # Defino listas para realizar graficas de los resultados train_loss = [] valid_loss = [] # Defino mi loop de entrenamiento for epoch in range(epochs): epoch_train_loss = 0.0 epoch_train_accuracy = 0.0 for train_data, train_target in train_loader: # Seteo los gradientes en cero ya que, por defecto, PyTorch # los va acumulando optimizer.zero_grad() output = model(train_data) # Computo el error de la salida comparando contra las etiquetas loss = criterion(output, train_target) # Almaceno el error del batch para luego tener el error promedio de la epoca epoch_train_loss += loss.item() # Computo el nuevo set de gradientes a lo largo de toda la red loss.backward() # Realizo el paso de optimizacion actualizando los parametros de toda la red optimizer.step() # Calculo la media de error para la epoca de entrenamiento. # La longitud de train_loader es igual a la cantidad de batches dentro de una epoca. epoch_train_loss = epoch_train_loss / len(train_loader) train_loss.append(epoch_train_loss) # Realizo el paso de validaciÃ³n computando error y accuracy, y # almacenando los valores para imprimirlos y graficarlos valid_data, valid_target = iter(valid_loader).next() output = model(valid_data) epoch_valid_loss = criterion(output, valid_target).item() valid_loss.append(epoch_valid_loss) print(f\"Epoch: {epoch+1}/{epochs} - Train loss {epoch_train_loss:.3f} - Valid Loss {epoch_valid_loss:.3f}\") history = { \"loss\": train_loss, \"val_loss\": valid_loss, } return history # Generar datos sintÃ©ticos X = list() y = list() # X serÃ¡ una lista de 1 a 45 agrupado de a 3 nÃºmeros consecutivos # [ [1, 2, 3], [4, 5, 6], ....] X = [ [x, x+1, x+2] for x in range(1, 46, 3)] # \"y\" (target) se obtiene como la suma de cada grupo de 3 nÃºmeros de entrada y = [sum(x) for x in X] print(\"datos X:\", X) print(\"datos y:\", y) # Cada dato X lo transformarmos en una matriz de 1 fila 1 columna (1x1) X = np.array(X).reshape(len(X), len(X[0]), 1) print(\"datos X:\", X) # (batch size, seq_len, input_size) X.shape y = np.asanyarray(y) y.shape class Data(Dataset): def __init__(self, x, y): # Convertir los arrays de numpy a tensores. # pytorch espera en general entradas 32bits self.x = torch.from_numpy(x.astype(np.float32)) # las loss unfction esperan la salida float self.y = torch.from_numpy(y.astype(np.int32)).float().view(-1, 1) self.len = self.y.shape[0] def __getitem__(self,index): return self.x[index], self.y[index] def __len__(self): return self.len data_set = Data(X, y) input_dim = data_set.x.shape[1:] seq_length = input_dim[0] input_size = input_dim[1] print(\"Input dim\", input_dim) print(\"seq_length:\", seq_length) print(\"input_size:\", input_size) output_dim = data_set.y.shape[1] print(\"Output dim\", output_dim) torch.manual_seed(42) valid_set_size = int(data_set.len * 0.2) train_set_size = data_set.len - valid_set_size # Cuando trabajmos con una serie temporal no mezclamos (shuffle) los datos train_set = torch.utils.data.Subset(data_set, range(train_set_size)) valid_set = torch.utils.data.Subset(data_set, range(train_set_size, data_set.len)) print(\"TamaÃ±o del conjunto de entrenamiento:\", len(train_set)) print(\"TamaÃ±o del conjunto de validacion:\", len(valid_set)) train_loader = torch.utils.data.DataLoader(train_set, batch_size=len(train_set), shuffle=False) valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=len(valid_set), shuffle=False) \"\"\"### 2 - Entrenar el modelo\"\"\" from torch_helpers import CustomLSTM class Model1(nn.Module): def __init__(self, input_size, output_dim): super().__init__() self.lstm1 = CustomLSTM(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # LSTM layer self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer def forward(self, x): lstm_output, _ = self.lstm1(x) out = self.fc(lstm_output[:,-1,:]) # take last output (last seq) return out model1 = Model1(input_size=input_size, output_dim=output_dim) # Crear el optimizador la una funciÃ³n de error model1_optimizer = torch.optim.Adam(model1.parameters(), lr=0.01) model1_criterion = nn.MSELoss()  # mean squared error summary(model1, input_size=(1, seq_length, input_size)) history1 = train(model1, train_loader, valid_loader, model1_optimizer, model1_criterion, epochs=500 ) epoch_count = range(1, len(history1[\\'loss\\']) + 1) sns.lineplot(x=epoch_count,  y=history1[\\'loss\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history1[\\'val_loss\\'], label=\\'valid\\') plt.show() # Ensayo x_test = [50, 51, 52] y_test = sum(x_test) test_input = np.array([x_test]) test_input = test_input.reshape((1, seq_length, input_size)) test_input = torch.from_numpy(test_input.astype(np.float32)) test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1) y_hat = model1(test_input) print(\"y_test:\", y_test) print(\"y_hat:\", y_hat) loss = model1_criterion(y_hat, test_target).item() print(\"loss:\", loss) \"\"\"### 3 - Bidirectional RNN (BRNN) Como la implementaciÃ³n de \"CustomLSTM\" no soporta el flag de \"bidirectional\" que trae Pytorch, utilizaremos la layer por defecto de LSTM. Por eso notarÃ¡ que la cantidad de parÃ¡metros con bidireccional no es exactamente el doble (porque pytorch agrega otro bias a la ecuaciÃ³n tradicional).\\\\ La Ãºnica desventaja en este caso, al tratarse de una serie temporal no podremos utilizar la funciÃ³n de activaciÃ³n \"relu\" y por lo tanto el resultado alcanzado no serÃ¡ tan bueno como con la CustomLSTM. \"\"\" from torch_helpers import CustomLSTM # En esta oportunidad se utilizarÃ¡ Bidirectional, dentro se especifica # que lo que se desea hacer bidireccional es una capa LSTM # En el summary se puede observar que la cantidad de parÃ¡metros # de nuestor nueva capa LSTM bidireccional es el doble que la anterior class Model2(nn.Module): def __init__(self, input_size, output_dim): super().__init__() self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=64, batch_first=True, bidirectional=True) self.fc = nn.Linear(in_features=2*64, out_features=output_dim) #  # Fully connected layer def forward(self, x): lstm_output, _ = self.lstm1(x) out = self.fc(lstm_output[:,-1,:]) # take last output (last seq) return out model2 = Model2(input_size=input_size, output_dim=output_dim) # Crear el optimizador la una funciÃ³n de error model2_optimizer = torch.optim.Adam(model2.parameters(), lr=0.01) model2_criterion = nn.MSELoss()  # mean squared error summary(model2, input_size=(1, seq_length, input_size)) history2 = train(model2, train_loader, valid_loader, model2_optimizer, model2_criterion, epochs=500 ) epoch_count = range(1, len(history2[\\'loss\\']) + 1) sns.lineplot(x=epoch_count,  y=history2[\\'loss\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history2[\\'val_loss\\'], label=\\'valid\\') plt.show() # Ensayo x_test = [50, 51, 52] y_test = sum(x_test) test_input = np.array([x_test]) test_input = test_input.reshape((1, seq_length, input_size)) test_input = torch.from_numpy(test_input.astype(np.float32)) test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1) y_hat = model2(test_input) print(\"y_test:\", y_test) print(\"y_hat:\", y_hat) loss = model2_criterion(y_hat, test_target).item() print(\"loss:\", loss) \"\"\"### 4 - ConclusiÃ³n Implementar un modelo bidireccional basado en RNN (en este caso LSTM) es muy sensillo. En este ejemplo no se explotÃ³ su potencialidad pero queda como nota de como implementar una capa BRNN. \"\"\" import random import io import pickle import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import torch import torch.nn.functional as F import torch.nn as nn from torch.utils.data import Dataset, DataLoader cuda = torch.cuda.is_available() device = torch.device(\\'cuda\\' if torch.cuda.is_available() else \\'cpu\\') device # torchsummar actualmente tiene un problema con las LSTM, por eso # se utiliza torchinfo, un fork del proyecto original con el bug solucionado !pip3 install torchinfo from torchinfo import summary import os import platform if os.access(\\'torch_helpers.py\\', os.F_OK) is False: if platform.system() == \\'Windows\\': !curl !wget https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py > torch_helpers.py else: !wget torch_helpers.py https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py from torch_helpers import categorical_acc def train(model, train_loader, valid_loader, optimizer, criterion, epochs=100): # Defino listas para realizar graficas de los resultados train_loss = [] train_accuracy = [] valid_loss = [] valid_accuracy = [] # Defino mi loop de entrenamiento for epoch in range(epochs): epoch_train_loss = 0.0 epoch_train_accuracy = 0.0 for train_data, train_target in train_loader: # Seteo los gradientes en cero ya que, por defecto, PyTorch # los va acumulando optimizer.zero_grad() output = model(train_data.to(device)).cpu() # Computo el error de la salida comparando contra las etiquetas loss = criterion(output, train_target) # Almaceno el error del batch para luego tener el error promedio de la epoca epoch_train_loss += loss.item() # Computo el nuevo set de gradientes a lo largo de toda la red loss.backward() # Realizo el paso de optimizacion actualizando los parametros de toda la red optimizer.step() # Calculo el accuracy del batch accuracy = categorical_acc(output, train_target) # Almaceno el accuracy del batch para luego tener el accuracy promedio de la epoca epoch_train_accuracy += accuracy.item() # Calculo la media de error para la epoca de entrenamiento. # La longitud de train_loader es igual a la cantidad de batches dentro de una epoca. epoch_train_loss = epoch_train_loss / len(train_loader) train_loss.append(epoch_train_loss) epoch_train_accuracy = epoch_train_accuracy / len(train_loader) train_accuracy.append(epoch_train_accuracy) # Realizo el paso de validaciÃ³n computando error y accuracy, y # almacenando los valores para imprimirlos y graficarlos valid_data, valid_target = iter(valid_loader).next() output = model(valid_data.to(device)).cpu() epoch_valid_loss = criterion(output, valid_target).item() valid_loss.append(epoch_valid_loss) # Calculo el accuracy de la epoch epoch_valid_accuracy = categorical_acc(output, valid_target).item() valid_accuracy.append(epoch_valid_accuracy) print(f\"Epoch: {epoch+1}/{epochs} - Train loss {epoch_train_loss:.3f} - Train accuracy {epoch_train_accuracy:.3f} - Valid Loss {epoch_valid_loss:.3f} - Valid accuracy {epoch_valid_accuracy:.3f}\") history = { \"loss\": train_loss, \"accuracy\": train_accuracy, \"val_loss\": valid_loss, \"val_accuracy\": valid_accuracy, } return history \"\"\"### Datos Utilizaremos como dataset canciones de bandas de habla inglÃ©s. \"\"\" # Descargar la carpeta de dataset import os import platform if os.access(\\'./songs_dataset\\', os.F_OK) is False: if os.access(\\'songs_dataset.zip\\', os.F_OK) is False: if platform.system() == \\'Windows\\': !curl https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/datasets/songs_dataset.zip -o songs_dataset.zip else: !wget songs_dataset.zip https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/raw/main/datasets/songs_dataset.zip !unzip -q songs_dataset.zip else: print(\"El dataset ya se encuentra descargado\") # Posibles bandas os.listdir(\"./songs_dataset/\") # Armar el dataset utilizando salto de lÃ\\xadnea para separar las oraciones/docs df = pd.read_csv(\\'songs_dataset/beatles.txt\\', sep=\\'/n\\', header=None) df.head() print(\"Cantidad de documentos:\", df.shape[0]) \"\"\"### 1 - Ejemplo de Preprocesamiento - Hay que transformar las oraciones en tokens. - Dichas oraciones hay que ajustarlas al tamaÃ±o fijo de nuestra sentencia de entrada al modelo. - Hay que separar las palabras objetivos (target) que el modelo debe predecir en cada sentencia armada. \"\"\" from torch_helpers import Tokenizer # tool de keras equivalente a ltokenizer de nltk from torch_helpers import text_to_word_sequence # tool de keras equivalente a word_teokenize de nltk from torch_helpers import pad_sequences # tool de keras qye se utilizarÃ¡ para padding # largo de la secuencia, incluye seq input + word output train_len = 4 # Ejemplo de como transformar una oraciÃ³n a tokens usando keras text = df.loc[0,0] text tokens = text_to_word_sequence(text) # entran oraciones -> salen vectores de N posiciones (tokens) tokens \"\"\"1.1 - Transformar las oraciones en secuencias (tokens) de palabras\"\"\" # Recorrer todas las filas y transformar las oraciones # en secuencias de palabras sentence_tokens = [] for _, row in df[:None].iterrows(): sentence_tokens.append(text_to_word_sequence(row[0])) # Demos un vistazo sentence_tokens[:2] # CÃ³digo para hacer el desfazaje de las palabras # segÃºn el train_len text_sequences = [] for i in range(train_len, len(tokens)): seq = tokens[i-train_len:i] text_sequences.append(seq) # Demos un vistazo a nuestros vectores para entrenar el modelo # seq_input + output text_sequences \"\"\"1.2 - Crear los vectores de palabras (word2vec) Ahora necesitamos pasarlos a nÃºmeros para que lo entienda la red y separar input de output. - El Input seran integers (word2vec) - Mientras que el output serÃ¡ one hot encodeado (labels) del tamaÃ±o del vocabulario \"\"\" tok = Tokenizer() # El tokeinzer \"aprende\" las palabras que se usaran # Se construye (fit) una vez por proyecto, se aplica N veces (tal cual un encoder) tok.fit_on_texts(text_sequences) # Convertimos las palabras a nÃºmeros # entran palabras -> salen nÃºmeros sequences = tok.texts_to_sequences(text_sequences) # Ahora sequences tiene los nÃºmeros \"ID\", largo 4 sequences # Cantidad de casos (doc) de entrada print(tok.document_count) # Cantidad de palabras distintas print(len(tok.word_counts)) # El Ã\\xadndice para cada palabra # El sistema las ordena de las mÃ¡s populares a las menos populares print(tok.word_index) # Cantidad de veces quea aparece cada palabra en cada \"documento\" # (1 documento = 1 caso de entrada) print(tok.word_docs) \"\"\"### 2 - Preprocesamiento completo Debemos realizar los mismos pasos que en el ejemplo anterior, pero antes de eso debemos transformar ese dataset de filas de oraciones en un texto completo continuo para poder extraer el vocabulario. \"\"\" # Vistazo a las primeras filas df.loc[:15,0] # Concatenamos todos los rows en un solo valor corpus = df.apply(lambda row: \\' \\'.join(row.values.astype(str)), axis=0)[0] corpus # Transformar el corpus a tokens tokens=text_to_word_sequence(corpus) # Vistazo general de los primeros tokens tokens[:20] print(\"Cantidad de tokens en el corpus:\", len(tokens)) # CÃ³digo para hacer el desfazaje de las palabras # segÃºn el train_len text_sequences = [] for i in range(train_len, len(tokens)): seq = tokens[i-train_len:i] text_sequences.append(seq) # Demos un vistazo a nuestros vectores para entrenar el modelo text_sequences[:20] # Proceso de tokenizacion tok = Tokenizer() tok.fit_on_texts(text_sequences) # Convertimos las palabras a nÃºmeros # entran palabras -> salen nÃºmeros sequences = tok.texts_to_sequences(text_sequences) # Damos un vistazo sequences[:20] print(\"Cantidad de rows del dataset:\", len(sequences)) \"\"\"### 3 - Input y target\"\"\" # Con numpy es muy fÃ¡cil realizar el slicing de vectores ex = np.array([[1,2,3,4],[5,6,7,8]]) ex # Con numpy es muy fÃ¡cil realizar el slicing de vectores print(\"Dimension:\", ex.shape) print(\"Todos los elementos:\", ex) print(\"Todos los elementos menos el Ãºltimo:\", ex[:, :-1]) input = ex[:,:-1] # todos los rows, menos la ultima col target = ex[:, -1] # Ãºltima col de cada row print(\"Input:\", input) print(\"Target:\", target) arr_sequences = np.array(sequences) x_data = arr_sequences[:,:-1] y_data_int = arr_sequences[:,-1] # aÃºn falta el oneHotEncoder print(x_data.shape) print(y_data_int.shape) # Palabras del vocabulario tok.index_word # Cantidad de palabras en el vocabulario vocab_size = len(tok.word_counts) vocab_size # Â¡Ojo! y_data_int comienza en \"1\" en vez de \"0\" # valor minimo: min(y_data_int) # Hay que restar 1 y_data_int_offset = y_data_int - 1 min(y_data_int_offset) class Data(Dataset): def __init__(self, x, y): # Convertir los arrays de numpy a tensores. # pytorch espera en general entradas 32bits self.x = torch.from_numpy(x.astype(np.int32)) # Transformar los datos a oneHotEncoding # la loss function esperan la salida float self.y = F.one_hot(torch.from_numpy(y), num_classes=vocab_size).float() self.len = self.y.shape[0] def __getitem__(self,index): return self.x[index], self.y[index] def __len__(self): return self.len data_set = Data(x_data, y_data_int_offset) input_size = data_set.x.shape[1] print(\"input_size:\", input_size) output_dim = data_set.y.shape[1] print(\"Output dim\", output_dim) torch.manual_seed(42) valid_set_size = int(data_set.len * 0.2) train_set_size = data_set.len - valid_set_size train_set = torch.utils.data.Subset(data_set, range(train_set_size)) valid_set = torch.utils.data.Subset(data_set, range(train_set_size, data_set.len)) print(\"TamaÃ±o del conjunto de entrenamiento:\", len(train_set)) print(\"TamaÃ±o del conjunto de validacion:\", len(valid_set)) train_batch_size = 32 train_loader = torch.utils.data.DataLoader(train_set, batch_size=train_batch_size, shuffle=True) valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=len(valid_set), shuffle=False) \"\"\"### 4 - Entrenar el modelo\"\"\" class Model1(nn.Module): def __init__(self, vocab_size, output_dim): super().__init__() # num_embeddings = vocab_size --> 1628 palabras distintas + 1 para padding o UNK # embedding_dim = 5 --> crear embeddings de tamaÃ±o 5 (tamaÃ±o variable y ajustable) self.lstm_size = 64 self.num_layers = 2 self.embedding = nn.Embedding(num_embeddings=vocab_size+1, embedding_dim=5, padding_idx=0) self.lstm1 = nn.LSTM(input_size=5, hidden_size=self.lstm_size, batch_first=True, num_layers=self.num_layers, dropout=0.2) # LSTM layer self.fc1 = nn.Linear(in_features=self.lstm_size, out_features=32) # Fully connected layer self.fc2 = nn.Linear(in_features=32, out_features=output_dim) # Fully connected layer self.relu = nn.ReLU() self.softmax = nn.Softmax(dim=1) # normalize in dim 1 def forward(self, x, prev_state=None): if prev_state is None: # En cada nueva inferencia reinicio el hidden state # de la LSTM al menos que sea pasado por parÃ¡metro el # elstado de previo # Esta acciÃ³n se realiza especialmente para que # el hidden_state de la Ãºltima inferencia no afecte # a la siguiente batch_size = x.shape[0] #(batch, seq_size) prev_state = self.init_hidden(batch_size) out = self.embedding(x) lstm_output, (ht, ct) = self.lstm1(out, prev_state) out = self.relu(self.fc1(lstm_output[:,-1,:])) # take last output (last seq) out = self.softmax(self.fc2(out)) return out def init_hidden(self, batch_size): return (torch.zeros(self.num_layers, batch_size, self.lstm_size).to(device), torch.zeros(self.num_layers, batch_size, self.lstm_size).to(device)) model1 = Model1(vocab_size=vocab_size, output_dim=output_dim) if cuda: model1.cuda() # Crear el optimizador la una funciÃ³n de error model1_optimizer = torch.optim.Adam(model1.parameters(), lr=0.001) model1_criterion = torch.nn.CrossEntropyLoss()  # Para clasificaciÃ³n multi categÃ³rica # Por defecto torchinfo testea el modelo con torch.FloatTensor #summary(model1, input_size=(1, input_size), dtypes=[\\'torch.IntTensor\\'], device=torch.device(device)) # otra posibilidad summary(model1, input_data=data_set[0][0].unsqueeze(0)) history1 = train(model1, train_loader, valid_loader, model1_optimizer, model1_criterion, epochs=100 ) epoch_count = range(1, len(history1[\\'accuracy\\']) + 1) sns.lineplot(x=epoch_count,  y=history1[\\'accuracy\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history1[\\'val_accuracy\\'], label=\\'valid\\') plt.show() \"\"\"__Es importante__ destacar que en este ejemplo estamos entrenando nuestro propios Embeddings y para ello se requiere mucha data. En los ejemplos que realizaremos de aquÃ\\xad en mÃ¡s utilizaremos mÃ¡s datos, embeddings pre-enternados o modelos pre-entrenados. ### 5 - PredicciÃ³n de prÃ³xima palabra \"\"\" # pad_sequences # Si la secuencia de entrada supera al input_seq_len (3) se trunca # Si la secuencia es mÃ¡s corta se agregna ceros al comienzo # Se utilizarÃ¡ gradio para ensayar el modelo # Herramienta poderosa para crear interfaces rÃ¡pidas para ensayar modelos # https://gradio.app/ import sys !{sys.executable} -m pip install gradio --quiet import gradio as gr def model_response(human_text): # Encodeamos encoded = tok.texts_to_sequences([human_text])[0] # Si tienen distinto largo encoded = pad_sequences([encoded], maxlen=3, padding=\\'pre\\') # Transformo a tensor tensor = torch.from_numpy(encoded.astype(np.int32)) # PredicciÃ³n softmax y_hat = model1(tensor).argmax(axis=-1) # Debemos buscar en el vocabulario la palabra # que corresopnde al indice (y_hat) predicho por le modelo out_word = \\'\\' for word, index in tok.word_index.items(): if index == y_hat: out_word = word break # Agrego la palabra a la frase predicha return human_text + \\' \\' + out_word iface = gr.Interface( fn=model_response, inputs=[\"textbox\"], outputs=\"text\", layout=\"vertical\") iface.launch(debug=True) \"\"\"### 6 - GeneraciÃ³n de secuencias nuevas\"\"\" def generate_seq(model, tokenizer, seed_text, max_length, n_words): \"\"\" Exec model sequence prediction Args: model (keras): modelo entrenado tokenizer (keras tokenizer): tonenizer utilizado en el preprocesamiento seed_text (string): texto de entrada (input_seq) max_length (int): mÃ¡xima longitud de la sequencia de entrada n_words (int): nÃºmeros de palabras a agregar a la sequencia de entrada returns: output_text (string): sentencia con las \"n_words\" agregadas \"\"\" output_text = seed_text # generate a fixed number of words for _ in range(n_words): # Encodeamos encoded = tokenizer.texts_to_sequences([output_text])[0] # Si tienen distinto largo encoded = pad_sequences([encoded], maxlen=max_length, padding=\\'pre\\') # Transformo a tensor tensor = torch.from_numpy(encoded.astype(np.int32)) # PredicciÃ³n softmax y_hat = model1(tensor).argmax(axis=-1) # Vamos concatenando las predicciones out_word = \\'\\' # Debemos buscar en el vocabulario la palabra # que corresopnde al indice (y_hat) predicho por le modelo for word, index in tokenizer.word_index.items(): if index == y_hat: out_word = word break # Agrego las palabras a la frase predicha output_text += \\' \\' + out_word return output_text input_text=\\'hey jude don\\\\\\'t\\' generate_seq(model1, tok, input_text, max_length=3, n_words=2) \"\"\"### 7 - Conclusiones El modelo entrenado tuvo un muy mail desempeÃ±o en el entrenamiento ademÃ¡s de overfitting. Cuestiones que podrÃ\\xadan mejorarse: - Agregar mÃ¡s capas o neuronaes - Incrementar la cantidad de Ã©pocas - Agregar BRNN Es importante destacar que en este ejemplo estamos entrenando nuestro propios Embeddings, y para ello se requiere mucha data. En los ejemplos que realizaremos de aquÃ\\xad en mÃ¡s utilizaremos mÃ¡s datos, embeddings pre-enternados o modelos pre-entrenados. \"\"\" import random import io import pickle import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import torch import torch.nn.functional as F import torch.nn as nn from torch.utils.data import Dataset, DataLoader # torchsummar actualmente tiene un problema con las LSTM, por eso # se utiliza torchinfo, un fork del proyecto original con el bug solucionado !pip3 install torchinfo from torchinfo import summary # import os import platform if not os.path.exists(\\'torch_helpers.py\\'): if platform.system() == \\'Windows\\': os.system(\\'curl -o torch_helpers.py https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py\\') else: os.system(\\'curl -o torch_helpers.py https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py\\') from torch_helpers import categorical_acc def train(model, train_loader, valid_loader, optimizer, criterion, epochs=100): # Defino listas para realizar graficas de los resultados train_loss = [] train_accuracy = [] valid_loss = [] valid_accuracy = [] # Defino mi loop de entrenamiento for epoch in range(epochs): epoch_train_loss = 0.0 epoch_train_accuracy = 0.0 for train_data, train_target in train_loader: # Seteo los gradientes en cero ya que, por defecto, PyTorch # los va acumulando optimizer.zero_grad() output = model(train_data) # Computo el error de la salida comparando contra las etiquetas loss = criterion(output, train_target) # Almaceno el error del batch para luego tener el error promedio de la epoca epoch_train_loss += loss.item() # Computo el nuevo set de gradientes a lo largo de toda la red loss.backward() # Realizo el paso de optimizacion actualizando los parametros de toda la red optimizer.step() # Calculo el accuracy del batch accuracy = categorical_acc(output, train_target) # Almaceno el accuracy del batch para luego tener el accuracy promedio de la epoca epoch_train_accuracy += accuracy.item() # Calculo la media de error para la epoca de entrenamiento. # La longitud de train_loader es igual a la cantidad de batches dentro de una epoca. epoch_train_loss = epoch_train_loss / len(train_loader) train_loss.append(epoch_train_loss) epoch_train_accuracy = epoch_train_accuracy / len(train_loader) train_accuracy.append(epoch_train_accuracy) # Realizo el paso de validaciÃ³n computando error y accuracy, y # almacenando los valores para imprimirlos y graficarlos valid_data, valid_target = iter(valid_loader).__next__() output = model(valid_data) epoch_valid_loss = criterion(output, valid_target).item() valid_loss.append(epoch_valid_loss) # Calculo el accuracy de la epoch epoch_valid_accuracy = categorical_acc(output, valid_target).item() valid_accuracy.append(epoch_valid_accuracy) print(f\"Epoch: {epoch+1}/{epochs} - Train loss {epoch_train_loss:.3f} - Train accuracy {epoch_train_accuracy:.3f} - Valid Loss {epoch_valid_loss:.3f} - Valid accuracy {epoch_valid_accuracy:.3f}\") history = { \"loss\": train_loss, \"accuracy\": train_accuracy, \"val_loss\": valid_loss, \"val_accuracy\": valid_accuracy, } return history \"\"\"### Datos Utilizaremos como dataset canciones de bandas de habla inglÃ©s. \"\"\" import os import platform if not os.path.exists(\\'./songs_dataset\\'): if not os.path.exists(\\'songs_dataset.zip\\'): if platform.system() == \\'Windows\\': os.system(\\'curl -o songs_dataset.zip https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/datasets/songs_dataset.zip\\') else: os.system(\\'curl -o songs_dataset.zip https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/raw/main/datasets/songs_dataset.zip\\') os.system(\\'unzip -q songs_dataset.zip\\') else: print(\"El dataset ya se encuentra descargado\") # Posibles bandas os.listdir(\"./songs_dataset/\") # Armar el dataset utilizando salto de lÃ\\xadnea para separar las oraciones/docs df = pd.read_csv(\\'songs_dataset/beatles.txt\\', sep=\\'/n\\', header=None) df.head() print(\"Cantidad de documentos:\", df.shape[0]) \"\"\"### 1 - Ejemplo de Preprocesamiento - Hay que transformar las oraciones en tokens. - Dichas oraciones hay que ajustarlas al tamaÃ±o fijo de nuestra sentencia de entrada al modelo. - Hay que separar las palabras objetivos (target) que el modelo debe predecir en cada sentencia armada. \"\"\" from torch_helpers import Tokenizer # tool de keras equivalente a ltokenizer de nltk from torch_helpers import text_to_word_sequence # tool de keras equivalente a word_teokenize de nltk from torch_helpers import pad_sequences # tool de keras qye se utilizarÃ¡ para padding # largo de la secuencia, incluye seq input + word output train_len = 4 # Ejemplo de como transformar una oraciÃ³n a tokens usando keras text = df.loc[0,0] text tokens = text_to_word_sequence(text) # entran oraciones -> salen vectores de N posiciones (tokens) tokens \"\"\"1.1 - Transformar las oraciones en secuencias (tokens) de palabras\"\"\" # Recorrer todas las filas y transformar las oraciones # en secuencias de palabras sentence_tokens = [] for _, row in df[:None].iterrows(): sentence_tokens.append(text_to_word_sequence(row[0])) # Demos un vistazo sentence_tokens[:2] # CÃ³digo para hacer el desfazaje de las palabras # segÃºn el train_len text_sequences = [] for i in range(train_len, len(tokens)): seq = tokens[i-train_len:i] text_sequences.append(seq) # Demos un vistazo a nuestros vectores para entrenar el modelo # seq_input + output text_sequences \"\"\"1.2 - Crear los vectores de palabras (word2vec) Ahora necesitamos pasarlos a nÃºmeros para que lo entienda la red y separar input de output. - El Input seran integers (word2vec) - Mientras que el output serÃ¡ one hot encodeado (labels) del tamaÃ±o del vocabulario \"\"\" tok = Tokenizer() # El tokeinzer \"aprende\" las palabras que se usaran # Se construye (fit) una vez por proyecto, se aplica N veces (tal cual un encoder) tok.fit_on_texts(text_sequences) # Convertimos las palabras a nÃºmeros # entran palabras -> salen nÃºmeros sequences = tok.texts_to_sequences(text_sequences) # Ahora sequences tiene los nÃºmeros \"ID\", largo 4 sequences # Cantidad de casos (doc) de entrada print(tok.document_count) # Cantidad de palabras distintas print(len(tok.word_counts)) # El Ã\\xadndice para cada palabra # El sistema las ordena de las mÃ¡s populares a las menos populares print(tok.word_index) # Cantidad de veces quea aparece cada palabra en cada \"documento\" # (1 documento = 1 caso de entrada) print(tok.word_docs) \"\"\"### 2 - Preprocesamiento completo Debemos realizar los mismos pasos que en el ejemplo anterior, pero antes de eso debemos transformar ese dataset de filas de oraciones en un texto completo continuo para poder extraer el vocabulario. \"\"\" # Vistazo a las primeras filas df.loc[:15,0] # Concatenamos todos los rows en un solo valor corpus = df.apply(lambda row: \\' \\'.join(row.values.astype(str)), axis=0)[0] corpus # Transformar el corpus a tokens tokens=text_to_word_sequence(corpus) # Vistazo general de los primeros tokens tokens[:20] print(\"Cantidad de tokens en el corpus:\", len(tokens)) # CÃ³digo para hacer el desfazaje de las palabras # segÃºn el train_len text_sequences = [] for i in range(train_len, len(tokens)): seq = tokens[i-train_len:i] text_sequences.append(seq) # Demos un vistazo a nuestros vectores para entrenar el modelo text_sequences[:20] # Proceso de tokenizacion tok = Tokenizer() tok.fit_on_texts(text_sequences) # Convertimos las palabras a nÃºmeros # entran palabras -> salen nÃºmeros sequences = tok.texts_to_sequences(text_sequences) # Damos un vistazo sequences[:20] print(\"Cantidad de rows del dataset:\", len(sequences)) \"\"\"### 3 - Input y target\"\"\" # Con numpy es muy fÃ¡cil realizar el slicing de vectores ex = np.array([[1,2,3,4],[5,6,7,8]]) ex # Con numpy es muy fÃ¡cil realizar el slicing de vectores print(\"Dimension:\", ex.shape) print(\"Todos los elementos:\", ex) print(\"Todos los elementos menos el Ãºltimo:\", ex[:, :-1]) input = ex[:,:-1] # todos los rows, menos la ultima col target = ex[:, -1] # Ãºltima col de cada row print(\"Input:\", input) print(\"Target:\", target) arr_sequences = np.array(sequences) x_data = arr_sequences[:,:-1] y_data_int = arr_sequences[:,-1] # aÃºn falta el oneHotEncoder print(x_data.shape) print(y_data_int.shape) # Palabras del vocabulario tok.index_word # Cantidad de palabras en el vocabulario vocab_size = len(tok.word_counts) vocab_size # Â¡Ojo! y_data_int comienza en \"1\" en vez de \"0\" # valor minimo: min(y_data_int) # Hay que restar 1 y_data_int_offset = y_data_int - 1 min(y_data_int_offset) class Data(Dataset): def __init__(self, x, y): # Convertir los arrays de numpy a tensores. # pytorch espera en general entradas 32bits self.x = torch.from_numpy(x.astype(np.int32)) # Transformar los datos a oneHotEncoding # la loss function esperan la salida float self.y = F.one_hot(torch.from_numpy(y), num_classes=vocab_size).float() self.len = self.y.shape[0] def __getitem__(self,index): return self.x[index], self.y[index] def __len__(self): return self.len data_set = Data(x_data, y_data_int_offset) input_size = data_set.x.shape[1] print(\"input_size:\", input_size) output_dim = data_set.y.shape[1] print(\"Output dim\", output_dim) torch.manual_seed(42) valid_set_size = int(data_set.len * 0.2) train_set_size = data_set.len - valid_set_size train_set = torch.utils.data.Subset(data_set, range(train_set_size)) valid_set = torch.utils.data.Subset(data_set, range(train_set_size, data_set.len)) print(\"TamaÃ±o del conjunto de entrenamiento:\", len(train_set)) print(\"TamaÃ±o del conjunto de validacion:\", len(valid_set)) train_batch_size = 32 train_loader = torch.utils.data.DataLoader(train_set, batch_size=train_batch_size, shuffle=True) valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=len(valid_set), shuffle=False) \"\"\"### 4 - Entrenar el modelo\"\"\" class Model1(nn.Module): def __init__(self, vocab_size, output_dim): super().__init__() # num_embeddings = vocab_size --> 1628 palabras distintas + 1 para padding o UNK # embedding_dim = 5 --> crear embeddings de tamaÃ±o 5 (tamaÃ±o variable y ajustable) self.lstm_size = 64 self.num_layers = 2 self.embedding = nn.Embedding(num_embeddings=vocab_size+1, embedding_dim=5, padding_idx=0) self.lstm1 = nn.LSTM(input_size=5, hidden_size=self.lstm_size, batch_first=True, num_layers=self.num_layers, dropout=0.2) # LSTM layer self.fc1 = nn.Linear(in_features=self.lstm_size, out_features=32) # Fully connected layer self.fc2 = nn.Linear(in_features=32, out_features=output_dim) # Fully connected layer self.relu = nn.ReLU() self.softmax = nn.Softmax(dim=1) # normalize in dim 1 def forward(self, x, prev_state=None): if prev_state is None: # En cada nueva inferencia reinicio el hidden state # de la LSTM al menos que sea pasado por parÃ¡metro el # elstado de previo # Esta acciÃ³n se realiza especialmente para que # el hidden_state de la Ãºltima inferencia no afecte # a la siguiente batch_size = x.shape[0] #(batch, seq_size) prev_state = self.init_hidden(batch_size) out = self.embedding(x) lstm_output, (ht, ct) = self.lstm1(out, prev_state) out = self.relu(self.fc1(lstm_output[:,-1,:])) # take last output (last seq) out = self.softmax(self.fc2(out)) return out def init_hidden(self, batch_size): return (torch.zeros(self.num_layers, batch_size, self.lstm_size), torch.zeros(self.num_layers, batch_size, self.lstm_size)) model1 = Model1(vocab_size=vocab_size, output_dim=output_dim) # Crear el optimizador la una funciÃ³n de error model1_optimizer = torch.optim.Adam(model1.parameters(), lr=0.001) model1_criterion = torch.nn.CrossEntropyLoss()  # Para clasificaciÃ³n multi categÃ³rica # Por defecto torchinfo testea el modelo con torch.FloatTensor summary(model1, input_size=(1, input_size), dtypes=[\\'torch.IntTensor\\'], device=torch.device(\\'cpu\\')) # otra posibilidad #summary(model1, input_data=data_set[0][0].unsqueeze(0)) history1 = train(model1, train_loader, valid_loader, model1_optimizer, model1_criterion, epochs=100 ) epoch_count = range(1, len(history1[\\'accuracy\\']) + 1) sns.lineplot(x=epoch_count,  y=history1[\\'accuracy\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history1[\\'val_accuracy\\'], label=\\'valid\\') plt.show() \"\"\"__Es importante__ destacar que en este ejemplo estamos entrenando nuestro propios Embeddings y para ello se requiere mucha data. En los ejemplos que realizaremos de aquÃ\\xad en mÃ¡s utilizaremos mÃ¡s datos, embeddings pre-enternados o modelos pre-entrenados. ### 5 - PredicciÃ³n de prÃ³xima palabra \"\"\" # pad_sequences # Si la secuencia de entrada supera al input_seq_len (3) se trunca # Si la secuencia es mÃ¡s corta se agregna ceros al comienzo # Se utilizarÃ¡ gradio para ensayar el modelo # Herramienta poderosa para crear interfaces rÃ¡pidas para ensayar modelos # https://gradio.app/ import sys !{sys.executable} -m pip install gradio --quiet import gradio as gr def model_response(human_text): # Encodeamos encoded = tok.texts_to_sequences([human_text])[0] # Si tienen distinto largo encoded = pad_sequences([encoded], maxlen=3, padding=\\'pre\\') # Transformo a tensor tensor = torch.from_numpy(encoded.astype(np.int32)) # PredicciÃ³n softmax y_hat = model1(tensor).argmax(axis=-1) # Debemos buscar en el vocabulario la palabra # que corresopnde al indice (y_hat) predicho por le modelo out_word = \\'\\' for word, index in tok.word_index.items(): if index == y_hat: out_word = word break # Agrego la palabra a la frase predicha return human_text + \\' \\' + out_word iface = gr.Interface( fn=model_response, inputs=[\"textbox\"], outputs=\"text\", layout=\"vertical\") iface.launch(debug=True) \"\"\"### 6 - GeneraciÃ³n de secuencias nuevas\"\"\" def generate_seq(model, tokenizer, seed_text, max_length, n_words): \"\"\" Exec model sequence prediction Args: model (keras): modelo entrenado tokenizer (keras tokenizer): tonenizer utilizado en el preprocesamiento seed_text (string): texto de entrada (input_seq) max_length (int): mÃ¡xima longitud de la sequencia de entrada n_words (int): nÃºmeros de palabras a agregar a la sequencia de entrada returns: output_text (string): sentencia con las \"n_words\" agregadas \"\"\" output_text = seed_text # generate a fixed number of words for _ in range(n_words): # Encodeamos encoded = tokenizer.texts_to_sequences([output_text])[0] # Si tienen distinto largo encoded = pad_sequences([encoded], maxlen=max_length, padding=\\'pre\\') # Transformo a tensor tensor = torch.from_numpy(encoded.astype(np.int32)) # PredicciÃ³n softmax y_hat = model1(tensor).argmax(axis=-1) # Vamos concatenando las predicciones out_word = \\'\\' # Debemos buscar en el vocabulario la palabra # que corresopnde al indice (y_hat) predicho por le modelo for word, index in tokenizer.word_index.items(): if index == y_hat: out_word = word break # Agrego las palabras a la frase predicha output_text += \\' \\' + out_word return output_text input_text=\\'hey jude don\\\\\\'t\\' generate_seq(model1, tok, input_text, max_length=3, n_words=2) \"\"\"### 7 - Conclusiones El modelo entrenado tuvo un muy mail desempeÃ±o en el entrenamiento ademÃ¡s de overfitting. Cuestiones que podrÃ\\xadan mejorarse: - Agregar mÃ¡s capas o neuronaes - Incrementar la cantidad de Ã©pocas - Agregar BRNN Es importante destacar que en este ejemplo estamos entrenando nuestro propios Embeddings, y para ello se requiere mucha data. En los ejemplos que realizaremos de aquÃ\\xad en mÃ¡s utilizaremos mÃ¡s datos, embeddings pre-enternados o modelos pre-entrenados. \"\"\" import re import numpy as np import matplotlib.pyplot as plt import seaborn as sns import torch import torch.nn.functional as F import torch.nn as nn from torch.utils.data import Dataset, DataLoader # torchsummar actualmente tiene un problema con las LSTM, por eso # se utiliza torchinfo, un fork del proyecto original con el bug solucionado !pip3 install torchinfo from torchinfo import summary import os import platform if os.access(\\'torch_helpers.py\\', os.F_OK) is False: if platform.system() == \\'Windows\\': !curl !wget https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py > torch_helpers.py else: !wget torch_helpers.py https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py def train(model, train_loader, valid_loader, optimizer, criterion, epochs=100): # Defino listas para realizar graficas de los resultados train_loss = [] valid_loss = [] # Defino mi loop de entrenamiento for epoch in range(epochs): epoch_train_loss = 0.0 epoch_train_accuracy = 0.0 for train_data, train_target in train_loader: # Seteo los gradientes en cero ya que, por defecto, PyTorch # los va acumulando optimizer.zero_grad() output = model(train_data) # Computo el error de la salida comparando contra las etiquetas loss = criterion(output, train_target) # Almaceno el error del batch para luego tener el error promedio de la epoca epoch_train_loss += loss.item() # Computo el nuevo set de gradientes a lo largo de toda la red loss.backward() # Realizo el paso de optimizacion actualizando los parametros de toda la red optimizer.step() # Calculo la media de error para la epoca de entrenamiento. # La longitud de train_loader es igual a la cantidad de batches dentro de una epoca. epoch_train_loss = epoch_train_loss / len(train_loader) train_loss.append(epoch_train_loss) # Realizo el paso de validaciÃ³n computando error y accuracy, y # almacenando los valores para imprimirlos y graficarlos valid_data, valid_target = iter(valid_loader).next() output = model(valid_data) epoch_valid_loss = criterion(output, valid_target).item() valid_loss.append(epoch_valid_loss) print(f\"Epoch: {epoch+1}/{epochs} - Train loss {epoch_train_loss:.3f} - Valid Loss {epoch_valid_loss:.3f}\") history = { \"loss\": train_loss, \"val_loss\": valid_loss, } return history # Generar datos sintÃ©ticos X = list() y = list() X = [x+1 for x in range(20)] # \"y\" (target) se obtiene como cada dato de entrada multiplicado por 15 y = [x * 15 for x in X] print(\"datos X:\", X) print(\"datos y:\", y) # Cada dato X lo transformarmos en una matriz de 1 fila 1 columna (1x1) X = np.array(X).reshape(len(X), 1, 1) print(\"datos X:\", X) # (batch size, seq_len, input_size) X.shape y = np.asanyarray(y) y.shape class Data(Dataset): def __init__(self, x, y): # Convertir los arrays de numpy a tensores. # pytorch espera en general entradas 32bits self.x = torch.from_numpy(x.astype(np.float32)) # las loss unfction esperan la salida float self.y = torch.from_numpy(y.astype(np.int32)).float().view(-1, 1) self.len = self.y.shape[0] def __getitem__(self,index): return self.x[index], self.y[index] def __len__(self): return self.len data_set = Data(X, y) input_dim = data_set.x.shape[1:] seq_length = input_dim[0] input_size = input_dim[1] print(\"Input dim\", input_dim) print(\"seq_length:\", seq_length) print(\"input_size:\", input_size) output_dim = data_set.y.shape[1] print(\"Output dim\", output_dim) data_set.x.shape data_set.y.shape torch.manual_seed(42) valid_set_size = int(data_set.len * 0.2) train_set_size = data_set.len - valid_set_size # Cuando trabajmos con una serie temporal no mezclamos (shuffle) los datos train_set = torch.utils.data.Subset(data_set, range(train_set_size)) valid_set = torch.utils.data.Subset(data_set, range(train_set_size, data_set.len)) print(\"TamaÃ±o del conjunto de entrenamiento:\", len(train_set)) print(\"TamaÃ±o del conjunto de validacion:\", len(valid_set)) train_loader = torch.utils.data.DataLoader(train_set, batch_size=len(train_set), shuffle=False) valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=len(valid_set), shuffle=False) \"\"\"### 2 - Entrenar el modelo (RNN y LSTM)\"\"\" from torch_helpers import CustomRNN class Model1(nn.Module): def __init__(self, input_size, output_dim): super().__init__() #self.rnn1 = nn.RNN(input_size=input_size, hidden_size=64, batch_first=True) # RNN layer # Utilizamos la CustomRNN ya que para series temporales suele funcionar mejor # la activacion \"relu\" en las RNN en vez de la \"tanh\", pero por defecto la # layer de Pytorch RNN no permite modificar la funcion de activacion #self.rnn1 = CustomRNN(input_size=input_size, hidden_size=64) # RNN layer self.rnn1 = CustomRNN(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # RNN layer self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer def forward(self, x): lstm_output, _ = self.rnn1(x) out = self.fc(lstm_output[:,-1,:]) # take last output (last seq) return out model1 = Model1(input_size=input_size, output_dim=output_dim) # Crear el optimizador la una funciÃ³n de error model1_optimizer = torch.optim.Adam(model1.parameters(), lr=0.01) model1_criterion = nn.MSELoss()  # mean squared error summary(model1, input_size=(1, seq_length, input_size)) history1 = train(model1, train_loader, valid_loader, model1_optimizer, model1_criterion, epochs=500 ) epoch_count = range(1, len(history1[\\'loss\\']) + 1) sns.lineplot(x=epoch_count,  y=history1[\\'loss\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history1[\\'val_loss\\'], label=\\'valid\\') plt.show() # Ensayo # x = 30 # y_test = x * 15 x_test = 30 y_test = x_test * 15 test_input = np.array([x_test]) test_input = test_input.reshape((1, seq_length, input_size)) test_input = torch.from_numpy(test_input.astype(np.float32)) test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1) y_hat = model1(test_input) print(\"y_test:\", y_test) print(\"y_hat:\", y_hat) loss = model1_criterion(y_hat, test_target).item() print(\"loss:\", loss) from torch_helpers import CustomLSTM class Model2(nn.Module): def __init__(self, input_size, output_dim): super().__init__() #self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=64, batch_first=True) # LSTM layer # Utilizamos la CustomLSTM ya que para series temporales suele funcionar mejor # la activacion \"relu\" en las LSTM en vez de la \"tanh\", pero por defecto la # layer de Pytorch LSTM no permite modificar la funcion de activacion self.lstm1 = CustomLSTM(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # LSTM layer self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer def forward(self, x): lstm_output, _ = self.lstm1(x) out = self.fc(lstm_output[:,-1,:]) # take last output (last seq) return out model2 = Model2(input_size=input_size, output_dim=output_dim) # Crear el optimizador la una funciÃ³n de error model2_optimizer = torch.optim.Adam(model2.parameters(), lr=0.01) model2_criterion = nn.MSELoss()  # mean squared error summary(model2, input_size=(1, seq_length, input_size)) history2 = train(model2, train_loader, valid_loader, model2_optimizer, model2_criterion, epochs=500 ) epoch_count = range(1, len(history2[\\'loss\\']) + 1) sns.lineplot(x=epoch_count,  y=history2[\\'loss\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history2[\\'val_loss\\'], label=\\'valid\\') plt.show() # Ensayo # x = 30 # y_test = x * 15 x_test = 30 y_test = x_test * 15 test_input = np.array([x_test]) test_input = test_input.reshape((1, seq_length, input_size)) test_input = torch.from_numpy(test_input.astype(np.float32)) test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1) y_hat = model2(test_input) print(\"y_test:\", y_test) print(\"y_hat:\", y_hat) loss = model2_criterion(y_hat, test_target).item() print(\"loss:\", loss) \"\"\"Se puede observar que para un problema tan simple como este no hay mucha diferencia entre utilizar una RNN o LSTM. La LSTM tiene muchos mÃ¡s parÃ¡metros que la RNN. ### 3 - Multi-layer LSTM \"\"\" from torch_helpers import CustomLSTM # En esta oportunidad se utilizarÃ¡n dos layer LSTM class Model3(nn.Module): def __init__(self, input_size, output_dim): super().__init__() self.lstm1 = CustomLSTM(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # LSTM layer self.lstm2 = CustomLSTM(input_size=64, hidden_size=64, activation=nn.ReLU()) # LSTM layer self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer def forward(self, x): lstm_output, _ = self.lstm1(x) lstm_output, _ = self.lstm2(lstm_output) out = self.fc(lstm_output[:,-1,:]) # take last output (last seq) return out model3 = Model3(input_size=input_size, output_dim=output_dim) # Crear el optimizador la una funciÃ³n de error model3_optimizer = torch.optim.Adam(model3.parameters(), lr=0.01) model3_criterion = nn.MSELoss()  # mean squared error summary(model3, input_size=(1, seq_length, input_size)) history3 = train(model3, train_loader, valid_loader, model3_optimizer, model3_criterion, epochs=500 ) epoch_count = range(1, len(history3[\\'loss\\']) + 1) sns.lineplot(x=epoch_count,  y=history3[\\'loss\\'], label=\\'train\\') sns.lineplot(x=epoch_count,  y=history3[\\'val_loss\\'], label=\\'valid\\') plt.show() # Ensayo # x = 30 # y_test = x * 15 x_test = 30 y_test = x_test * 15 test_input = np.array([x_test]) test_input = test_input.reshape((1, seq_length, input_size)) test_input = torch.from_numpy(test_input.astype(np.float32)) test_target = torch.from_numpy(np.array(y_test).astype(np.int32)).float().view(-1, 1) y_hat = model3(test_input) print(\"y_test:\", y_test) print(\"y_hat:\", y_hat) loss = model3_criterion(y_hat, test_target).item() print(\"loss:\", loss) \"\"\"### 4 - ConclusiÃ³n El resultado alcanzado es bueno pero podrÃ\\xada mejorarse agregando mÃ¡s layer LSTM o mÃ¡s layer fully connected. \"\"\"'"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenamos todos los rows en un solo valor\n",
    "temp_corpus = df.apply(lambda row: ' '.join(row.values.astype(str)), axis=0)[0]\n",
    "temp_corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para poder ayudar a identificar los caracteres, aplico espacios entre los caracteres que no son letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import re import numpy as np import matplotlib . pyplot as plt import seaborn as sns import torch import torch . nn . functional as F import torch . nn as nn from torch . utils . data import Dataset ,  DataLoader  #  torchsummar actualmente tiene un problema con las LSTM ,  por eso  #  se utiliza torchinfo ,  un fork del proyecto original con el bug solucionado  ! pip 3  install torchinfo from torchinfo import summary import os import platform if os . access (  ' torch _ helpers . py '  ,  os . F _ OK )  is False :  if platform . system (  )   =  =   ' Windows '  :   ! curl  ! wget https :  /  / raw . githubusercontent . com / FIUBA - Posgrado - Inteligencia - Artificial / procesamiento _ lenguaje _ natural / main / scripts / torch _ helpers . py  >  torch _ helpers . py else :   ! wget torch _ helpers . py https :  /  / raw . githubusercontent . com / FIUBA - Posgrado - Inteligencia - Artificial / procesamiento _ lenguaje _ natural / main / scripts / torch _ helpers . py def train ( model ,  train _ loader ,  valid _ loader ,  optimizer ,  criterion ,  epochs =  1  0  0  )  :   #  Defino listas para realizar graficas de los resultados train _ loss  =   [  ]  valid _ loss  =   [  ]   #  Defino mi loop de entrenamiento for epoch in range ( epochs )  :  epoch _ train _ loss  =   0  .  0  epoch _ train _ accuracy  =   0  .  0  for train _ data ,  train _ target in train _ loader :   #  Seteo los gradientes en cero ya que ,  por defecto ,  PyTorch  #  los va acumulando optimizer . zero _ grad (  )  output  =  model ( train _ data )   #  Computo el error de la salida comparando contra las etiquetas loss  =  criterion ( output ,  train _ target )   #  Almaceno el error del batch para luego tener el error promedio de la epoca epoch _ train _ loss  +  =  loss . item (  )   #  Computo el nuevo set de gradientes a lo largo de toda la red loss . backward (  )   #  Realizo el paso de optimizacion actualizando los parametros de toda la red optimizer . step (  )   #  Calculo la media de error para la epoca de entrenamiento .   #  La longitud de train _ loader es igual a la cantidad de batches dentro de una epoca .  epoch _ train _ loss  =  epoch _ train _ loss  /  len ( train _ loader )  train _ loss . append ( epoch _ train _ loss )   #  Realizo el paso de validaci Ã  ³ n computando error y accuracy ,  y  #  almacenando los valores para imprimirlos y graficarlos valid _ data ,  valid _ target  =  iter ( valid _ loader )  . next (  )  output  =  model ( valid _ data )  epoch _ valid _ loss  =  criterion ( output ,  valid _ target )  . item (  )  valid _ loss . append ( epoch _ valid _ loss )  print ( f \" Epoch :   { epoch +  1  }  /  { epochs }   -  Train loss  { epoch _ train _ loss :  .  3 f }   -  Valid Loss  { epoch _ valid _ loss :  .  3 f }  \"  )  history  =   {   \" loss \"  :  train _ loss ,   \" val _ loss \"  :  valid _ loss ,   }  return history  #  Generar datos sint Ã  © ticos X  =  list (  )  y  =  list (  )  X  =   [ x +  1  for x in range (  2  0  )  ]   #   \" y \"   ( target )  se obtiene como cada dato de entrada multiplicado por  1  5  y  =   [ x  *   1  5  for x in X ]  print (  \" datos X :  \"  ,  X )  print (  \" datos y :  \"  ,  y )   #  Cada dato X lo transformarmos en una matriz de  1  fila  1  columna  (  1 x 1  )  X  =  np . array ( X )  . reshape ( len ( X )  ,   1  ,   1  )  print (  \" datos X :  \"  ,  X )   #   ( batch size ,  seq _ len ,  input _ size )  X . shape y  =  np . asanyarray ( y )  y . shape class Data ( Dataset )  :  def  _  _ init _  _  ( self ,  x ,  y )  :   #  Convertir los arrays de numpy a tensores .   #  pytorch espera en general entradas  3  2 bits self . x  =  torch . from _ numpy ( x . astype ( np . float 3  2  )  )   #  las loss unfction esperan la salida float self . y  =  torch . from _ numpy ( y . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  self . len  =  self . y . shape [  0  ]  def  _  _ getitem _  _  ( self , index )  :  return self . x [ index ]  ,  self . y [ index ]  def  _  _ len _  _  ( self )  :  return self . len data _ set  =  Data ( X ,  y )  input _ dim  =  data _ set . x . shape [  1  :  ]  seq _ length  =  input _ dim [  0  ]  input _ size  =  input _ dim [  1  ]  print (  \" Input dim \"  ,  input _ dim )  print (  \" seq _ length :  \"  ,  seq _ length )  print (  \" input _ size :  \"  ,  input _ size )  output _ dim  =  data _ set . y . shape [  1  ]  print (  \" Output dim \"  ,  output _ dim )  data _ set . x . shape data _ set . y . shape torch . manual _ seed (  4  2  )  valid _ set _ size  =  int ( data _ set . len  *   0  .  2  )  train _ set _ size  =  data _ set . len  -  valid _ set _ size  #  Cuando trabajmos con una serie temporal no mezclamos  ( shuffle )  los datos train _ set  =  torch . utils . data . Subset ( data _ set ,  range ( train _ set _ size )  )  valid _ set  =  torch . utils . data . Subset ( data _ set ,  range ( train _ set _ size ,  data _ set . len )  )  print (  \" Tama Ã  ± o del conjunto de entrenamiento :  \"  ,  len ( train _ set )  )  print (  \" Tama Ã  ± o del conjunto de validacion :  \"  ,  len ( valid _ set )  )  train _ loader  =  torch . utils . data . DataLoader ( train _ set ,  batch _ size = len ( train _ set )  ,  shuffle = False )  valid _ loader  =  torch . utils . data . DataLoader ( valid _ set ,  batch _ size = len ( valid _ set )  ,  shuffle = False )   \"  \"  \"  #  #  #   2   -  Entrenar el modelo  ( RNN y LSTM )  \"  \"  \"  from torch _ helpers import CustomRNN class Model 1  ( nn . Module )  :  def  _  _ init _  _  ( self ,  input _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )   # self . rnn 1   =  nn . RNN ( input _ size = input _ size ,  hidden _ size =  6  4  ,  batch _ first = True )   #  RNN layer  #  Utilizamos la CustomRNN ya que para series temporales suele funcionar mejor  #  la activacion  \" relu \"  en las RNN en vez de la  \" tanh \"  ,  pero por defecto la  #  layer de Pytorch RNN no permite modificar la funcion de activacion  # self . rnn 1   =  CustomRNN ( input _ size = input _ size ,  hidden _ size =  6  4  )   #  RNN layer self . rnn 1   =  CustomRNN ( input _ size = input _ size ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  RNN layer self . fc  =  nn . Linear ( in _ features =  6  4  ,  out _ features = output _ dim )   #    #  Fully connected layer def forward ( self ,  x )  :  lstm _ output ,   _   =  self . rnn 1  ( x )  out  =  self . fc ( lstm _ output [  :  ,  -  1  ,  :  ]  )   #  take last output  ( last seq )  return out model 1   =  Model 1  ( input _ size = input _ size ,  output _ dim = output _ dim )   #  Crear el optimizador la una funci Ã  ³ n de error model 1  _ optimizer  =  torch . optim . Adam ( model 1  . parameters (  )  ,  lr =  0  .  0  1  )  model 1  _ criterion  =  nn . MSELoss (  )    #  mean squared error summary ( model 1  ,  input _ size =  (  1  ,  seq _ length ,  input _ size )  )  history 1   =  train ( model 1  ,  train _ loader ,  valid _ loader ,  model 1  _ optimizer ,  model 1  _ criterion ,  epochs =  5  0  0   )  epoch _ count  =  range (  1  ,  len ( history 1  [  ' loss '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 1  [  ' loss '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 1  [  ' val _ loss '  ]  ,  label =  ' valid '  )  plt . show (  )   #  Ensayo  #  x  =   3  0   #  y _ test  =  x  *   1  5  x _ test  =   3  0  y _ test  =  x _ test  *   1  5  test _ input  =  np . array (  [ x _ test ]  )  test _ input  =  test _ input . reshape (  (  1  ,  seq _ length ,  input _ size )  )  test _ input  =  torch . from _ numpy ( test _ input . astype ( np . float 3  2  )  )  test _ target  =  torch . from _ numpy ( np . array ( y _ test )  . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  y _ hat  =  model 1  ( test _ input )  print (  \" y _ test :  \"  ,  y _ test )  print (  \" y _ hat :  \"  ,  y _ hat )  loss  =  model 1  _ criterion ( y _ hat ,  test _ target )  . item (  )  print (  \" loss :  \"  ,  loss )  from torch _ helpers import CustomLSTM class Model 2  ( nn . Module )  :  def  _  _ init _  _  ( self ,  input _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )   # self . lstm 1   =  nn . LSTM ( input _ size = input _ size ,  hidden _ size =  6  4  ,  batch _ first = True )   #  LSTM layer  #  Utilizamos la CustomLSTM ya que para series temporales suele funcionar mejor  #  la activacion  \" relu \"  en las LSTM en vez de la  \" tanh \"  ,  pero por defecto la  #  layer de Pytorch LSTM no permite modificar la funcion de activacion self . lstm 1   =  CustomLSTM ( input _ size = input _ size ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  LSTM layer self . fc  =  nn . Linear ( in _ features =  6  4  ,  out _ features = output _ dim )   #    #  Fully connected layer def forward ( self ,  x )  :  lstm _ output ,   _   =  self . lstm 1  ( x )  out  =  self . fc ( lstm _ output [  :  ,  -  1  ,  :  ]  )   #  take last output  ( last seq )  return out model 2   =  Model 2  ( input _ size = input _ size ,  output _ dim = output _ dim )   #  Crear el optimizador la una funci Ã  ³ n de error model 2  _ optimizer  =  torch . optim . Adam ( model 2  . parameters (  )  ,  lr =  0  .  0  1  )  model 2  _ criterion  =  nn . MSELoss (  )    #  mean squared error summary ( model 2  ,  input _ size =  (  1  ,  seq _ length ,  input _ size )  )  history 2   =  train ( model 2  ,  train _ loader ,  valid _ loader ,  model 2  _ optimizer ,  model 2  _ criterion ,  epochs =  5  0  0   )  epoch _ count  =  range (  1  ,  len ( history 2  [  ' loss '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 2  [  ' loss '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 2  [  ' val _ loss '  ]  ,  label =  ' valid '  )  plt . show (  )   #  Ensayo  #  x  =   3  0   #  y _ test  =  x  *   1  5  x _ test  =   3  0  y _ test  =  x _ test  *   1  5  test _ input  =  np . array (  [ x _ test ]  )  test _ input  =  test _ input . reshape (  (  1  ,  seq _ length ,  input _ size )  )  test _ input  =  torch . from _ numpy ( test _ input . astype ( np . float 3  2  )  )  test _ target  =  torch . from _ numpy ( np . array ( y _ test )  . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  y _ hat  =  model 2  ( test _ input )  print (  \" y _ test :  \"  ,  y _ test )  print (  \" y _ hat :  \"  ,  y _ hat )  loss  =  model 2  _ criterion ( y _ hat ,  test _ target )  . item (  )  print (  \" loss :  \"  ,  loss )   \"  \"  \" Se puede observar que para un problema tan simple como este no hay mucha diferencia entre utilizar una RNN o LSTM .  La LSTM tiene muchos m Ã  ¡ s par Ã  ¡ metros que la RNN .   #  #  #   3   -  Multi - layer LSTM  \"  \"  \"  from torch _ helpers import CustomLSTM  #  En esta oportunidad se utilizar Ã  ¡ n dos layer LSTM class Model 3  ( nn . Module )  :  def  _  _ init _  _  ( self ,  input _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )  self . lstm 1   =  CustomLSTM ( input _ size = input _ size ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  LSTM layer self . lstm 2   =  CustomLSTM ( input _ size =  6  4  ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  LSTM layer self . fc  =  nn . Linear ( in _ features =  6  4  ,  out _ features = output _ dim )   #    #  Fully connected layer def forward ( self ,  x )  :  lstm _ output ,   _   =  self . lstm 1  ( x )  lstm _ output ,   _   =  self . lstm 2  ( lstm _ output )  out  =  self . fc ( lstm _ output [  :  ,  -  1  ,  :  ]  )   #  take last output  ( last seq )  return out model 3   =  Model 3  ( input _ size = input _ size ,  output _ dim = output _ dim )   #  Crear el optimizador la una funci Ã  ³ n de error model 3  _ optimizer  =  torch . optim . Adam ( model 3  . parameters (  )  ,  lr =  0  .  0  1  )  model 3  _ criterion  =  nn . MSELoss (  )    #  mean squared error summary ( model 3  ,  input _ size =  (  1  ,  seq _ length ,  input _ size )  )  history 3   =  train ( model 3  ,  train _ loader ,  valid _ loader ,  model 3  _ optimizer ,  model 3  _ criterion ,  epochs =  5  0  0   )  epoch _ count  =  range (  1  ,  len ( history 3  [  ' loss '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 3  [  ' loss '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 3  [  ' val _ loss '  ]  ,  label =  ' valid '  )  plt . show (  )   #  Ensayo  #  x  =   3  0   #  y _ test  =  x  *   1  5  x _ test  =   3  0  y _ test  =  x _ test  *   1  5  test _ input  =  np . array (  [ x _ test ]  )  test _ input  =  test _ input . reshape (  (  1  ,  seq _ length ,  input _ size )  )  test _ input  =  torch . from _ numpy ( test _ input . astype ( np . float 3  2  )  )  test _ target  =  torch . from _ numpy ( np . array ( y _ test )  . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  y _ hat  =  model 3  ( test _ input )  print (  \" y _ test :  \"  ,  y _ test )  print (  \" y _ hat :  \"  ,  y _ hat )  loss  =  model 3  _ criterion ( y _ hat ,  test _ target )  . item (  )  print (  \" loss :  \"  ,  loss )   \"  \"  \"  #  #  #   4   -  Conclusi Ã  ³ n El resultado alcanzado es bueno pero podr Ã  ­ a mejorarse agregando m Ã  ¡ s layer LSTM o m Ã  ¡ s layer fully connected .   \"  \"  \"  import re import numpy as np import matplotlib . pyplot as plt import seaborn as sns import torch import torch . nn . functional as F import torch . nn as nn from torch . utils . data import Dataset ,  DataLoader  #  torchsummar actualmente tiene un problema con las LSTM ,  por eso  #  se utiliza torchinfo ,  un fork del proyecto original con el bug solucionado  ! pip 3  install torchinfo from torchinfo import summary import os import platform if os . access (  ' torch _ helpers . py '  ,  os . F _ OK )  is False :  if platform . system (  )   =  =   ' Windows '  :   ! curl  ! wget https :  /  / raw . githubusercontent . com / FIUBA - Posgrado - Inteligencia - Artificial / procesamiento _ lenguaje _ natural / main / scripts / torch _ helpers . py  >  torch _ helpers . py else :   ! wget torch _ helpers . py https :  /  / raw . githubusercontent . com / FIUBA - Posgrado - Inteligencia - Artificial / procesamiento _ lenguaje _ natural / main / scripts / torch _ helpers . py def train ( model ,  train _ loader ,  valid _ loader ,  optimizer ,  criterion ,  epochs =  1  0  0  )  :   #  Defino listas para realizar graficas de los resultados train _ loss  =   [  ]  valid _ loss  =   [  ]   #  Defino mi loop de entrenamiento for epoch in range ( epochs )  :  epoch _ train _ loss  =   0  .  0  epoch _ train _ accuracy  =   0  .  0  for train _ data ,  train _ target in train _ loader :   #  Seteo los gradientes en cero ya que ,  por defecto ,  PyTorch  #  los va acumulando optimizer . zero _ grad (  )  output  =  model ( train _ data )   #  Computo el error de la salida comparando contra las etiquetas loss  =  criterion ( output ,  train _ target )   #  Almaceno el error del batch para luego tener el error promedio de la epoca epoch _ train _ loss  +  =  loss . item (  )   #  Computo el nuevo set de gradientes a lo largo de toda la red loss . backward (  )   #  Realizo el paso de optimizacion actualizando los parametros de toda la red optimizer . step (  )   #  Calculo la media de error para la epoca de entrenamiento .   #  La longitud de train _ loader es igual a la cantidad de batches dentro de una epoca .  epoch _ train _ loss  =  epoch _ train _ loss  /  len ( train _ loader )  train _ loss . append ( epoch _ train _ loss )   #  Realizo el paso de validaci Ã  ³ n computando error y accuracy ,  y  #  almacenando los valores para imprimirlos y graficarlos valid _ data ,  valid _ target  =  iter ( valid _ loader )  . next (  )  output  =  model ( valid _ data )  epoch _ valid _ loss  =  criterion ( output ,  valid _ target )  . item (  )  valid _ loss . append ( epoch _ valid _ loss )  print ( f \" Epoch :   { epoch +  1  }  /  { epochs }   -  Train loss  { epoch _ train _ loss :  .  3 f }   -  Valid Loss  { epoch _ valid _ loss :  .  3 f }  \"  )  history  =   {   \" loss \"  :  train _ loss ,   \" val _ loss \"  :  valid _ loss ,   }  return history  #  Generar datos sint Ã  © ticos X  =  list (  )  y  =  list (  )  X  =   [ x +  1  for x in range (  2  0  )  ]   #   \" y \"   ( target )  se obtiene como cada dato de entrada multiplicado por  1  5  y  =   [ x  *   1  5  for x in X ]  print (  \" datos X :  \"  ,  X )  print (  \" datos y :  \"  ,  y )   #  Cada dato X lo transformarmos en una matriz de  1  fila  1  columna  (  1 x 1  )  X  =  np . array ( X )  . reshape ( len ( X )  ,   1  ,   1  )  print (  \" datos X :  \"  ,  X )   #   ( batch size ,  seq _ len ,  input _ size )  X . shape y  =  np . asanyarray ( y )  y . shape class Data ( Dataset )  :  def  _  _ init _  _  ( self ,  x ,  y )  :   #  Convertir los arrays de numpy a tensores .   #  pytorch espera en general entradas  3  2 bits self . x  =  torch . from _ numpy ( x . astype ( np . float 3  2  )  )   #  las loss unfction esperan la salida float self . y  =  torch . from _ numpy ( y . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  self . len  =  self . y . shape [  0  ]  def  _  _ getitem _  _  ( self , index )  :  return self . x [ index ]  ,  self . y [ index ]  def  _  _ len _  _  ( self )  :  return self . len data _ set  =  Data ( X ,  y )  input _ dim  =  data _ set . x . shape [  1  :  ]  seq _ length  =  input _ dim [  0  ]  input _ size  =  input _ dim [  1  ]  print (  \" Input dim \"  ,  input _ dim )  print (  \" seq _ length :  \"  ,  seq _ length )  print (  \" input _ size :  \"  ,  input _ size )  output _ dim  =  data _ set . y . shape [  1  ]  print (  \" Output dim \"  ,  output _ dim )  data _ set . x . shape data _ set . y . shape torch . manual _ seed (  4  2  )  valid _ set _ size  =  int ( data _ set . len  *   0  .  2  )  train _ set _ size  =  data _ set . len  -  valid _ set _ size  #  Cuando trabajmos con una serie temporal no mezclamos  ( shuffle )  los datos train _ set  =  torch . utils . data . Subset ( data _ set ,  range ( train _ set _ size )  )  valid _ set  =  torch . utils . data . Subset ( data _ set ,  range ( train _ set _ size ,  data _ set . len )  )  print (  \" Tama Ã  ± o del conjunto de entrenamiento :  \"  ,  len ( train _ set )  )  print (  \" Tama Ã  ± o del conjunto de validacion :  \"  ,  len ( valid _ set )  )  train _ loader  =  torch . utils . data . DataLoader ( train _ set ,  batch _ size = len ( train _ set )  ,  shuffle = False )  valid _ loader  =  torch . utils . data . DataLoader ( valid _ set ,  batch _ size = len ( valid _ set )  ,  shuffle = False )   \"  \"  \"  #  #  #   2   -  Entrenar el modelo  ( RNN y LSTM )  \"  \"  \"  from torch _ helpers import CustomRNN class Model 1  ( nn . Module )  :  def  _  _ init _  _  ( self ,  input _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )   # self . rnn 1   =  nn . RNN ( input _ size = input _ size ,  hidden _ size =  6  4  ,  batch _ first = True )   #  RNN layer  #  Utilizamos la CustomRNN ya que para series temporales suele funcionar mejor  #  la activacion  \" relu \"  en las RNN en vez de la  \" tanh \"  ,  pero por defecto la  #  layer de Pytorch RNN no permite modificar la funcion de activacion  # self . rnn 1   =  CustomRNN ( input _ size = input _ size ,  hidden _ size =  6  4  )   #  RNN layer self . rnn 1   =  CustomRNN ( input _ size = input _ size ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  RNN layer self . fc  =  nn . Linear ( in _ features =  6  4  ,  out _ features = output _ dim )   #    #  Fully connected layer def forward ( self ,  x )  :  lstm _ output ,   _   =  self . rnn 1  ( x )  out  =  self . fc ( lstm _ output [  :  ,  -  1  ,  :  ]  )   #  take last output  ( last seq )  return out model 1   =  Model 1  ( input _ size = input _ size ,  output _ dim = output _ dim )   #  Crear el optimizador la una funci Ã  ³ n de error model 1  _ optimizer  =  torch . optim . Adam ( model 1  . parameters (  )  ,  lr =  0  .  0  1  )  model 1  _ criterion  =  nn . MSELoss (  )    #  mean squared error summary ( model 1  ,  input _ size =  (  1  ,  seq _ length ,  input _ size )  )  history 1   =  train ( model 1  ,  train _ loader ,  valid _ loader ,  model 1  _ optimizer ,  model 1  _ criterion ,  epochs =  5  0  0   )  epoch _ count  =  range (  1  ,  len ( history 1  [  ' loss '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 1  [  ' loss '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 1  [  ' val _ loss '  ]  ,  label =  ' valid '  )  plt . show (  )   #  Ensayo  #  x  =   3  0   #  y _ test  =  x  *   1  5  x _ test  =   3  0  y _ test  =  x _ test  *   1  5  test _ input  =  np . array (  [ x _ test ]  )  test _ input  =  test _ input . reshape (  (  1  ,  seq _ length ,  input _ size )  )  test _ input  =  torch . from _ numpy ( test _ input . astype ( np . float 3  2  )  )  test _ target  =  torch . from _ numpy ( np . array ( y _ test )  . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  y _ hat  =  model 1  ( test _ input )  print (  \" y _ test :  \"  ,  y _ test )  print (  \" y _ hat :  \"  ,  y _ hat )  loss  =  model 1  _ criterion ( y _ hat ,  test _ target )  . item (  )  print (  \" loss :  \"  ,  loss )  from torch _ helpers import CustomLSTM class Model 2  ( nn . Module )  :  def  _  _ init _  _  ( self ,  input _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )   # self . lstm 1   =  nn . LSTM ( input _ size = input _ size ,  hidden _ size =  6  4  ,  batch _ first = True )   #  LSTM layer  #  Utilizamos la CustomLSTM ya que para series temporales suele funcionar mejor  #  la activacion  \" relu \"  en las LSTM en vez de la  \" tanh \"  ,  pero por defecto la  #  layer de Pytorch LSTM no permite modificar la funcion de activacion self . lstm 1   =  CustomLSTM ( input _ size = input _ size ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  LSTM layer self . fc  =  nn . Linear ( in _ features =  6  4  ,  out _ features = output _ dim )   #    #  Fully connected layer def forward ( self ,  x )  :  lstm _ output ,   _   =  self . lstm 1  ( x )  out  =  self . fc ( lstm _ output [  :  ,  -  1  ,  :  ]  )   #  take last output  ( last seq )  return out model 2   =  Model 2  ( input _ size = input _ size ,  output _ dim = output _ dim )   #  Crear el optimizador la una funci Ã  ³ n de error model 2  _ optimizer  =  torch . optim . Adam ( model 2  . parameters (  )  ,  lr =  0  .  0  1  )  model 2  _ criterion  =  nn . MSELoss (  )    #  mean squared error summary ( model 2  ,  input _ size =  (  1  ,  seq _ length ,  input _ size )  )  history 2   =  train ( model 2  ,  train _ loader ,  valid _ loader ,  model 2  _ optimizer ,  model 2  _ criterion ,  epochs =  5  0  0   )  epoch _ count  =  range (  1  ,  len ( history 2  [  ' loss '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 2  [  ' loss '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 2  [  ' val _ loss '  ]  ,  label =  ' valid '  )  plt . show (  )   #  Ensayo  #  x  =   3  0   #  y _ test  =  x  *   1  5  x _ test  =   3  0  y _ test  =  x _ test  *   1  5  test _ input  =  np . array (  [ x _ test ]  )  test _ input  =  test _ input . reshape (  (  1  ,  seq _ length ,  input _ size )  )  test _ input  =  torch . from _ numpy ( test _ input . astype ( np . float 3  2  )  )  test _ target  =  torch . from _ numpy ( np . array ( y _ test )  . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  y _ hat  =  model 2  ( test _ input )  print (  \" y _ test :  \"  ,  y _ test )  print (  \" y _ hat :  \"  ,  y _ hat )  loss  =  model 2  _ criterion ( y _ hat ,  test _ target )  . item (  )  print (  \" loss :  \"  ,  loss )   \"  \"  \" Se puede observar que para un problema tan simple como este no hay mucha diferencia entre utilizar una RNN o LSTM .  La LSTM tiene muchos m Ã  ¡ s par Ã  ¡ metros que la RNN .   #  #  #   3   -  Multi - layer LSTM  \"  \"  \"  from torch _ helpers import CustomLSTM  #  En esta oportunidad se utilizar Ã  ¡ n dos layer LSTM class Model 3  ( nn . Module )  :  def  _  _ init _  _  ( self ,  input _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )  self . lstm 1   =  CustomLSTM ( input _ size = input _ size ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  LSTM layer self . lstm 2   =  CustomLSTM ( input _ size =  6  4  ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  LSTM layer self . fc  =  nn . Linear ( in _ features =  6  4  ,  out _ features = output _ dim )   #    #  Fully connected layer def forward ( self ,  x )  :  lstm _ output ,   _   =  self . lstm 1  ( x )  lstm _ output ,   _   =  self . lstm 2  ( lstm _ output )  out  =  self . fc ( lstm _ output [  :  ,  -  1  ,  :  ]  )   #  take last output  ( last seq )  return out model 3   =  Model 3  ( input _ size = input _ size ,  output _ dim = output _ dim )   #  Crear el optimizador la una funci Ã  ³ n de error model 3  _ optimizer  =  torch . optim . Adam ( model 3  . parameters (  )  ,  lr =  0  .  0  1  )  model 3  _ criterion  =  nn . MSELoss (  )    #  mean squared error summary ( model 3  ,  input _ size =  (  1  ,  seq _ length ,  input _ size )  )  history 3   =  train ( model 3  ,  train _ loader ,  valid _ loader ,  model 3  _ optimizer ,  model 3  _ criterion ,  epochs =  5  0  0   )  epoch _ count  =  range (  1  ,  len ( history 3  [  ' loss '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 3  [  ' loss '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 3  [  ' val _ loss '  ]  ,  label =  ' valid '  )  plt . show (  )   #  Ensayo  #  x  =   3  0   #  y _ test  =  x  *   1  5  x _ test  =   3  0  y _ test  =  x _ test  *   1  5  test _ input  =  np . array (  [ x _ test ]  )  test _ input  =  test _ input . reshape (  (  1  ,  seq _ length ,  input _ size )  )  test _ input  =  torch . from _ numpy ( test _ input . astype ( np . float 3  2  )  )  test _ target  =  torch . from _ numpy ( np . array ( y _ test )  . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  y _ hat  =  model 3  ( test _ input )  print (  \" y _ test :  \"  ,  y _ test )  print (  \" y _ hat :  \"  ,  y _ hat )  loss  =  model 3  _ criterion ( y _ hat ,  test _ target )  . item (  )  print (  \" loss :  \"  ,  loss )   \"  \"  \"  #  #  #   4   -  Conclusi Ã  ³ n El resultado alcanzado es bueno pero podr Ã  ­ a mejorarse agregando m Ã  ¡ s layer LSTM o m Ã  ¡ s layer fully connected .   \"  \"  \"  import re import numpy as np import matplotlib . pyplot as plt import seaborn as sns import torch import torch . nn . functional as F import torch . nn as nn from torch . utils . data import Dataset ,  DataLoader  #  torchsummar actualmente tiene un problema con las LSTM ,  por eso  #  se utiliza torchinfo ,  un fork del proyecto original con el bug solucionado  ! pip 3  install torchinfo from torchinfo import summary import os import platform if os . access (  ' torch _ helpers . py '  ,  os . F _ OK )  is False :  if platform . system (  )   =  =   ' Windows '  :   ! curl  ! wget https :  /  / raw . githubusercontent . com / FIUBA - Posgrado - Inteligencia - Artificial / procesamiento _ lenguaje _ natural / main / scripts / torch _ helpers . py  >  torch _ helpers . py else :   ! wget torch _ helpers . py https :  /  / raw . githubusercontent . com / FIUBA - Posgrado - Inteligencia - Artificial / procesamiento _ lenguaje _ natural / main / scripts / torch _ helpers . py def train ( model ,  train _ loader ,  valid _ loader ,  optimizer ,  criterion ,  epochs =  1  0  0  )  :   #  Defino listas para realizar graficas de los resultados train _ loss  =   [  ]  valid _ loss  =   [  ]   #  Defino mi loop de entrenamiento for epoch in range ( epochs )  :  epoch _ train _ loss  =   0  .  0  epoch _ train _ accuracy  =   0  .  0  for train _ data ,  train _ target in train _ loader :   #  Seteo los gradientes en cero ya que ,  por defecto ,  PyTorch  #  los va acumulando optimizer . zero _ grad (  )  output  =  model ( train _ data )   #  Computo el error de la salida comparando contra las etiquetas loss  =  criterion ( output ,  train _ target )   #  Almaceno el error del batch para luego tener el error promedio de la epoca epoch _ train _ loss  +  =  loss . item (  )   #  Computo el nuevo set de gradientes a lo largo de toda la red loss . backward (  )   #  Realizo el paso de optimizacion actualizando los parametros de toda la red optimizer . step (  )   #  Calculo la media de error para la epoca de entrenamiento .   #  La longitud de train _ loader es igual a la cantidad de batches dentro de una epoca .  epoch _ train _ loss  =  epoch _ train _ loss  /  len ( train _ loader )  train _ loss . append ( epoch _ train _ loss )   #  Realizo el paso de validaci Ã  ³ n computando error y accuracy ,  y  #  almacenando los valores para imprimirlos y graficarlos valid _ data ,  valid _ target  =  iter ( valid _ loader )  . next (  )  output  =  model ( valid _ data )  epoch _ valid _ loss  =  criterion ( output ,  valid _ target )  . item (  )  valid _ loss . append ( epoch _ valid _ loss )  print ( f \" Epoch :   { epoch +  1  }  /  { epochs }   -  Train loss  { epoch _ train _ loss :  .  3 f }   -  Valid Loss  { epoch _ valid _ loss :  .  3 f }  \"  )  history  =   {   \" loss \"  :  train _ loss ,   \" val _ loss \"  :  valid _ loss ,   }  return history  #  Generar datos sint Ã  © ticos X  =  list (  )  y  =  list (  )  X  =   [ x +  1  for x in range (  2  0  )  ]   #   \" y \"   ( target )  se obtiene como cada dato de entrada multiplicado por  1  5  y  =   [ x  *   1  5  for x in X ]  print (  \" datos X :  \"  ,  X )  print (  \" datos y :  \"  ,  y )   #  Cada dato X lo transformarmos en una matriz de  1  fila  1  columna  (  1 x 1  )  X  =  np . array ( X )  . reshape ( len ( X )  ,   1  ,   1  )  print (  \" datos X :  \"  ,  X )   #   ( batch size ,  seq _ len ,  input _ size )  X . shape y  =  np . asanyarray ( y )  y . shape class Data ( Dataset )  :  def  _  _ init _  _  ( self ,  x ,  y )  :   #  Convertir los arrays de numpy a tensores .   #  pytorch espera en general entradas  3  2 bits self . x  =  torch . from _ numpy ( x . astype ( np . float 3  2  )  )   #  las loss unfction esperan la salida float self . y  =  torch . from _ numpy ( y . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  self . len  =  self . y . shape [  0  ]  def  _  _ getitem _  _  ( self , index )  :  return self . x [ index ]  ,  self . y [ index ]  def  _  _ len _  _  ( self )  :  return self . len data _ set  =  Data ( X ,  y )  input _ dim  =  data _ set . x . shape [  1  :  ]  seq _ length  =  input _ dim [  0  ]  input _ size  =  input _ dim [  1  ]  print (  \" Input dim \"  ,  input _ dim )  print (  \" seq _ length :  \"  ,  seq _ length )  print (  \" input _ size :  \"  ,  input _ size )  output _ dim  =  data _ set . y . shape [  1  ]  print (  \" Output dim \"  ,  output _ dim )  data _ set . x . shape data _ set . y . shape torch . manual _ seed (  4  2  )  valid _ set _ size  =  int ( data _ set . len  *   0  .  2  )  train _ set _ size  =  data _ set . len  -  valid _ set _ size  #  Cuando trabajmos con una serie temporal no mezclamos  ( shuffle )  los datos train _ set  =  torch . utils . data . Subset ( data _ set ,  range ( train _ set _ size )  )  valid _ set  =  torch . utils . data . Subset ( data _ set ,  range ( train _ set _ size ,  data _ set . len )  )  print (  \" Tama Ã  ± o del conjunto de entrenamiento :  \"  ,  len ( train _ set )  )  print (  \" Tama Ã  ± o del conjunto de validacion :  \"  ,  len ( valid _ set )  )  train _ loader  =  torch . utils . data . DataLoader ( train _ set ,  batch _ size = len ( train _ set )  ,  shuffle = False )  valid _ loader  =  torch . utils . data . DataLoader ( valid _ set ,  batch _ size = len ( valid _ set )  ,  shuffle = False )   \"  \"  \"  #  #  #   2   -  Entrenar el modelo  ( RNN y LSTM )  \"  \"  \"  from torch _ helpers import CustomRNN class Model 1  ( nn . Module )  :  def  _  _ init _  _  ( self ,  input _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )   # self . rnn 1   =  nn . RNN ( input _ size = input _ size ,  hidden _ size =  6  4  ,  batch _ first = True )   #  RNN layer  #  Utilizamos la CustomRNN ya que para series temporales suele funcionar mejor  #  la activacion  \" relu \"  en las RNN en vez de la  \" tanh \"  ,  pero por defecto la  #  layer de Pytorch RNN no permite modificar la funcion de activacion  # self . rnn 1   =  CustomRNN ( input _ size = input _ size ,  hidden _ size =  6  4  )   #  RNN layer self . rnn 1   =  CustomRNN ( input _ size = input _ size ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  RNN layer self . fc  =  nn . Linear ( in _ features =  6  4  ,  out _ features = output _ dim )   #    #  Fully connected layer def forward ( self ,  x )  :  lstm _ output ,   _   =  self . rnn 1  ( x )  out  =  self . fc ( lstm _ output [  :  ,  -  1  ,  :  ]  )   #  take last output  ( last seq )  return out model 1   =  Model 1  ( input _ size = input _ size ,  output _ dim = output _ dim )   #  Crear el optimizador la una funci Ã  ³ n de error model 1  _ optimizer  =  torch . optim . Adam ( model 1  . parameters (  )  ,  lr =  0  .  0  1  )  model 1  _ criterion  =  nn . MSELoss (  )    #  mean squared error summary ( model 1  ,  input _ size =  (  1  ,  seq _ length ,  input _ size )  )  history 1   =  train ( model 1  ,  train _ loader ,  valid _ loader ,  model 1  _ optimizer ,  model 1  _ criterion ,  epochs =  5  0  0   )  epoch _ count  =  range (  1  ,  len ( history 1  [  ' loss '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 1  [  ' loss '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 1  [  ' val _ loss '  ]  ,  label =  ' valid '  )  plt . show (  )   #  Ensayo  #  x  =   3  0   #  y _ test  =  x  *   1  5  x _ test  =   3  0  y _ test  =  x _ test  *   1  5  test _ input  =  np . array (  [ x _ test ]  )  test _ input  =  test _ input . reshape (  (  1  ,  seq _ length ,  input _ size )  )  test _ input  =  torch . from _ numpy ( test _ input . astype ( np . float 3  2  )  )  test _ target  =  torch . from _ numpy ( np . array ( y _ test )  . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  y _ hat  =  model 1  ( test _ input )  print (  \" y _ test :  \"  ,  y _ test )  print (  \" y _ hat :  \"  ,  y _ hat )  loss  =  model 1  _ criterion ( y _ hat ,  test _ target )  . item (  )  print (  \" loss :  \"  ,  loss )  from torch _ helpers import CustomLSTM class Model 2  ( nn . Module )  :  def  _  _ init _  _  ( self ,  input _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )   # self . lstm 1   =  nn . LSTM ( input _ size = input _ size ,  hidden _ size =  6  4  ,  batch _ first = True )   #  LSTM layer  #  Utilizamos la CustomLSTM ya que para series temporales suele funcionar mejor  #  la activacion  \" relu \"  en las LSTM en vez de la  \" tanh \"  ,  pero por defecto la  #  layer de Pytorch LSTM no permite modificar la funcion de activacion self . lstm 1   =  CustomLSTM ( input _ size = input _ size ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  LSTM layer self . fc  =  nn . Linear ( in _ features =  6  4  ,  out _ features = output _ dim )   #    #  Fully connected layer def forward ( self ,  x )  :  lstm _ output ,   _   =  self . lstm 1  ( x )  out  =  self . fc ( lstm _ output [  :  ,  -  1  ,  :  ]  )   #  take last output  ( last seq )  return out model 2   =  Model 2  ( input _ size = input _ size ,  output _ dim = output _ dim )   #  Crear el optimizador la una funci Ã  ³ n de error model 2  _ optimizer  =  torch . optim . Adam ( model 2  . parameters (  )  ,  lr =  0  .  0  1  )  model 2  _ criterion  =  nn . MSELoss (  )    #  mean squared error summary ( model 2  ,  input _ size =  (  1  ,  seq _ length ,  input _ size )  )  history 2   =  train ( model 2  ,  train _ loader ,  valid _ loader ,  model 2  _ optimizer ,  model 2  _ criterion ,  epochs =  5  0  0   )  epoch _ count  =  range (  1  ,  len ( history 2  [  ' loss '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 2  [  ' loss '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 2  [  ' val _ loss '  ]  ,  label =  ' valid '  )  plt . show (  )   #  Ensayo  #  x  =   3  0   #  y _ test  =  x  *   1  5  x _ test  =   3  0  y _ test  =  x _ test  *   1  5  test _ input  =  np . array (  [ x _ test ]  )  test _ input  =  test _ input . reshape (  (  1  ,  seq _ length ,  input _ size )  )  test _ input  =  torch . from _ numpy ( test _ input . astype ( np . float 3  2  )  )  test _ target  =  torch . from _ numpy ( np . array ( y _ test )  . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  y _ hat  =  model 2  ( test _ input )  print (  \" y _ test :  \"  ,  y _ test )  print (  \" y _ hat :  \"  ,  y _ hat )  loss  =  model 2  _ criterion ( y _ hat ,  test _ target )  . item (  )  print (  \" loss :  \"  ,  loss )   \"  \"  \" Se puede observar que para un problema tan simple como este no hay mucha diferencia entre utilizar una RNN o LSTM .  La LSTM tiene muchos m Ã  ¡ s par Ã  ¡ metros que la RNN .   #  #  #   3   -  Multi - layer LSTM  \"  \"  \"  from torch _ helpers import CustomLSTM  #  En esta oportunidad se utilizar Ã  ¡ n dos layer LSTM class Model 3  ( nn . Module )  :  def  _  _ init _  _  ( self ,  input _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )  self . lstm 1   =  CustomLSTM ( input _ size = input _ size ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  LSTM layer self . lstm 2   =  CustomLSTM ( input _ size =  6  4  ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  LSTM layer self . fc  =  nn . Linear ( in _ features =  6  4  ,  out _ features = output _ dim )   #    #  Fully connected layer def forward ( self ,  x )  :  lstm _ output ,   _   =  self . lstm 1  ( x )  lstm _ output ,   _   =  self . lstm 2  ( lstm _ output )  out  =  self . fc ( lstm _ output [  :  ,  -  1  ,  :  ]  )   #  take last output  ( last seq )  return out model 3   =  Model 3  ( input _ size = input _ size ,  output _ dim = output _ dim )   #  Crear el optimizador la una funci Ã  ³ n de error model 3  _ optimizer  =  torch . optim . Adam ( model 3  . parameters (  )  ,  lr =  0  .  0  1  )  model 3  _ criterion  =  nn . MSELoss (  )    #  mean squared error summary ( model 3  ,  input _ size =  (  1  ,  seq _ length ,  input _ size )  )  history 3   =  train ( model 3  ,  train _ loader ,  valid _ loader ,  model 3  _ optimizer ,  model 3  _ criterion ,  epochs =  5  0  0   )  epoch _ count  =  range (  1  ,  len ( history 3  [  ' loss '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 3  [  ' loss '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 3  [  ' val _ loss '  ]  ,  label =  ' valid '  )  plt . show (  )   #  Ensayo  #  x  =   3  0   #  y _ test  =  x  *   1  5  x _ test  =   3  0  y _ test  =  x _ test  *   1  5  test _ input  =  np . array (  [ x _ test ]  )  test _ input  =  test _ input . reshape (  (  1  ,  seq _ length ,  input _ size )  )  test _ input  =  torch . from _ numpy ( test _ input . astype ( np . float 3  2  )  )  test _ target  =  torch . from _ numpy ( np . array ( y _ test )  . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  y _ hat  =  model 3  ( test _ input )  print (  \" y _ test :  \"  ,  y _ test )  print (  \" y _ hat :  \"  ,  y _ hat )  loss  =  model 3  _ criterion ( y _ hat ,  test _ target )  . item (  )  print (  \" loss :  \"  ,  loss )   \"  \"  \"  #  #  #   4   -  Conclusi Ã  ³ n El resultado alcanzado es bueno pero podr Ã  ­ a mejorarse agregando m Ã  ¡ s layer LSTM o m Ã  ¡ s layer fully connected .   \"  \"  \"  import re import numpy as np import matplotlib . pyplot as plt import seaborn as sns import torch import torch . nn . functional as F import torch . nn as nn from torch . utils . data import Dataset ,  DataLoader  #  torchsummar actualmente tiene un problema con las LSTM ,  por eso  #  se utiliza torchinfo ,  un fork del proyecto original con el bug solucionado  ! pip 3  install torchinfo from torchinfo import summary import os import platform if os . access (  ' torch _ helpers . py '  ,  os . F _ OK )  is False :  if platform . system (  )   =  =   ' Windows '  :   ! curl  ! wget https :  /  / raw . githubusercontent . com / FIUBA - Posgrado - Inteligencia - Artificial / procesamiento _ lenguaje _ natural / main / scripts / torch _ helpers . py  >  torch _ helpers . py else :   ! wget torch _ helpers . py https :  /  / raw . githubusercontent . com / FIUBA - Posgrado - Inteligencia - Artificial / procesamiento _ lenguaje _ natural / main / scripts / torch _ helpers . py def train ( model ,  train _ loader ,  valid _ loader ,  optimizer ,  criterion ,  epochs =  1  0  0  )  :   #  Defino listas para realizar graficas de los resultados train _ loss  =   [  ]  valid _ loss  =   [  ]   #  Defino mi loop de entrenamiento for epoch in range ( epochs )  :  epoch _ train _ loss  =   0  .  0  epoch _ train _ accuracy  =   0  .  0  for train _ data ,  train _ target in train _ loader :   #  Seteo los gradientes en cero ya que ,  por defecto ,  PyTorch  #  los va acumulando optimizer . zero _ grad (  )  output  =  model ( train _ data )   #  Computo el error de la salida comparando contra las etiquetas loss  =  criterion ( output ,  train _ target )   #  Almaceno el error del batch para luego tener el error promedio de la epoca epoch _ train _ loss  +  =  loss . item (  )   #  Computo el nuevo set de gradientes a lo largo de toda la red loss . backward (  )   #  Realizo el paso de optimizacion actualizando los parametros de toda la red optimizer . step (  )   #  Calculo la media de error para la epoca de entrenamiento .   #  La longitud de train _ loader es igual a la cantidad de batches dentro de una epoca .  epoch _ train _ loss  =  epoch _ train _ loss  /  len ( train _ loader )  train _ loss . append ( epoch _ train _ loss )   #  Realizo el paso de validaci Ã  ³ n computando error y accuracy ,  y  #  almacenando los valores para imprimirlos y graficarlos valid _ data ,  valid _ target  =  iter ( valid _ loader )  . next (  )  output  =  model ( valid _ data )  epoch _ valid _ loss  =  criterion ( output ,  valid _ target )  . item (  )  valid _ loss . append ( epoch _ valid _ loss )  print ( f \" Epoch :   { epoch +  1  }  /  { epochs }   -  Train loss  { epoch _ train _ loss :  .  3 f }   -  Valid Loss  { epoch _ valid _ loss :  .  3 f }  \"  )  history  =   {   \" loss \"  :  train _ loss ,   \" val _ loss \"  :  valid _ loss ,   }  return history  #  Generar datos sint Ã  © ticos X  =  list (  )  y  =  list (  )  X  =   [ x +  1  for x in range (  2  0  )  ]   #   \" y \"   ( target )  se obtiene como cada dato de entrada multiplicado por  1  5  y  =   [ x  *   1  5  for x in X ]  print (  \" datos X :  \"  ,  X )  print (  \" datos y :  \"  ,  y )   #  Cada dato X lo transformarmos en una matriz de  1  fila  1  columna  (  1 x 1  )  X  =  np . array ( X )  . reshape ( len ( X )  ,   1  ,   1  )  print (  \" datos X :  \"  ,  X )   #   ( batch size ,  seq _ len ,  input _ size )  X . shape y  =  np . asanyarray ( y )  y . shape class Data ( Dataset )  :  def  _  _ init _  _  ( self ,  x ,  y )  :   #  Convertir los arrays de numpy a tensores .   #  pytorch espera en general entradas  3  2 bits self . x  =  torch . from _ numpy ( x . astype ( np . float 3  2  )  )   #  las loss unfction esperan la salida float self . y  =  torch . from _ numpy ( y . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  self . len  =  self . y . shape [  0  ]  def  _  _ getitem _  _  ( self , index )  :  return self . x [ index ]  ,  self . y [ index ]  def  _  _ len _  _  ( self )  :  return self . len data _ set  =  Data ( X ,  y )  input _ dim  =  data _ set . x . shape [  1  :  ]  seq _ length  =  input _ dim [  0  ]  input _ size  =  input _ dim [  1  ]  print (  \" Input dim \"  ,  input _ dim )  print (  \" seq _ length :  \"  ,  seq _ length )  print (  \" input _ size :  \"  ,  input _ size )  output _ dim  =  data _ set . y . shape [  1  ]  print (  \" Output dim \"  ,  output _ dim )  data _ set . x . shape data _ set . y . shape torch . manual _ seed (  4  2  )  valid _ set _ size  =  int ( data _ set . len  *   0  .  2  )  train _ set _ size  =  data _ set . len  -  valid _ set _ size  #  Cuando trabajmos con una serie temporal no mezclamos  ( shuffle )  los datos train _ set  =  torch . utils . data . Subset ( data _ set ,  range ( train _ set _ size )  )  valid _ set  =  torch . utils . data . Subset ( data _ set ,  range ( train _ set _ size ,  data _ set . len )  )  print (  \" Tama Ã  ± o del conjunto de entrenamiento :  \"  ,  len ( train _ set )  )  print (  \" Tama Ã  ± o del conjunto de validacion :  \"  ,  len ( valid _ set )  )  train _ loader  =  torch . utils . data . DataLoader ( train _ set ,  batch _ size = len ( train _ set )  ,  shuffle = False )  valid _ loader  =  torch . utils . data . DataLoader ( valid _ set ,  batch _ size = len ( valid _ set )  ,  shuffle = False )   \"  \"  \"  #  #  #   2   -  Entrenar el modelo  ( RNN y LSTM )  \"  \"  \"  from torch _ helpers import CustomRNN class Model 1  ( nn . Module )  :  def  _  _ init _  _  ( self ,  input _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )   # self . rnn 1   =  nn . RNN ( input _ size = input _ size ,  hidden _ size =  6  4  ,  batch _ first = True )   #  RNN layer  #  Utilizamos la CustomRNN ya que para series temporales suele funcionar mejor  #  la activacion  \" relu \"  en las RNN en vez de la  \" tanh \"  ,  pero por defecto la  #  layer de Pytorch RNN no permite modificar la funcion de activacion  # self . rnn 1   =  CustomRNN ( input _ size = input _ size ,  hidden _ size =  6  4  )   #  RNN layer self . rnn 1   =  CustomRNN ( input _ size = input _ size ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  RNN layer self . fc  =  nn . Linear ( in _ features =  6  4  ,  out _ features = output _ dim )   #    #  Fully connected layer def forward ( self ,  x )  :  lstm _ output ,   _   =  self . rnn 1  ( x )  out  =  self . fc ( lstm _ output [  :  ,  -  1  ,  :  ]  )   #  take last output  ( last seq )  return out model 1   =  Model 1  ( input _ size = input _ size ,  output _ dim = output _ dim )   #  Crear el optimizador la una funci Ã  ³ n de error model 1  _ optimizer  =  torch . optim . Adam ( model 1  . parameters (  )  ,  lr =  0  .  0  1  )  model 1  _ criterion  =  nn . MSELoss (  )    #  mean squared error summary ( model 1  ,  input _ size =  (  1  ,  seq _ length ,  input _ size )  )  history 1   =  train ( model 1  ,  train _ loader ,  valid _ loader ,  model 1  _ optimizer ,  model 1  _ criterion ,  epochs =  5  0  0   )  epoch _ count  =  range (  1  ,  len ( history 1  [  ' loss '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 1  [  ' loss '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 1  [  ' val _ loss '  ]  ,  label =  ' valid '  )  plt . show (  )   #  Ensayo  #  x  =   3  0   #  y _ test  =  x  *   1  5  x _ test  =   3  0  y _ test  =  x _ test  *   1  5  test _ input  =  np . array (  [ x _ test ]  )  test _ input  =  test _ input . reshape (  (  1  ,  seq _ length ,  input _ size )  )  test _ input  =  torch . from _ numpy ( test _ input . astype ( np . float 3  2  )  )  test _ target  =  torch . from _ numpy ( np . array ( y _ test )  . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  y _ hat  =  model 1  ( test _ input )  print (  \" y _ test :  \"  ,  y _ test )  print (  \" y _ hat :  \"  ,  y _ hat )  loss  =  model 1  _ criterion ( y _ hat ,  test _ target )  . item (  )  print (  \" loss :  \"  ,  loss )  from torch _ helpers import CustomLSTM class Model 2  ( nn . Module )  :  def  _  _ init _  _  ( self ,  input _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )   # self . lstm 1   =  nn . LSTM ( input _ size = input _ size ,  hidden _ size =  6  4  ,  batch _ first = True )   #  LSTM layer  #  Utilizamos la CustomLSTM ya que para series temporales suele funcionar mejor  #  la activacion  \" relu \"  en las LSTM en vez de la  \" tanh \"  ,  pero por defecto la  #  layer de Pytorch LSTM no permite modificar la funcion de activacion self . lstm 1   =  CustomLSTM ( input _ size = input _ size ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  LSTM layer self . fc  =  nn . Linear ( in _ features =  6  4  ,  out _ features = output _ dim )   #    #  Fully connected layer def forward ( self ,  x )  :  lstm _ output ,   _   =  self . lstm 1  ( x )  out  =  self . fc ( lstm _ output [  :  ,  -  1  ,  :  ]  )   #  take last output  ( last seq )  return out model 2   =  Model 2  ( input _ size = input _ size ,  output _ dim = output _ dim )   #  Crear el optimizador la una funci Ã  ³ n de error model 2  _ optimizer  =  torch . optim . Adam ( model 2  . parameters (  )  ,  lr =  0  .  0  1  )  model 2  _ criterion  =  nn . MSELoss (  )    #  mean squared error summary ( model 2  ,  input _ size =  (  1  ,  seq _ length ,  input _ size )  )  history 2   =  train ( model 2  ,  train _ loader ,  valid _ loader ,  model 2  _ optimizer ,  model 2  _ criterion ,  epochs =  5  0  0   )  epoch _ count  =  range (  1  ,  len ( history 2  [  ' loss '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 2  [  ' loss '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 2  [  ' val _ loss '  ]  ,  label =  ' valid '  )  plt . show (  )   #  Ensayo  #  x  =   3  0   #  y _ test  =  x  *   1  5  x _ test  =   3  0  y _ test  =  x _ test  *   1  5  test _ input  =  np . array (  [ x _ test ]  )  test _ input  =  test _ input . reshape (  (  1  ,  seq _ length ,  input _ size )  )  test _ input  =  torch . from _ numpy ( test _ input . astype ( np . float 3  2  )  )  test _ target  =  torch . from _ numpy ( np . array ( y _ test )  . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  y _ hat  =  model 2  ( test _ input )  print (  \" y _ test :  \"  ,  y _ test )  print (  \" y _ hat :  \"  ,  y _ hat )  loss  =  model 2  _ criterion ( y _ hat ,  test _ target )  . item (  )  print (  \" loss :  \"  ,  loss )   \"  \"  \" Se puede observar que para un problema tan simple como este no hay mucha diferencia entre utilizar una RNN o LSTM .  La LSTM tiene muchos m Ã  ¡ s par Ã  ¡ metros que la RNN .   #  #  #   3   -  Multi - layer LSTM  \"  \"  \"  from torch _ helpers import CustomLSTM  #  En esta oportunidad se utilizar Ã  ¡ n dos layer LSTM class Model 3  ( nn . Module )  :  def  _  _ init _  _  ( self ,  input _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )  self . lstm 1   =  CustomLSTM ( input _ size = input _ size ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  LSTM layer self . lstm 2   =  CustomLSTM ( input _ size =  6  4  ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  LSTM layer self . fc  =  nn . Linear ( in _ features =  6  4  ,  out _ features = output _ dim )   #    #  Fully connected layer def forward ( self ,  x )  :  lstm _ output ,   _   =  self . lstm 1  ( x )  lstm _ output ,   _   =  self . lstm 2  ( lstm _ output )  out  =  self . fc ( lstm _ output [  :  ,  -  1  ,  :  ]  )   #  take last output  ( last seq )  return out model 3   =  Model 3  ( input _ size = input _ size ,  output _ dim = output _ dim )   #  Crear el optimizador la una funci Ã  ³ n de error model 3  _ optimizer  =  torch . optim . Adam ( model 3  . parameters (  )  ,  lr =  0  .  0  1  )  model 3  _ criterion  =  nn . MSELoss (  )    #  mean squared error summary ( model 3  ,  input _ size =  (  1  ,  seq _ length ,  input _ size )  )  history 3   =  train ( model 3  ,  train _ loader ,  valid _ loader ,  model 3  _ optimizer ,  model 3  _ criterion ,  epochs =  5  0  0   )  epoch _ count  =  range (  1  ,  len ( history 3  [  ' loss '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 3  [  ' loss '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 3  [  ' val _ loss '  ]  ,  label =  ' valid '  )  plt . show (  )   #  Ensayo  #  x  =   3  0   #  y _ test  =  x  *   1  5  x _ test  =   3  0  y _ test  =  x _ test  *   1  5  test _ input  =  np . array (  [ x _ test ]  )  test _ input  =  test _ input . reshape (  (  1  ,  seq _ length ,  input _ size )  )  test _ input  =  torch . from _ numpy ( test _ input . astype ( np . float 3  2  )  )  test _ target  =  torch . from _ numpy ( np . array ( y _ test )  . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  y _ hat  =  model 3  ( test _ input )  print (  \" y _ test :  \"  ,  y _ test )  print (  \" y _ hat :  \"  ,  y _ hat )  loss  =  model 3  _ criterion ( y _ hat ,  test _ target )  . item (  )  print (  \" loss :  \"  ,  loss )   \"  \"  \"  #  #  #   4   -  Conclusi Ã  ³ n El resultado alcanzado es bueno pero podr Ã  ­ a mejorarse agregando m Ã  ¡ s layer LSTM o m Ã  ¡ s layer fully connected .   \"  \"  \"  import re import numpy as np import matplotlib . pyplot as plt import seaborn as sns import torch import torch . nn . functional as F import torch . nn as nn from torch . utils . data import Dataset ,  DataLoader  #  torchsummar actualmente tiene un problema con las LSTM ,  por eso  #  se utiliza torchinfo ,  un fork del proyecto original con el bug solucionado  ! pip 3  install torchinfo from torchinfo import summary import os import platform if os . access (  ' torch _ helpers . py '  ,  os . F _ OK )  is False :  if platform . system (  )   =  =   ' Windows '  :   ! curl  ! wget https :  /  / raw . githubusercontent . com / FIUBA - Posgrado - Inteligencia - Artificial / procesamiento _ lenguaje _ natural / main / scripts / torch _ helpers . py  >  torch _ helpers . py else :   ! wget torch _ helpers . py https :  /  / raw . githubusercontent . com / FIUBA - Posgrado - Inteligencia - Artificial / procesamiento _ lenguaje _ natural / main / scripts / torch _ helpers . py def train ( model ,  train _ loader ,  valid _ loader ,  optimizer ,  criterion ,  epochs =  1  0  0  )  :   #  Defino listas para realizar graficas de los resultados train _ loss  =   [  ]  valid _ loss  =   [  ]   #  Defino mi loop de entrenamiento for epoch in range ( epochs )  :  epoch _ train _ loss  =   0  .  0  epoch _ train _ accuracy  =   0  .  0  for train _ data ,  train _ target in train _ loader :   #  Seteo los gradientes en cero ya que ,  por defecto ,  PyTorch  #  los va acumulando optimizer . zero _ grad (  )  output  =  model ( train _ data )   #  Computo el error de la salida comparando contra las etiquetas loss  =  criterion ( output ,  train _ target )   #  Almaceno el error del batch para luego tener el error promedio de la epoca epoch _ train _ loss  +  =  loss . item (  )   #  Computo el nuevo set de gradientes a lo largo de toda la red loss . backward (  )   #  Realizo el paso de optimizacion actualizando los parametros de toda la red optimizer . step (  )   #  Calculo la media de error para la epoca de entrenamiento .   #  La longitud de train _ loader es igual a la cantidad de batches dentro de una epoca .  epoch _ train _ loss  =  epoch _ train _ loss  /  len ( train _ loader )  train _ loss . append ( epoch _ train _ loss )   #  Realizo el paso de validaci Ã  ³ n computando error y accuracy ,  y  #  almacenando los valores para imprimirlos y graficarlos valid _ data ,  valid _ target  =  iter ( valid _ loader )  . next (  )  output  =  model ( valid _ data )  epoch _ valid _ loss  =  criterion ( output ,  valid _ target )  . item (  )  valid _ loss . append ( epoch _ valid _ loss )  print ( f \" Epoch :   { epoch +  1  }  /  { epochs }   -  Train loss  { epoch _ train _ loss :  .  3 f }   -  Valid Loss  { epoch _ valid _ loss :  .  3 f }  \"  )  history  =   {   \" loss \"  :  train _ loss ,   \" val _ loss \"  :  valid _ loss ,   }  return history  #  Generar datos sint Ã  © ticos X  =  list (  )  y  =  list (  )   #  X es una lista de n Ã  º meros de  1  al  4  3  que avanzan de  3  en  3  X  =   [ x for x in range (  1  ,   4  4  ,   3  )  ]   #   \" y \"   ( target )  se obtiene como por cada dato de entrada se  #  se obtienen dos datos de salida como x +  1  y x +  2  y  =   [   [ x +  1  ,  x +  2  ]  for x in X ]  print (  \" datos X :  \"  ,  X )  print (  \" datos y :  \"  ,  y )   #  Cada dato X lo transformarmos en una matriz de  1  fila  1  columna  (  1 x 1  )  X  =  np . array ( X )  . reshape ( len ( X )  ,   1  ,   1  )  print (  \" datos X :  \"  ,  X )   #   ( batch size ,  seq _ len ,  input _ size )  X . shape y  =  np . asanyarray ( y )  y . shape class Data ( Dataset )  :  def  _  _ init _  _  ( self ,  x ,  y )  :   #  Convertir los arrays de numpy a tensores .   #  pytorch espera en general entradas  3  2 bits self . x  =  torch . from _ numpy ( x . astype ( np . float 3  2  )  )   #  las loss unfction esperan la salida float self . y  =  torch . from _ numpy ( y . astype ( np . float 3  2  )  )  self . len  =  self . y . shape [  0  ]  def  _  _ getitem _  _  ( self , index )  :  return self . x [ index ]  ,  self . y [ index ]  def  _  _ len _  _  ( self )  :  return self . len data _ set  =  Data ( X ,  y )  input _ dim  =  data _ set . x . shape [  1  :  ]  seq _ length  =  input _ dim [  0  ]  input _ size  =  input _ dim [  1  ]  print (  \" Input dim \"  ,  input _ dim )  print (  \" seq _ length :  \"  ,  seq _ length )  print (  \" input _ size :  \"  ,  input _ size )  output _ dim  =  data _ set . y . shape [  1  ]  print (  \" Output dim \"  ,  output _ dim )  data _ set . x . shape data _ set . y . shape torch . manual _ seed (  4  2  )  valid _ set _ size  =  int ( data _ set . len  *   0  .  2  )  train _ set _ size  =  data _ set . len  -  valid _ set _ size  #  Cuando trabajmos con una serie temporal no mezclamos  ( shuffle )  los datos train _ set  =  torch . utils . data . Subset ( data _ set ,  range ( train _ set _ size )  )  valid _ set  =  torch . utils . data . Subset ( data _ set ,  range ( train _ set _ size ,  data _ set . len )  )  print (  \" Tama Ã  ± o del conjunto de entrenamiento :  \"  ,  len ( train _ set )  )  print (  \" Tama Ã  ± o del conjunto de validacion :  \"  ,  len ( valid _ set )  )  train _ loader  =  torch . utils . data . DataLoader ( train _ set ,  batch _ size = len ( train _ set )  ,  shuffle = False )  valid _ loader  =  torch . utils . data . DataLoader ( valid _ set ,  batch _ size = len ( valid _ set )  ,  shuffle = False )   \"  \"  \"  #  #  #   2   -  Entrenar el modelo \"  \"  \"  from torch _ helpers import CustomLSTM class Model 1  ( nn . Module )  :  def  _  _ init _  _  ( self ,  input _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )   # self . rnn 1   =  nn . RNN ( input _ size = input _ size ,  hidden _ size =  6  4  ,  batch _ first = True )   #  RNN layer  #  Utilizamos la CustomRNN ya que para series temporales suele funcionar mejor  #  la activacion  \" relu \"  en las RNN en vez de la  \" tanh \"  ,  pero por defecto la  #  layer de Pytorch RNN no permite modificar la funcion de activacion  # self . rnn 1   =  CustomRNN ( input _ size = input _ size ,  hidden _ size =  6  4  )   #  RNN layer self . rnn 1   =  CustomLSTM ( input _ size = input _ size ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  RNN layer self . fc  =  nn . Linear ( in _ features =  6  4  ,  out _ features = output _ dim )   #    #  Fully connected layer def forward ( self ,  x )  :  lstm _ output ,   _   =  self . rnn 1  ( x )  out  =  self . fc ( lstm _ output [  :  ,  -  1  ,  :  ]  )   #  take last output  ( last seq )  return out model 1   =  Model 1  ( input _ size = input _ size ,  output _ dim = output _ dim )   #  Crear el optimizador la una funci Ã  ³ n de error model 1  _ optimizer  =  torch . optim . Adam ( model 1  . parameters (  )  ,  lr =  0  .  0  1  )  model 1  _ criterion  =  nn . MSELoss (  )    #  mean squared error summary ( model 1  ,  input _ size =  (  1  ,  seq _ length ,  input _ size )  )  history 1   =  train ( model 1  ,  train _ loader ,  valid _ loader ,  model 1  _ optimizer ,  model 1  _ criterion ,  epochs =  5  0  0   )  epoch _ count  =  range (  1  ,  len ( history 1  [  ' loss '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 1  [  ' loss '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 1  [  ' val _ loss '  ]  ,  label =  ' valid '  )  plt . show (  )   #  Ensayo x _ test  =   1  0  y _ test  =   [ x _ test  +   1  ,  x _ test  +   2  ]  test _ input  =  np . array (  [ x _ test ]  )  test _ input  =  test _ input . reshape (  (  1  ,  seq _ length ,  input _ size )  )  test _ input  =  torch . from _ numpy ( test _ input . astype ( np . float 3  2  )  )  test _ target  =  torch . from _ numpy ( np . array ( y _ test )  . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  y _ hat  =  model 1  ( test _ input )  print (  \" y _ test :  \"  ,  y _ test )  print (  \" y _ hat :  \"  ,  y _ hat )  loss  =  model 1  _ criterion ( y _ hat ,  test _ target )  . item (  )  print (  \" loss :  \"  ,  loss )   \"  \"  \"  #  #  #   3   -  Multi - layer LSTM \"  \"  \"  from torch _ helpers import CustomLSTM  #  En esta oportunidad se utilizar Ã  ¡ n dos layer LSTM class Model 2  ( nn . Module )  :  def  _  _ init _  _  ( self ,  input _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )  self . lstm 1   =  CustomLSTM ( input _ size = input _ size ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  LSTM layer self . lstm 2   =  CustomLSTM ( input _ size =  6  4  ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  LSTM layer self . fc  =  nn . Linear ( in _ features =  6  4  ,  out _ features = output _ dim )   #    #  Fully connected layer def forward ( self ,  x )  :  lstm _ output ,   _   =  self . lstm 1  ( x )  lstm _ output ,   _   =  self . lstm 2  ( lstm _ output )  out  =  self . fc ( lstm _ output [  :  ,  -  1  ,  :  ]  )   #  take last output  ( last seq )  return out model 2   =  Model 2  ( input _ size = input _ size ,  output _ dim = output _ dim )   #  Crear el optimizador la una funci Ã  ³ n de error model 2  _ optimizer  =  torch . optim . Adam ( model 2  . parameters (  )  ,  lr =  0  .  0  1  )  model 2  _ criterion  =  nn . MSELoss (  )    #  mean squared error summary ( model 2  ,  input _ size =  (  1  ,  seq _ length ,  input _ size )  )  history 2   =  train ( model 2  ,  train _ loader ,  valid _ loader ,  model 2  _ optimizer ,  model 2  _ criterion ,  epochs =  5  0  0   )  epoch _ count  =  range (  1  ,  len ( history 2  [  ' loss '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 2  [  ' loss '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 2  [  ' val _ loss '  ]  ,  label =  ' valid '  )  plt . show (  )   #  Ensayo x _ test  =   1  0  y _ test  =   [ x _ test  +   1  ,  x _ test  +   2  ]  test _ input  =  np . array (  [ x _ test ]  )  test _ input  =  test _ input . reshape (  (  1  ,  seq _ length ,  input _ size )  )  test _ input  =  torch . from _ numpy ( test _ input . astype ( np . float 3  2  )  )  test _ target  =  torch . from _ numpy ( np . array ( y _ test )  . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  y _ hat  =  model 2  ( test _ input )  print (  \" y _ test :  \"  ,  y _ test )  print (  \" y _ hat :  \"  ,  y _ hat )  loss  =  model 2  _ criterion ( y _ hat ,  test _ target )  . item (  )  print (  \" loss :  \"  ,  loss )   \"  \"  \"  #  #  #   4   -  Conclusi Ã  ³ n La unica diferencia que se debe tener en cuenta cuando hay m Ã  ¡ s de una salida es que la cantidad de neuronas de la  Ã  º ltima capa debe coincidir con el tama Ã  ± o de la secuencia de salida .  En este ejemplo ,  donde el problema es m Ã  ¡ s complejo ,  hubo una diferencia apreciable entre utilizar una sola capa o varias LSTM .   \"  \"  \"  import re import numpy as np import matplotlib . pyplot as plt import seaborn as sns import torch import torch . nn . functional as F import torch . nn as nn from torch . utils . data import Dataset ,  DataLoader  #  torchsummar actualmente tiene un problema con las LSTM ,  por eso  #  se utiliza torchinfo ,  un fork del proyecto original con el bug solucionado  ! pip 3  install torchinfo from torchinfo import summary import os import platform if os . access (  ' torch _ helpers . py '  ,  os . F _ OK )  is False :  if platform . system (  )   =  =   ' Windows '  :   ! curl  ! wget https :  /  / raw . githubusercontent . com / FIUBA - Posgrado - Inteligencia - Artificial / procesamiento _ lenguaje _ natural / main / scripts / torch _ helpers . py  >  torch _ helpers . py else :   ! wget torch _ helpers . py https :  /  / raw . githubusercontent . com / FIUBA - Posgrado - Inteligencia - Artificial / procesamiento _ lenguaje _ natural / main / scripts / torch _ helpers . py def train ( model ,  train _ loader ,  valid _ loader ,  optimizer ,  criterion ,  epochs =  1  0  0  )  :   #  Defino listas para realizar graficas de los resultados train _ loss  =   [  ]  valid _ loss  =   [  ]   #  Defino mi loop de entrenamiento for epoch in range ( epochs )  :  epoch _ train _ loss  =   0  .  0  epoch _ train _ accuracy  =   0  .  0  for train _ data ,  train _ target in train _ loader :   #  Seteo los gradientes en cero ya que ,  por defecto ,  PyTorch  #  los va acumulando optimizer . zero _ grad (  )  output  =  model ( train _ data )   #  Computo el error de la salida comparando contra las etiquetas loss  =  criterion ( output ,  train _ target )   #  Almaceno el error del batch para luego tener el error promedio de la epoca epoch _ train _ loss  +  =  loss . item (  )   #  Computo el nuevo set de gradientes a lo largo de toda la red loss . backward (  )   #  Realizo el paso de optimizacion actualizando los parametros de toda la red optimizer . step (  )   #  Calculo la media de error para la epoca de entrenamiento .   #  La longitud de train _ loader es igual a la cantidad de batches dentro de una epoca .  epoch _ train _ loss  =  epoch _ train _ loss  /  len ( train _ loader )  train _ loss . append ( epoch _ train _ loss )   #  Realizo el paso de validaci Ã  ³ n computando error y accuracy ,  y  #  almacenando los valores para imprimirlos y graficarlos valid _ data ,  valid _ target  =  iter ( valid _ loader )  . next (  )  output  =  model ( valid _ data )  epoch _ valid _ loss  =  criterion ( output ,  valid _ target )  . item (  )  valid _ loss . append ( epoch _ valid _ loss )  print ( f \" Epoch :   { epoch +  1  }  /  { epochs }   -  Train loss  { epoch _ train _ loss :  .  3 f }   -  Valid Loss  { epoch _ valid _ loss :  .  3 f }  \"  )  history  =   {   \" loss \"  :  train _ loss ,   \" val _ loss \"  :  valid _ loss ,   }  return history  #  Generar datos sint Ã  © ticos X  =  list (  )  y  =  list (  )  X  =   [ x +  1  for x in range (  2  0  )  ]   #   \" y \"   ( target )  se obtiene como cada dato de entrada multiplicado por  1  5  y  =   [ x  *   1  5  for x in X ]  print (  \" datos X :  \"  ,  X )  print (  \" datos y :  \"  ,  y )   #  Cada dato X lo transformarmos en una matriz de  1  fila  1  columna  (  1 x 1  )  X  =  np . array ( X )  . reshape ( len ( X )  ,   1  ,   1  )  print (  \" datos X :  \"  ,  X )   #   ( batch size ,  seq _ len ,  input _ size )  X . shape y  =  np . asanyarray ( y )  y . shape class Data ( Dataset )  :  def  _  _ init _  _  ( self ,  x ,  y )  :   #  Convertir los arrays de numpy a tensores .   #  pytorch espera en general entradas  3  2 bits self . x  =  torch . from _ numpy ( x . astype ( np . float 3  2  )  )   #  las loss unfction esperan la salida float self . y  =  torch . from _ numpy ( y . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  self . len  =  self . y . shape [  0  ]  def  _  _ getitem _  _  ( self , index )  :  return self . x [ index ]  ,  self . y [ index ]  def  _  _ len _  _  ( self )  :  return self . len data _ set  =  Data ( X ,  y )  input _ dim  =  data _ set . x . shape [  1  :  ]  seq _ length  =  input _ dim [  0  ]  input _ size  =  input _ dim [  1  ]  print (  \" Input dim \"  ,  input _ dim )  print (  \" seq _ length :  \"  ,  seq _ length )  print (  \" input _ size :  \"  ,  input _ size )  output _ dim  =  data _ set . y . shape [  1  ]  print (  \" Output dim \"  ,  output _ dim )  data _ set . x . shape data _ set . y . shape torch . manual _ seed (  4  2  )  valid _ set _ size  =  int ( data _ set . len  *   0  .  2  )  train _ set _ size  =  data _ set . len  -  valid _ set _ size  #  Cuando trabajmos con una serie temporal no mezclamos  ( shuffle )  los datos train _ set  =  torch . utils . data . Subset ( data _ set ,  range ( train _ set _ size )  )  valid _ set  =  torch . utils . data . Subset ( data _ set ,  range ( train _ set _ size ,  data _ set . len )  )  print (  \" Tama Ã  ± o del conjunto de entrenamiento :  \"  ,  len ( train _ set )  )  print (  \" Tama Ã  ± o del conjunto de validacion :  \"  ,  len ( valid _ set )  )  train _ loader  =  torch . utils . data . DataLoader ( train _ set ,  batch _ size = len ( train _ set )  ,  shuffle = False )  valid _ loader  =  torch . utils . data . DataLoader ( valid _ set ,  batch _ size = len ( valid _ set )  ,  shuffle = False )   \"  \"  \"  #  #  #   2   -  Entrenar el modelo  ( RNN y LSTM )  \"  \"  \"  from torch _ helpers import CustomRNN class Model 1  ( nn . Module )  :  def  _  _ init _  _  ( self ,  input _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )   # self . rnn 1   =  nn . RNN ( input _ size = input _ size ,  hidden _ size =  6  4  ,  batch _ first = True )   #  RNN layer  #  Utilizamos la CustomRNN ya que para series temporales suele funcionar mejor  #  la activacion  \" relu \"  en las RNN en vez de la  \" tanh \"  ,  pero por defecto la  #  layer de Pytorch RNN no permite modificar la funcion de activacion  # self . rnn 1   =  CustomRNN ( input _ size = input _ size ,  hidden _ size =  6  4  )   #  RNN layer self . rnn 1   =  CustomRNN ( input _ size = input _ size ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  RNN layer self . fc  =  nn . Linear ( in _ features =  6  4  ,  out _ features = output _ dim )   #    #  Fully connected layer def forward ( self ,  x )  :  lstm _ output ,   _   =  self . rnn 1  ( x )  out  =  self . fc ( lstm _ output [  :  ,  -  1  ,  :  ]  )   #  take last output  ( last seq )  return out model 1   =  Model 1  ( input _ size = input _ size ,  output _ dim = output _ dim )   #  Crear el optimizador la una funci Ã  ³ n de error model 1  _ optimizer  =  torch . optim . Adam ( model 1  . parameters (  )  ,  lr =  0  .  0  1  )  model 1  _ criterion  =  nn . MSELoss (  )    #  mean squared error summary ( model 1  ,  input _ size =  (  1  ,  seq _ length ,  input _ size )  )  history 1   =  train ( model 1  ,  train _ loader ,  valid _ loader ,  model 1  _ optimizer ,  model 1  _ criterion ,  epochs =  5  0  0   )  epoch _ count  =  range (  1  ,  len ( history 1  [  ' loss '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 1  [  ' loss '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 1  [  ' val _ loss '  ]  ,  label =  ' valid '  )  plt . show (  )   #  Ensayo  #  x  =   3  0   #  y _ test  =  x  *   1  5  x _ test  =   3  0  y _ test  =  x _ test  *   1  5  test _ input  =  np . array (  [ x _ test ]  )  test _ input  =  test _ input . reshape (  (  1  ,  seq _ length ,  input _ size )  )  test _ input  =  torch . from _ numpy ( test _ input . astype ( np . float 3  2  )  )  test _ target  =  torch . from _ numpy ( np . array ( y _ test )  . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  y _ hat  =  model 1  ( test _ input )  print (  \" y _ test :  \"  ,  y _ test )  print (  \" y _ hat :  \"  ,  y _ hat )  loss  =  model 1  _ criterion ( y _ hat ,  test _ target )  . item (  )  print (  \" loss :  \"  ,  loss )  from torch _ helpers import CustomLSTM class Model 2  ( nn . Module )  :  def  _  _ init _  _  ( self ,  input _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )   # self . lstm 1   =  nn . LSTM ( input _ size = input _ size ,  hidden _ size =  6  4  ,  batch _ first = True )   #  LSTM layer  #  Utilizamos la CustomLSTM ya que para series temporales suele funcionar mejor  #  la activacion  \" relu \"  en las LSTM en vez de la  \" tanh \"  ,  pero por defecto la  #  layer de Pytorch LSTM no permite modificar la funcion de activacion self . lstm 1   =  CustomLSTM ( input _ size = input _ size ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  LSTM layer self . fc  =  nn . Linear ( in _ features =  6  4  ,  out _ features = output _ dim )   #    #  Fully connected layer def forward ( self ,  x )  :  lstm _ output ,   _   =  self . lstm 1  ( x )  out  =  self . fc ( lstm _ output [  :  ,  -  1  ,  :  ]  )   #  take last output  ( last seq )  return out model 2   =  Model 2  ( input _ size = input _ size ,  output _ dim = output _ dim )   #  Crear el optimizador la una funci Ã  ³ n de error model 2  _ optimizer  =  torch . optim . Adam ( model 2  . parameters (  )  ,  lr =  0  .  0  1  )  model 2  _ criterion  =  nn . MSELoss (  )    #  mean squared error summary ( model 2  ,  input _ size =  (  1  ,  seq _ length ,  input _ size )  )  history 2   =  train ( model 2  ,  train _ loader ,  valid _ loader ,  model 2  _ optimizer ,  model 2  _ criterion ,  epochs =  5  0  0   )  epoch _ count  =  range (  1  ,  len ( history 2  [  ' loss '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 2  [  ' loss '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 2  [  ' val _ loss '  ]  ,  label =  ' valid '  )  plt . show (  )   #  Ensayo  #  x  =   3  0   #  y _ test  =  x  *   1  5  x _ test  =   3  0  y _ test  =  x _ test  *   1  5  test _ input  =  np . array (  [ x _ test ]  )  test _ input  =  test _ input . reshape (  (  1  ,  seq _ length ,  input _ size )  )  test _ input  =  torch . from _ numpy ( test _ input . astype ( np . float 3  2  )  )  test _ target  =  torch . from _ numpy ( np . array ( y _ test )  . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  y _ hat  =  model 2  ( test _ input )  print (  \" y _ test :  \"  ,  y _ test )  print (  \" y _ hat :  \"  ,  y _ hat )  loss  =  model 2  _ criterion ( y _ hat ,  test _ target )  . item (  )  print (  \" loss :  \"  ,  loss )   \"  \"  \" Se puede observar que para un problema tan simple como este no hay mucha diferencia entre utilizar una RNN o LSTM .  La LSTM tiene muchos m Ã  ¡ s par Ã  ¡ metros que la RNN .   #  #  #   3   -  Multi - layer LSTM  \"  \"  \"  from torch _ helpers import CustomLSTM  #  En esta oportunidad se utilizar Ã  ¡ n dos layer LSTM class Model 3  ( nn . Module )  :  def  _  _ init _  _  ( self ,  input _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )  self . lstm 1   =  CustomLSTM ( input _ size = input _ size ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  LSTM layer self . lstm 2   =  CustomLSTM ( input _ size =  6  4  ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  LSTM layer self . fc  =  nn . Linear ( in _ features =  6  4  ,  out _ features = output _ dim )   #    #  Fully connected layer def forward ( self ,  x )  :  lstm _ output ,   _   =  self . lstm 1  ( x )  lstm _ output ,   _   =  self . lstm 2  ( lstm _ output )  out  =  self . fc ( lstm _ output [  :  ,  -  1  ,  :  ]  )   #  take last output  ( last seq )  return out model 3   =  Model 3  ( input _ size = input _ size ,  output _ dim = output _ dim )   #  Crear el optimizador la una funci Ã  ³ n de error model 3  _ optimizer  =  torch . optim . Adam ( model 3  . parameters (  )  ,  lr =  0  .  0  1  )  model 3  _ criterion  =  nn . MSELoss (  )    #  mean squared error summary ( model 3  ,  input _ size =  (  1  ,  seq _ length ,  input _ size )  )  history 3   =  train ( model 3  ,  train _ loader ,  valid _ loader ,  model 3  _ optimizer ,  model 3  _ criterion ,  epochs =  5  0  0   )  epoch _ count  =  range (  1  ,  len ( history 3  [  ' loss '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 3  [  ' loss '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 3  [  ' val _ loss '  ]  ,  label =  ' valid '  )  plt . show (  )   #  Ensayo  #  x  =   3  0   #  y _ test  =  x  *   1  5  x _ test  =   3  0  y _ test  =  x _ test  *   1  5  test _ input  =  np . array (  [ x _ test ]  )  test _ input  =  test _ input . reshape (  (  1  ,  seq _ length ,  input _ size )  )  test _ input  =  torch . from _ numpy ( test _ input . astype ( np . float 3  2  )  )  test _ target  =  torch . from _ numpy ( np . array ( y _ test )  . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  y _ hat  =  model 3  ( test _ input )  print (  \" y _ test :  \"  ,  y _ test )  print (  \" y _ hat :  \"  ,  y _ hat )  loss  =  model 3  _ criterion ( y _ hat ,  test _ target )  . item (  )  print (  \" loss :  \"  ,  loss )   \"  \"  \"  #  #  #   4   -  Conclusi Ã  ³ n El resultado alcanzado es bueno pero podr Ã  ­ a mejorarse agregando m Ã  ¡ s layer LSTM o m Ã  ¡ s layer fully connected .   \"  \"  \"  import re import numpy as np import matplotlib . pyplot as plt import seaborn as sns import torch import torch . nn . functional as F import torch . nn as nn from torch . utils . data import Dataset ,  DataLoader  #  torchsummar actualmente tiene un problema con las LSTM ,  por eso  #  se utiliza torchinfo ,  un fork del proyecto original con el bug solucionado  ! pip 3  install torchinfo from torchinfo import summary import os import platform if os . access (  ' torch _ helpers . py '  ,  os . F _ OK )  is False :  if platform . system (  )   =  =   ' Windows '  :   ! curl  ! wget https :  /  / raw . githubusercontent . com / FIUBA - Posgrado - Inteligencia - Artificial / procesamiento _ lenguaje _ natural / main / scripts / torch _ helpers . py  >  torch _ helpers . py else :   ! wget torch _ helpers . py https :  /  / raw . githubusercontent . com / FIUBA - Posgrado - Inteligencia - Artificial / procesamiento _ lenguaje _ natural / main / scripts / torch _ helpers . py def train ( model ,  train _ loader ,  valid _ loader ,  optimizer ,  criterion ,  epochs =  1  0  0  )  :   #  Defino listas para realizar graficas de los resultados train _ loss  =   [  ]  valid _ loss  =   [  ]   #  Defino mi loop de entrenamiento for epoch in range ( epochs )  :  epoch _ train _ loss  =   0  .  0  epoch _ train _ accuracy  =   0  .  0  for train _ data ,  train _ target in train _ loader :   #  Seteo los gradientes en cero ya que ,  por defecto ,  PyTorch  #  los va acumulando optimizer . zero _ grad (  )  output  =  model ( train _ data )   #  Computo el error de la salida comparando contra las etiquetas loss  =  criterion ( output ,  train _ target )   #  Almaceno el error del batch para luego tener el error promedio de la epoca epoch _ train _ loss  +  =  loss . item (  )   #  Computo el nuevo set de gradientes a lo largo de toda la red loss . backward (  )   #  Realizo el paso de optimizacion actualizando los parametros de toda la red optimizer . step (  )   #  Calculo la media de error para la epoca de entrenamiento .   #  La longitud de train _ loader es igual a la cantidad de batches dentro de una epoca .  epoch _ train _ loss  =  epoch _ train _ loss  /  len ( train _ loader )  train _ loss . append ( epoch _ train _ loss )   #  Realizo el paso de validaci Ã  ³ n computando error y accuracy ,  y  #  almacenando los valores para imprimirlos y graficarlos valid _ data ,  valid _ target  =  iter ( valid _ loader )  . next (  )  output  =  model ( valid _ data )  epoch _ valid _ loss  =  criterion ( output ,  valid _ target )  . item (  )  valid _ loss . append ( epoch _ valid _ loss )  print ( f \" Epoch :   { epoch +  1  }  /  { epochs }   -  Train loss  { epoch _ train _ loss :  .  3 f }   -  Valid Loss  { epoch _ valid _ loss :  .  3 f }  \"  )  history  =   {   \" loss \"  :  train _ loss ,   \" val _ loss \"  :  valid _ loss ,   }  return history  #  Generar datos sint Ã  © ticos X  =  list (  )  y  =  list (  )  X  =   [ x +  1  for x in range (  2  0  )  ]   #   \" y \"   ( target )  se obtiene como cada dato de entrada multiplicado por  1  5  y  =   [ x  *   1  5  for x in X ]  print (  \" datos X :  \"  ,  X )  print (  \" datos y :  \"  ,  y )   #  Cada dato X lo transformarmos en una matriz de  1  fila  1  columna  (  1 x 1  )  X  =  np . array ( X )  . reshape ( len ( X )  ,   1  ,   1  )  print (  \" datos X :  \"  ,  X )   #   ( batch size ,  seq _ len ,  input _ size )  X . shape y  =  np . asanyarray ( y )  y . shape class Data ( Dataset )  :  def  _  _ init _  _  ( self ,  x ,  y )  :   #  Convertir los arrays de numpy a tensores .   #  pytorch espera en general entradas  3  2 bits self . x  =  torch . from _ numpy ( x . astype ( np . float 3  2  )  )   #  las loss unfction esperan la salida float self . y  =  torch . from _ numpy ( y . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  self . len  =  self . y . shape [  0  ]  def  _  _ getitem _  _  ( self , index )  :  return self . x [ index ]  ,  self . y [ index ]  def  _  _ len _  _  ( self )  :  return self . len data _ set  =  Data ( X ,  y )  input _ dim  =  data _ set . x . shape [  1  :  ]  seq _ length  =  input _ dim [  0  ]  input _ size  =  input _ dim [  1  ]  print (  \" Input dim \"  ,  input _ dim )  print (  \" seq _ length :  \"  ,  seq _ length )  print (  \" input _ size :  \"  ,  input _ size )  output _ dim  =  data _ set . y . shape [  1  ]  print (  \" Output dim \"  ,  output _ dim )  data _ set . x . shape data _ set . y . shape torch . manual _ seed (  4  2  )  valid _ set _ size  =  int ( data _ set . len  *   0  .  2  )  train _ set _ size  =  data _ set . len  -  valid _ set _ size  #  Cuando trabajmos con una serie temporal no mezclamos  ( shuffle )  los datos train _ set  =  torch . utils . data . Subset ( data _ set ,  range ( train _ set _ size )  )  valid _ set  =  torch . utils . data . Subset ( data _ set ,  range ( train _ set _ size ,  data _ set . len )  )  print (  \" Tama Ã  ± o del conjunto de entrenamiento :  \"  ,  len ( train _ set )  )  print (  \" Tama Ã  ± o del conjunto de validacion :  \"  ,  len ( valid _ set )  )  train _ loader  =  torch . utils . data . DataLoader ( train _ set ,  batch _ size = len ( train _ set )  ,  shuffle = False )  valid _ loader  =  torch . utils . data . DataLoader ( valid _ set ,  batch _ size = len ( valid _ set )  ,  shuffle = False )   \"  \"  \"  #  #  #   2   -  Entrenar el modelo  ( RNN y LSTM )  \"  \"  \"  from torch _ helpers import CustomRNN class Model 1  ( nn . Module )  :  def  _  _ init _  _  ( self ,  input _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )   # self . rnn 1   =  nn . RNN ( input _ size = input _ size ,  hidden _ size =  6  4  ,  batch _ first = True )   #  RNN layer  #  Utilizamos la CustomRNN ya que para series temporales suele funcionar mejor  #  la activacion  \" relu \"  en las RNN en vez de la  \" tanh \"  ,  pero por defecto la  #  layer de Pytorch RNN no permite modificar la funcion de activacion  # self . rnn 1   =  CustomRNN ( input _ size = input _ size ,  hidden _ size =  6  4  )   #  RNN layer self . rnn 1   =  CustomRNN ( input _ size = input _ size ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  RNN layer self . fc  =  nn . Linear ( in _ features =  6  4  ,  out _ features = output _ dim )   #    #  Fully connected layer def forward ( self ,  x )  :  lstm _ output ,   _   =  self . rnn 1  ( x )  out  =  self . fc ( lstm _ output [  :  ,  -  1  ,  :  ]  )   #  take last output  ( last seq )  return out model 1   =  Model 1  ( input _ size = input _ size ,  output _ dim = output _ dim )   #  Crear el optimizador la una funci Ã  ³ n de error model 1  _ optimizer  =  torch . optim . Adam ( model 1  . parameters (  )  ,  lr =  0  .  0  1  )  model 1  _ criterion  =  nn . MSELoss (  )    #  mean squared error summary ( model 1  ,  input _ size =  (  1  ,  seq _ length ,  input _ size )  )  history 1   =  train ( model 1  ,  train _ loader ,  valid _ loader ,  model 1  _ optimizer ,  model 1  _ criterion ,  epochs =  5  0  0   )  epoch _ count  =  range (  1  ,  len ( history 1  [  ' loss '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 1  [  ' loss '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 1  [  ' val _ loss '  ]  ,  label =  ' valid '  )  plt . show (  )   #  Ensayo  #  x  =   3  0   #  y _ test  =  x  *   1  5  x _ test  =   3  0  y _ test  =  x _ test  *   1  5  test _ input  =  np . array (  [ x _ test ]  )  test _ input  =  test _ input . reshape (  (  1  ,  seq _ length ,  input _ size )  )  test _ input  =  torch . from _ numpy ( test _ input . astype ( np . float 3  2  )  )  test _ target  =  torch . from _ numpy ( np . array ( y _ test )  . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  y _ hat  =  model 1  ( test _ input )  print (  \" y _ test :  \"  ,  y _ test )  print (  \" y _ hat :  \"  ,  y _ hat )  loss  =  model 1  _ criterion ( y _ hat ,  test _ target )  . item (  )  print (  \" loss :  \"  ,  loss )  from torch _ helpers import CustomLSTM class Model 2  ( nn . Module )  :  def  _  _ init _  _  ( self ,  input _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )   # self . lstm 1   =  nn . LSTM ( input _ size = input _ size ,  hidden _ size =  6  4  ,  batch _ first = True )   #  LSTM layer  #  Utilizamos la CustomLSTM ya que para series temporales suele funcionar mejor  #  la activacion  \" relu \"  en las LSTM en vez de la  \" tanh \"  ,  pero por defecto la  #  layer de Pytorch LSTM no permite modificar la funcion de activacion self . lstm 1   =  CustomLSTM ( input _ size = input _ size ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  LSTM layer self . fc  =  nn . Linear ( in _ features =  6  4  ,  out _ features = output _ dim )   #    #  Fully connected layer def forward ( self ,  x )  :  lstm _ output ,   _   =  self . lstm 1  ( x )  out  =  self . fc ( lstm _ output [  :  ,  -  1  ,  :  ]  )   #  take last output  ( last seq )  return out model 2   =  Model 2  ( input _ size = input _ size ,  output _ dim = output _ dim )   #  Crear el optimizador la una funci Ã  ³ n de error model 2  _ optimizer  =  torch . optim . Adam ( model 2  . parameters (  )  ,  lr =  0  .  0  1  )  model 2  _ criterion  =  nn . MSELoss (  )    #  mean squared error summary ( model 2  ,  input _ size =  (  1  ,  seq _ length ,  input _ size )  )  history 2   =  train ( model 2  ,  train _ loader ,  valid _ loader ,  model 2  _ optimizer ,  model 2  _ criterion ,  epochs =  5  0  0   )  epoch _ count  =  range (  1  ,  len ( history 2  [  ' loss '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 2  [  ' loss '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 2  [  ' val _ loss '  ]  ,  label =  ' valid '  )  plt . show (  )   #  Ensayo  #  x  =   3  0   #  y _ test  =  x  *   1  5  x _ test  =   3  0  y _ test  =  x _ test  *   1  5  test _ input  =  np . array (  [ x _ test ]  )  test _ input  =  test _ input . reshape (  (  1  ,  seq _ length ,  input _ size )  )  test _ input  =  torch . from _ numpy ( test _ input . astype ( np . float 3  2  )  )  test _ target  =  torch . from _ numpy ( np . array ( y _ test )  . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  y _ hat  =  model 2  ( test _ input )  print (  \" y _ test :  \"  ,  y _ test )  print (  \" y _ hat :  \"  ,  y _ hat )  loss  =  model 2  _ criterion ( y _ hat ,  test _ target )  . item (  )  print (  \" loss :  \"  ,  loss )   \"  \"  \" Se puede observar que para un problema tan simple como este no hay mucha diferencia entre utilizar una RNN o LSTM .  La LSTM tiene muchos m Ã  ¡ s par Ã  ¡ metros que la RNN .   #  #  #   3   -  Multi - layer LSTM  \"  \"  \"  from torch _ helpers import CustomLSTM  #  En esta oportunidad se utilizar Ã  ¡ n dos layer LSTM class Model 3  ( nn . Module )  :  def  _  _ init _  _  ( self ,  input _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )  self . lstm 1   =  CustomLSTM ( input _ size = input _ size ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  LSTM layer self . lstm 2   =  CustomLSTM ( input _ size =  6  4  ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  LSTM layer self . fc  =  nn . Linear ( in _ features =  6  4  ,  out _ features = output _ dim )   #    #  Fully connected layer def forward ( self ,  x )  :  lstm _ output ,   _   =  self . lstm 1  ( x )  lstm _ output ,   _   =  self . lstm 2  ( lstm _ output )  out  =  self . fc ( lstm _ output [  :  ,  -  1  ,  :  ]  )   #  take last output  ( last seq )  return out model 3   =  Model 3  ( input _ size = input _ size ,  output _ dim = output _ dim )   #  Crear el optimizador la una funci Ã  ³ n de error model 3  _ optimizer  =  torch . optim . Adam ( model 3  . parameters (  )  ,  lr =  0  .  0  1  )  model 3  _ criterion  =  nn . MSELoss (  )    #  mean squared error summary ( model 3  ,  input _ size =  (  1  ,  seq _ length ,  input _ size )  )  history 3   =  train ( model 3  ,  train _ loader ,  valid _ loader ,  model 3  _ optimizer ,  model 3  _ criterion ,  epochs =  5  0  0   )  epoch _ count  =  range (  1  ,  len ( history 3  [  ' loss '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 3  [  ' loss '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 3  [  ' val _ loss '  ]  ,  label =  ' valid '  )  plt . show (  )   #  Ensayo  #  x  =   3  0   #  y _ test  =  x  *   1  5  x _ test  =   3  0  y _ test  =  x _ test  *   1  5  test _ input  =  np . array (  [ x _ test ]  )  test _ input  =  test _ input . reshape (  (  1  ,  seq _ length ,  input _ size )  )  test _ input  =  torch . from _ numpy ( test _ input . astype ( np . float 3  2  )  )  test _ target  =  torch . from _ numpy ( np . array ( y _ test )  . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  y _ hat  =  model 3  ( test _ input )  print (  \" y _ test :  \"  ,  y _ test )  print (  \" y _ hat :  \"  ,  y _ hat )  loss  =  model 3  _ criterion ( y _ hat ,  test _ target )  . item (  )  print (  \" loss :  \"  ,  loss )   \"  \"  \"  #  #  #   4   -  Conclusi Ã  ³ n El resultado alcanzado es bueno pero podr Ã  ­ a mejorarse agregando m Ã  ¡ s layer LSTM o m Ã  ¡ s layer fully connected .   \"  \"  \"  import re import numpy as np import matplotlib . pyplot as plt import seaborn as sns import torch import torch . nn . functional as F import torch . nn as nn from torch . utils . data import Dataset ,  DataLoader  #  torchsummar actualmente tiene un problema con las LSTM ,  por eso  #  se utiliza torchinfo ,  un fork del proyecto original con el bug solucionado  ! pip 3  install torchinfo from torchinfo import summary import os import platform if os . access (  ' torch _ helpers . py '  ,  os . F _ OK )  is False :  if platform . system (  )   =  =   ' Windows '  :   ! curl  ! wget https :  /  / raw . githubusercontent . com / FIUBA - Posgrado - Inteligencia - Artificial / procesamiento _ lenguaje _ natural / main / scripts / torch _ helpers . py  >  torch _ helpers . py else :   ! wget torch _ helpers . py https :  /  / raw . githubusercontent . com / FIUBA - Posgrado - Inteligencia - Artificial / procesamiento _ lenguaje _ natural / main / scripts / torch _ helpers . py def train ( model ,  train _ loader ,  valid _ loader ,  optimizer ,  criterion ,  epochs =  1  0  0  )  :   #  Defino listas para realizar graficas de los resultados train _ loss  =   [  ]  valid _ loss  =   [  ]   #  Defino mi loop de entrenamiento for epoch in range ( epochs )  :  epoch _ train _ loss  =   0  .  0  epoch _ train _ accuracy  =   0  .  0  for train _ data ,  train _ target in train _ loader :   #  Seteo los gradientes en cero ya que ,  por defecto ,  PyTorch  #  los va acumulando optimizer . zero _ grad (  )  output  =  model ( train _ data )   #  Computo el error de la salida comparando contra las etiquetas loss  =  criterion ( output ,  train _ target )   #  Almaceno el error del batch para luego tener el error promedio de la epoca epoch _ train _ loss  +  =  loss . item (  )   #  Computo el nuevo set de gradientes a lo largo de toda la red loss . backward (  )   #  Realizo el paso de optimizacion actualizando los parametros de toda la red optimizer . step (  )   #  Calculo la media de error para la epoca de entrenamiento .   #  La longitud de train _ loader es igual a la cantidad de batches dentro de una epoca .  epoch _ train _ loss  =  epoch _ train _ loss  /  len ( train _ loader )  train _ loss . append ( epoch _ train _ loss )   #  Realizo el paso de validaci Ã  ³ n computando error y accuracy ,  y  #  almacenando los valores para imprimirlos y graficarlos valid _ data ,  valid _ target  =  iter ( valid _ loader )  . next (  )  output  =  model ( valid _ data )  epoch _ valid _ loss  =  criterion ( output ,  valid _ target )  . item (  )  valid _ loss . append ( epoch _ valid _ loss )  print ( f \" Epoch :   { epoch +  1  }  /  { epochs }   -  Train loss  { epoch _ train _ loss :  .  3 f }   -  Valid Loss  { epoch _ valid _ loss :  .  3 f }  \"  )  history  =   {   \" loss \"  :  train _ loss ,   \" val _ loss \"  :  valid _ loss ,   }  return history  #  Generar datos sint Ã  © ticos X  =  list (  )  y  =  list (  )  X  =   [ x +  1  for x in range (  2  0  )  ]   #   \" y \"   ( target )  se obtiene como cada dato de entrada multiplicado por  1  5  y  =   [ x  *   1  5  for x in X ]  print (  \" datos X :  \"  ,  X )  print (  \" datos y :  \"  ,  y )   #  Cada dato X lo transformarmos en una matriz de  1  fila  1  columna  (  1 x 1  )  X  =  np . array ( X )  . reshape ( len ( X )  ,   1  ,   1  )  print (  \" datos X :  \"  ,  X )   #   ( batch size ,  seq _ len ,  input _ size )  X . shape y  =  np . asanyarray ( y )  y . shape class Data ( Dataset )  :  def  _  _ init _  _  ( self ,  x ,  y )  :   #  Convertir los arrays de numpy a tensores .   #  pytorch espera en general entradas  3  2 bits self . x  =  torch . from _ numpy ( x . astype ( np . float 3  2  )  )   #  las loss unfction esperan la salida float self . y  =  torch . from _ numpy ( y . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  self . len  =  self . y . shape [  0  ]  def  _  _ getitem _  _  ( self , index )  :  return self . x [ index ]  ,  self . y [ index ]  def  _  _ len _  _  ( self )  :  return self . len data _ set  =  Data ( X ,  y )  input _ dim  =  data _ set . x . shape [  1  :  ]  seq _ length  =  input _ dim [  0  ]  input _ size  =  input _ dim [  1  ]  print (  \" Input dim \"  ,  input _ dim )  print (  \" seq _ length :  \"  ,  seq _ length )  print (  \" input _ size :  \"  ,  input _ size )  output _ dim  =  data _ set . y . shape [  1  ]  print (  \" Output dim \"  ,  output _ dim )  data _ set . x . shape data _ set . y . shape torch . manual _ seed (  4  2  )  valid _ set _ size  =  int ( data _ set . len  *   0  .  2  )  train _ set _ size  =  data _ set . len  -  valid _ set _ size  #  Cuando trabajmos con una serie temporal no mezclamos  ( shuffle )  los datos train _ set  =  torch . utils . data . Subset ( data _ set ,  range ( train _ set _ size )  )  valid _ set  =  torch . utils . data . Subset ( data _ set ,  range ( train _ set _ size ,  data _ set . len )  )  print (  \" Tama Ã  ± o del conjunto de entrenamiento :  \"  ,  len ( train _ set )  )  print (  \" Tama Ã  ± o del conjunto de validacion :  \"  ,  len ( valid _ set )  )  train _ loader  =  torch . utils . data . DataLoader ( train _ set ,  batch _ size = len ( train _ set )  ,  shuffle = False )  valid _ loader  =  torch . utils . data . DataLoader ( valid _ set ,  batch _ size = len ( valid _ set )  ,  shuffle = False )   \"  \"  \"  #  #  #   2   -  Entrenar el modelo  ( RNN y LSTM )  \"  \"  \"  from torch _ helpers import CustomRNN class Model 1  ( nn . Module )  :  def  _  _ init _  _  ( self ,  input _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )   # self . rnn 1   =  nn . RNN ( input _ size = input _ size ,  hidden _ size =  6  4  ,  batch _ first = True )   #  RNN layer  #  Utilizamos la CustomRNN ya que para series temporales suele funcionar mejor  #  la activacion  \" relu \"  en las RNN en vez de la  \" tanh \"  ,  pero por defecto la  #  layer de Pytorch RNN no permite modificar la funcion de activacion  # self . rnn 1   =  CustomRNN ( input _ size = input _ size ,  hidden _ size =  6  4  )   #  RNN layer self . rnn 1   =  CustomRNN ( input _ size = input _ size ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  RNN layer self . fc  =  nn . Linear ( in _ features =  6  4  ,  out _ features = output _ dim )   #    #  Fully connected layer def forward ( self ,  x )  :  lstm _ output ,   _   =  self . rnn 1  ( x )  out  =  self . fc ( lstm _ output [  :  ,  -  1  ,  :  ]  )   #  take last output  ( last seq )  return out model 1   =  Model 1  ( input _ size = input _ size ,  output _ dim = output _ dim )   #  Crear el optimizador la una funci Ã  ³ n de error model 1  _ optimizer  =  torch . optim . Adam ( model 1  . parameters (  )  ,  lr =  0  .  0  1  )  model 1  _ criterion  =  nn . MSELoss (  )    #  mean squared error summary ( model 1  ,  input _ size =  (  1  ,  seq _ length ,  input _ size )  )  history 1   =  train ( model 1  ,  train _ loader ,  valid _ loader ,  model 1  _ optimizer ,  model 1  _ criterion ,  epochs =  5  0  0   )  epoch _ count  =  range (  1  ,  len ( history 1  [  ' loss '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 1  [  ' loss '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 1  [  ' val _ loss '  ]  ,  label =  ' valid '  )  plt . show (  )   #  Ensayo  #  x  =   3  0   #  y _ test  =  x  *   1  5  x _ test  =   3  0  y _ test  =  x _ test  *   1  5  test _ input  =  np . array (  [ x _ test ]  )  test _ input  =  test _ input . reshape (  (  1  ,  seq _ length ,  input _ size )  )  test _ input  =  torch . from _ numpy ( test _ input . astype ( np . float 3  2  )  )  test _ target  =  torch . from _ numpy ( np . array ( y _ test )  . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  y _ hat  =  model 1  ( test _ input )  print (  \" y _ test :  \"  ,  y _ test )  print (  \" y _ hat :  \"  ,  y _ hat )  loss  =  model 1  _ criterion ( y _ hat ,  test _ target )  . item (  )  print (  \" loss :  \"  ,  loss )  from torch _ helpers import CustomLSTM class Model 2  ( nn . Module )  :  def  _  _ init _  _  ( self ,  input _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )   # self . lstm 1   =  nn . LSTM ( input _ size = input _ size ,  hidden _ size =  6  4  ,  batch _ first = True )   #  LSTM layer  #  Utilizamos la CustomLSTM ya que para series temporales suele funcionar mejor  #  la activacion  \" relu \"  en las LSTM en vez de la  \" tanh \"  ,  pero por defecto la  #  layer de Pytorch LSTM no permite modificar la funcion de activacion self . lstm 1   =  CustomLSTM ( input _ size = input _ size ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  LSTM layer self . fc  =  nn . Linear ( in _ features =  6  4  ,  out _ features = output _ dim )   #    #  Fully connected layer def forward ( self ,  x )  :  lstm _ output ,   _   =  self . lstm 1  ( x )  out  =  self . fc ( lstm _ output [  :  ,  -  1  ,  :  ]  )   #  take last output  ( last seq )  return out model 2   =  Model 2  ( input _ size = input _ size ,  output _ dim = output _ dim )   #  Crear el optimizador la una funci Ã  ³ n de error model 2  _ optimizer  =  torch . optim . Adam ( model 2  . parameters (  )  ,  lr =  0  .  0  1  )  model 2  _ criterion  =  nn . MSELoss (  )    #  mean squared error summary ( model 2  ,  input _ size =  (  1  ,  seq _ length ,  input _ size )  )  history 2   =  train ( model 2  ,  train _ loader ,  valid _ loader ,  model 2  _ optimizer ,  model 2  _ criterion ,  epochs =  5  0  0   )  epoch _ count  =  range (  1  ,  len ( history 2  [  ' loss '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 2  [  ' loss '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 2  [  ' val _ loss '  ]  ,  label =  ' valid '  )  plt . show (  )   #  Ensayo  #  x  =   3  0   #  y _ test  =  x  *   1  5  x _ test  =   3  0  y _ test  =  x _ test  *   1  5  test _ input  =  np . array (  [ x _ test ]  )  test _ input  =  test _ input . reshape (  (  1  ,  seq _ length ,  input _ size )  )  test _ input  =  torch . from _ numpy ( test _ input . astype ( np . float 3  2  )  )  test _ target  =  torch . from _ numpy ( np . array ( y _ test )  . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  y _ hat  =  model 2  ( test _ input )  print (  \" y _ test :  \"  ,  y _ test )  print (  \" y _ hat :  \"  ,  y _ hat )  loss  =  model 2  _ criterion ( y _ hat ,  test _ target )  . item (  )  print (  \" loss :  \"  ,  loss )   \"  \"  \" Se puede observar que para un problema tan simple como este no hay mucha diferencia entre utilizar una RNN o LSTM .  La LSTM tiene muchos m Ã  ¡ s par Ã  ¡ metros que la RNN .   #  #  #   3   -  Multi - layer LSTM  \"  \"  \"  from torch _ helpers import CustomLSTM  #  En esta oportunidad se utilizar Ã  ¡ n dos layer LSTM class Model 3  ( nn . Module )  :  def  _  _ init _  _  ( self ,  input _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )  self . lstm 1   =  CustomLSTM ( input _ size = input _ size ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  LSTM layer self . lstm 2   =  CustomLSTM ( input _ size =  6  4  ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  LSTM layer self . fc  =  nn . Linear ( in _ features =  6  4  ,  out _ features = output _ dim )   #    #  Fully connected layer def forward ( self ,  x )  :  lstm _ output ,   _   =  self . lstm 1  ( x )  lstm _ output ,   _   =  self . lstm 2  ( lstm _ output )  out  =  self . fc ( lstm _ output [  :  ,  -  1  ,  :  ]  )   #  take last output  ( last seq )  return out model 3   =  Model 3  ( input _ size = input _ size ,  output _ dim = output _ dim )   #  Crear el optimizador la una funci Ã  ³ n de error model 3  _ optimizer  =  torch . optim . Adam ( model 3  . parameters (  )  ,  lr =  0  .  0  1  )  model 3  _ criterion  =  nn . MSELoss (  )    #  mean squared error summary ( model 3  ,  input _ size =  (  1  ,  seq _ length ,  input _ size )  )  history 3   =  train ( model 3  ,  train _ loader ,  valid _ loader ,  model 3  _ optimizer ,  model 3  _ criterion ,  epochs =  5  0  0   )  epoch _ count  =  range (  1  ,  len ( history 3  [  ' loss '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 3  [  ' loss '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 3  [  ' val _ loss '  ]  ,  label =  ' valid '  )  plt . show (  )   #  Ensayo  #  x  =   3  0   #  y _ test  =  x  *   1  5  x _ test  =   3  0  y _ test  =  x _ test  *   1  5  test _ input  =  np . array (  [ x _ test ]  )  test _ input  =  test _ input . reshape (  (  1  ,  seq _ length ,  input _ size )  )  test _ input  =  torch . from _ numpy ( test _ input . astype ( np . float 3  2  )  )  test _ target  =  torch . from _ numpy ( np . array ( y _ test )  . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  y _ hat  =  model 3  ( test _ input )  print (  \" y _ test :  \"  ,  y _ test )  print (  \" y _ hat :  \"  ,  y _ hat )  loss  =  model 3  _ criterion ( y _ hat ,  test _ target )  . item (  )  print (  \" loss :  \"  ,  loss )   \"  \"  \"  #  #  #   4   -  Conclusi Ã  ³ n El resultado alcanzado es bueno pero podr Ã  ­ a mejorarse agregando m Ã  ¡ s layer LSTM o m Ã  ¡ s layer fully connected .   \"  \"  \"  import re import numpy as np import matplotlib . pyplot as plt import seaborn as sns import torch import torch . nn . functional as F import torch . nn as nn from torch . utils . data import Dataset ,  DataLoader  #  torchsummar actualmente tiene un problema con las LSTM ,  por eso  #  se utiliza torchinfo ,  un fork del proyecto original con el bug solucionado  ! pip 3  install torchinfo from torchinfo import summary import os import platform if os . access (  ' torch _ helpers . py '  ,  os . F _ OK )  is False :  if platform . system (  )   =  =   ' Windows '  :   ! curl  ! wget https :  /  / raw . githubusercontent . com / FIUBA - Posgrado - Inteligencia - Artificial / procesamiento _ lenguaje _ natural / main / scripts / torch _ helpers . py  >  torch _ helpers . py else :   ! wget torch _ helpers . py https :  /  / raw . githubusercontent . com / FIUBA - Posgrado - Inteligencia - Artificial / procesamiento _ lenguaje _ natural / main / scripts / torch _ helpers . py def train ( model ,  train _ loader ,  valid _ loader ,  optimizer ,  criterion ,  epochs =  1  0  0  )  :   #  Defino listas para realizar graficas de los resultados train _ loss  =   [  ]  valid _ loss  =   [  ]   #  Defino mi loop de entrenamiento for epoch in range ( epochs )  :  epoch _ train _ loss  =   0  .  0  epoch _ train _ accuracy  =   0  .  0  for train _ data ,  train _ target in train _ loader :   #  Seteo los gradientes en cero ya que ,  por defecto ,  PyTorch  #  los va acumulando optimizer . zero _ grad (  )  output  =  model ( train _ data )   #  Computo el error de la salida comparando contra las etiquetas loss  =  criterion ( output ,  train _ target )   #  Almaceno el error del batch para luego tener el error promedio de la epoca epoch _ train _ loss  +  =  loss . item (  )   #  Computo el nuevo set de gradientes a lo largo de toda la red loss . backward (  )   #  Realizo el paso de optimizacion actualizando los parametros de toda la red optimizer . step (  )   #  Calculo la media de error para la epoca de entrenamiento .   #  La longitud de train _ loader es igual a la cantidad de batches dentro de una epoca .  epoch _ train _ loss  =  epoch _ train _ loss  /  len ( train _ loader )  train _ loss . append ( epoch _ train _ loss )   #  Realizo el paso de validaci Ã  ³ n computando error y accuracy ,  y  #  almacenando los valores para imprimirlos y graficarlos valid _ data ,  valid _ target  =  iter ( valid _ loader )  . next (  )  output  =  model ( valid _ data )  epoch _ valid _ loss  =  criterion ( output ,  valid _ target )  . item (  )  valid _ loss . append ( epoch _ valid _ loss )  print ( f \" Epoch :   { epoch +  1  }  /  { epochs }   -  Train loss  { epoch _ train _ loss :  .  3 f }   -  Valid Loss  { epoch _ valid _ loss :  .  3 f }  \"  )  history  =   {   \" loss \"  :  train _ loss ,   \" val _ loss \"  :  valid _ loss ,   }  return history  #  Generar datos sint Ã  © ticos X  =  list (  )  y  =  list (  )   #  X ser Ã  ¡  una lista de  1  a  4  5  agrupado de a  3  n Ã  º meros consecutivos  #   [   [  1  ,   2  ,   3  ]  ,   [  4  ,   5  ,   6  ]  ,   .  .  .  .  ]  X  =   [   [ x ,  x +  1  ,  x +  2  ]  for x in range (  1  ,   4  6  ,   3  )  ]   #   \" y \"   ( target )  se obtiene como la suma de cada grupo de  3  n Ã  º meros de entrada y  =   [ sum ( x )  for x in X ]  print (  \" datos X :  \"  ,  X )  print (  \" datos y :  \"  ,  y )   #  Cada dato X lo transformarmos en una matriz de  1  fila  1  columna  (  1 x 1  )  X  =  np . array ( X )  . reshape ( len ( X )  ,  len ( X [  0  ]  )  ,   1  )  print (  \" datos X :  \"  ,  X )   #   ( batch size ,  seq _ len ,  input _ size )  X . shape y  =  np . asanyarray ( y )  y . shape class Data ( Dataset )  :  def  _  _ init _  _  ( self ,  x ,  y )  :   #  Convertir los arrays de numpy a tensores .   #  pytorch espera en general entradas  3  2 bits self . x  =  torch . from _ numpy ( x . astype ( np . float 3  2  )  )   #  las loss unfction esperan la salida float self . y  =  torch . from _ numpy ( y . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  self . len  =  self . y . shape [  0  ]  def  _  _ getitem _  _  ( self , index )  :  return self . x [ index ]  ,  self . y [ index ]  def  _  _ len _  _  ( self )  :  return self . len data _ set  =  Data ( X ,  y )  input _ dim  =  data _ set . x . shape [  1  :  ]  seq _ length  =  input _ dim [  0  ]  input _ size  =  input _ dim [  1  ]  print (  \" Input dim \"  ,  input _ dim )  print (  \" seq _ length :  \"  ,  seq _ length )  print (  \" input _ size :  \"  ,  input _ size )  output _ dim  =  data _ set . y . shape [  1  ]  print (  \" Output dim \"  ,  output _ dim )  torch . manual _ seed (  4  2  )  valid _ set _ size  =  int ( data _ set . len  *   0  .  2  )  train _ set _ size  =  data _ set . len  -  valid _ set _ size  #  Cuando trabajmos con una serie temporal no mezclamos  ( shuffle )  los datos train _ set  =  torch . utils . data . Subset ( data _ set ,  range ( train _ set _ size )  )  valid _ set  =  torch . utils . data . Subset ( data _ set ,  range ( train _ set _ size ,  data _ set . len )  )  print (  \" Tama Ã  ± o del conjunto de entrenamiento :  \"  ,  len ( train _ set )  )  print (  \" Tama Ã  ± o del conjunto de validacion :  \"  ,  len ( valid _ set )  )  train _ loader  =  torch . utils . data . DataLoader ( train _ set ,  batch _ size = len ( train _ set )  ,  shuffle = False )  valid _ loader  =  torch . utils . data . DataLoader ( valid _ set ,  batch _ size = len ( valid _ set )  ,  shuffle = False )   \"  \"  \"  #  #  #   2   -  Entrenar el modelo \"  \"  \"  from torch _ helpers import CustomLSTM class Model 1  ( nn . Module )  :  def  _  _ init _  _  ( self ,  input _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )  self . lstm 1   =  CustomLSTM ( input _ size = input _ size ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  LSTM layer self . fc  =  nn . Linear ( in _ features =  6  4  ,  out _ features = output _ dim )   #    #  Fully connected layer def forward ( self ,  x )  :  lstm _ output ,   _   =  self . lstm 1  ( x )  out  =  self . fc ( lstm _ output [  :  ,  -  1  ,  :  ]  )   #  take last output  ( last seq )  return out model 1   =  Model 1  ( input _ size = input _ size ,  output _ dim = output _ dim )   #  Crear el optimizador la una funci Ã  ³ n de error model 1  _ optimizer  =  torch . optim . Adam ( model 1  . parameters (  )  ,  lr =  0  .  0  1  )  model 1  _ criterion  =  nn . MSELoss (  )    #  mean squared error summary ( model 1  ,  input _ size =  (  1  ,  seq _ length ,  input _ size )  )  history 1   =  train ( model 1  ,  train _ loader ,  valid _ loader ,  model 1  _ optimizer ,  model 1  _ criterion ,  epochs =  5  0  0   )  epoch _ count  =  range (  1  ,  len ( history 1  [  ' loss '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 1  [  ' loss '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 1  [  ' val _ loss '  ]  ,  label =  ' valid '  )  plt . show (  )   #  Ensayo x _ test  =   [  5  0  ,   5  1  ,   5  2  ]  y _ test  =  sum ( x _ test )  test _ input  =  np . array (  [ x _ test ]  )  test _ input  =  test _ input . reshape (  (  1  ,  seq _ length ,  input _ size )  )  test _ input  =  torch . from _ numpy ( test _ input . astype ( np . float 3  2  )  )  test _ target  =  torch . from _ numpy ( np . array ( y _ test )  . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  y _ hat  =  model 1  ( test _ input )  print (  \" y _ test :  \"  ,  y _ test )  print (  \" y _ hat :  \"  ,  y _ hat )  loss  =  model 1  _ criterion ( y _ hat ,  test _ target )  . item (  )  print (  \" loss :  \"  ,  loss )   \"  \"  \"  #  #  #   3   -  Bidirectional RNN  ( BRNN )  Como la implementaci Ã  ³ n de  \" CustomLSTM \"  no soporta el flag de  \" bidirectional \"  que trae Pytorch ,  utilizaremos la layer por defecto de LSTM .  Por eso notar Ã  ¡  que la cantidad de par Ã  ¡ metros con bidireccional no es exactamente el doble  ( porque pytorch agrega otro bias a la ecuaci Ã  ³ n tradicional )  .  \\  La  Ã  º nica desventaja en este caso ,  al tratarse de una serie temporal no podremos utilizar la funci Ã  ³ n de activaci Ã  ³ n  \" relu \"  y por lo tanto el resultado alcanzado no ser Ã  ¡  tan bueno como con la CustomLSTM .   \"  \"  \"  from torch _ helpers import CustomLSTM  #  En esta oportunidad se utilizar Ã  ¡  Bidirectional ,  dentro se especifica  #  que lo que se desea hacer bidireccional es una capa LSTM  #  En el summary se puede observar que la cantidad de par Ã  ¡ metros  #  de nuestor nueva capa LSTM bidireccional es el doble que la anterior class Model 2  ( nn . Module )  :  def  _  _ init _  _  ( self ,  input _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )  self . lstm 1   =  nn . LSTM ( input _ size = input _ size ,  hidden _ size =  6  4  ,  batch _ first = True ,  bidirectional = True )  self . fc  =  nn . Linear ( in _ features =  2  *  6  4  ,  out _ features = output _ dim )   #    #  Fully connected layer def forward ( self ,  x )  :  lstm _ output ,   _   =  self . lstm 1  ( x )  out  =  self . fc ( lstm _ output [  :  ,  -  1  ,  :  ]  )   #  take last output  ( last seq )  return out model 2   =  Model 2  ( input _ size = input _ size ,  output _ dim = output _ dim )   #  Crear el optimizador la una funci Ã  ³ n de error model 2  _ optimizer  =  torch . optim . Adam ( model 2  . parameters (  )  ,  lr =  0  .  0  1  )  model 2  _ criterion  =  nn . MSELoss (  )    #  mean squared error summary ( model 2  ,  input _ size =  (  1  ,  seq _ length ,  input _ size )  )  history 2   =  train ( model 2  ,  train _ loader ,  valid _ loader ,  model 2  _ optimizer ,  model 2  _ criterion ,  epochs =  5  0  0   )  epoch _ count  =  range (  1  ,  len ( history 2  [  ' loss '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 2  [  ' loss '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 2  [  ' val _ loss '  ]  ,  label =  ' valid '  )  plt . show (  )   #  Ensayo x _ test  =   [  5  0  ,   5  1  ,   5  2  ]  y _ test  =  sum ( x _ test )  test _ input  =  np . array (  [ x _ test ]  )  test _ input  =  test _ input . reshape (  (  1  ,  seq _ length ,  input _ size )  )  test _ input  =  torch . from _ numpy ( test _ input . astype ( np . float 3  2  )  )  test _ target  =  torch . from _ numpy ( np . array ( y _ test )  . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  y _ hat  =  model 2  ( test _ input )  print (  \" y _ test :  \"  ,  y _ test )  print (  \" y _ hat :  \"  ,  y _ hat )  loss  =  model 2  _ criterion ( y _ hat ,  test _ target )  . item (  )  print (  \" loss :  \"  ,  loss )   \"  \"  \"  #  #  #   4   -  Conclusi Ã  ³ n Implementar un modelo bidireccional basado en RNN  ( en este caso LSTM )  es muy sensillo .  En este ejemplo no se explot Ã  ³  su potencialidad pero queda como nota de como implementar una capa BRNN .   \"  \"  \"  import random import io import pickle import numpy as np import pandas as pd import matplotlib . pyplot as plt import seaborn as sns import torch import torch . nn . functional as F import torch . nn as nn from torch . utils . data import Dataset ,  DataLoader cuda  =  torch . cuda . is _ available (  )  device  =  torch . device (  ' cuda '  if torch . cuda . is _ available (  )  else  ' cpu '  )  device  #  torchsummar actualmente tiene un problema con las LSTM ,  por eso  #  se utiliza torchinfo ,  un fork del proyecto original con el bug solucionado  ! pip 3  install torchinfo from torchinfo import summary import os import platform if os . access (  ' torch _ helpers . py '  ,  os . F _ OK )  is False :  if platform . system (  )   =  =   ' Windows '  :   ! curl  ! wget https :  /  / raw . githubusercontent . com / FIUBA - Posgrado - Inteligencia - Artificial / procesamiento _ lenguaje _ natural / main / scripts / torch _ helpers . py  >  torch _ helpers . py else :   ! wget torch _ helpers . py https :  /  / raw . githubusercontent . com / FIUBA - Posgrado - Inteligencia - Artificial / procesamiento _ lenguaje _ natural / main / scripts / torch _ helpers . py from torch _ helpers import categorical _ acc def train ( model ,  train _ loader ,  valid _ loader ,  optimizer ,  criterion ,  epochs =  1  0  0  )  :   #  Defino listas para realizar graficas de los resultados train _ loss  =   [  ]  train _ accuracy  =   [  ]  valid _ loss  =   [  ]  valid _ accuracy  =   [  ]   #  Defino mi loop de entrenamiento for epoch in range ( epochs )  :  epoch _ train _ loss  =   0  .  0  epoch _ train _ accuracy  =   0  .  0  for train _ data ,  train _ target in train _ loader :   #  Seteo los gradientes en cero ya que ,  por defecto ,  PyTorch  #  los va acumulando optimizer . zero _ grad (  )  output  =  model ( train _ data . to ( device )  )  . cpu (  )   #  Computo el error de la salida comparando contra las etiquetas loss  =  criterion ( output ,  train _ target )   #  Almaceno el error del batch para luego tener el error promedio de la epoca epoch _ train _ loss  +  =  loss . item (  )   #  Computo el nuevo set de gradientes a lo largo de toda la red loss . backward (  )   #  Realizo el paso de optimizacion actualizando los parametros de toda la red optimizer . step (  )   #  Calculo el accuracy del batch accuracy  =  categorical _ acc ( output ,  train _ target )   #  Almaceno el accuracy del batch para luego tener el accuracy promedio de la epoca epoch _ train _ accuracy  +  =  accuracy . item (  )   #  Calculo la media de error para la epoca de entrenamiento .   #  La longitud de train _ loader es igual a la cantidad de batches dentro de una epoca .  epoch _ train _ loss  =  epoch _ train _ loss  /  len ( train _ loader )  train _ loss . append ( epoch _ train _ loss )  epoch _ train _ accuracy  =  epoch _ train _ accuracy  /  len ( train _ loader )  train _ accuracy . append ( epoch _ train _ accuracy )   #  Realizo el paso de validaci Ã  ³ n computando error y accuracy ,  y  #  almacenando los valores para imprimirlos y graficarlos valid _ data ,  valid _ target  =  iter ( valid _ loader )  . next (  )  output  =  model ( valid _ data . to ( device )  )  . cpu (  )  epoch _ valid _ loss  =  criterion ( output ,  valid _ target )  . item (  )  valid _ loss . append ( epoch _ valid _ loss )   #  Calculo el accuracy de la epoch epoch _ valid _ accuracy  =  categorical _ acc ( output ,  valid _ target )  . item (  )  valid _ accuracy . append ( epoch _ valid _ accuracy )  print ( f \" Epoch :   { epoch +  1  }  /  { epochs }   -  Train loss  { epoch _ train _ loss :  .  3 f }   -  Train accuracy  { epoch _ train _ accuracy :  .  3 f }   -  Valid Loss  { epoch _ valid _ loss :  .  3 f }   -  Valid accuracy  { epoch _ valid _ accuracy :  .  3 f }  \"  )  history  =   {   \" loss \"  :  train _ loss ,   \" accuracy \"  :  train _ accuracy ,   \" val _ loss \"  :  valid _ loss ,   \" val _ accuracy \"  :  valid _ accuracy ,   }  return history  \"  \"  \"  #  #  #  Datos Utilizaremos como dataset canciones de bandas de habla ingl Ã  © s .   \"  \"  \"   #  Descargar la carpeta de dataset import os import platform if os . access (  '  .  / songs _ dataset '  ,  os . F _ OK )  is False :  if os . access (  ' songs _ dataset . zip '  ,  os . F _ OK )  is False :  if platform . system (  )   =  =   ' Windows '  :   ! curl https :  /  / raw . githubusercontent . com / FIUBA - Posgrado - Inteligencia - Artificial / procesamiento _ lenguaje _ natural / main / datasets / songs _ dataset . zip  - o songs _ dataset . zip else :   ! wget songs _ dataset . zip https :  /  / github . com / FIUBA - Posgrado - Inteligencia - Artificial / procesamiento _ lenguaje _ natural / raw / main / datasets / songs _ dataset . zip  ! unzip  - q songs _ dataset . zip else :  print (  \" El dataset ya se encuentra descargado \"  )   #  Posibles bandas os . listdir (  \"  .  / songs _ dataset /  \"  )   #  Armar el dataset utilizando salto de l Ã  ­ nea para separar las oraciones / docs df  =  pd . read _ csv (  ' songs _ dataset / beatles . txt '  ,  sep =  '  / n '  ,  header = None )  df . head (  )  print (  \" Cantidad de documentos :  \"  ,  df . shape [  0  ]  )   \"  \"  \"  #  #  #   1   -  Ejemplo de Preprocesamiento  -  Hay que transformar las oraciones en tokens .   -  Dichas oraciones hay que ajustarlas al tama Ã  ± o fijo de nuestra sentencia de entrada al modelo .   -  Hay que separar las palabras objetivos  ( target )  que el modelo debe predecir en cada sentencia armada .   \"  \"  \"  from torch _ helpers import Tokenizer  #  tool de keras equivalente a ltokenizer de nltk from torch _ helpers import text _ to _ word _ sequence  #  tool de keras equivalente a word _ teokenize de nltk from torch _ helpers import pad _ sequences  #  tool de keras qye se utilizar Ã  ¡  para padding  #  largo de la secuencia ,  incluye seq input  +  word output train _ len  =   4   #  Ejemplo de como transformar una oraci Ã  ³ n a tokens usando keras text  =  df . loc [  0  ,  0  ]  text tokens  =  text _ to _ word _ sequence ( text )   #  entran oraciones  -  >  salen vectores de N posiciones  ( tokens )  tokens  \"  \"  \"  1  .  1   -  Transformar las oraciones en secuencias  ( tokens )  de palabras \"  \"  \"   #  Recorrer todas las filas y transformar las oraciones  #  en secuencias de palabras sentence _ tokens  =   [  ]  for  _  ,  row in df [  : None ]  . iterrows (  )  :  sentence _ tokens . append ( text _ to _ word _ sequence ( row [  0  ]  )  )   #  Demos un vistazo sentence _ tokens [  :  2  ]   #  C Ã  ³ digo para hacer el desfazaje de las palabras  #  seg Ã  º n el train _ len text _ sequences  =   [  ]  for i in range ( train _ len ,  len ( tokens )  )  :  seq  =  tokens [ i - train _ len : i ]  text _ sequences . append ( seq )   #  Demos un vistazo a nuestros vectores para entrenar el modelo  #  seq _ input  +  output text _ sequences  \"  \"  \"  1  .  2   -  Crear los vectores de palabras  ( word 2 vec )  Ahora necesitamos pasarlos a n Ã  º meros para que lo entienda la red y separar input de output .   -  El Input seran integers  ( word 2 vec )   -  Mientras que el output ser Ã  ¡  one hot encodeado  ( labels )  del tama Ã  ± o del vocabulario  \"  \"  \"  tok  =  Tokenizer (  )   #  El tokeinzer  \" aprende \"  las palabras que se usaran  #  Se construye  ( fit )  una vez por proyecto ,  se aplica N veces  ( tal cual un encoder )  tok . fit _ on _ texts ( text _ sequences )   #  Convertimos las palabras a n Ã  º meros  #  entran palabras  -  >  salen n Ã  º meros sequences  =  tok . texts _ to _ sequences ( text _ sequences )   #  Ahora sequences tiene los n Ã  º meros  \" ID \"  ,  largo  4  sequences  #  Cantidad de casos  ( doc )  de entrada print ( tok . document _ count )   #  Cantidad de palabras distintas print ( len ( tok . word _ counts )  )   #  El  Ã  ­ ndice para cada palabra  #  El sistema las ordena de las m Ã  ¡ s populares a las menos populares print ( tok . word _ index )   #  Cantidad de veces quea aparece cada palabra en cada  \" documento \"   #   (  1  documento  =   1  caso de entrada )  print ( tok . word _ docs )   \"  \"  \"  #  #  #   2   -  Preprocesamiento completo Debemos realizar los mismos pasos que en el ejemplo anterior ,  pero antes de eso debemos transformar ese dataset de filas de oraciones en un texto completo continuo para poder extraer el vocabulario .   \"  \"  \"   #  Vistazo a las primeras filas df . loc [  :  1  5  ,  0  ]   #  Concatenamos todos los rows en un solo valor corpus  =  df . apply ( lambda row :   '   '  . join ( row . values . astype ( str )  )  ,  axis =  0  )  [  0  ]  corpus  #  Transformar el corpus a tokens tokens = text _ to _ word _ sequence ( corpus )   #  Vistazo general de los primeros tokens tokens [  :  2  0  ]  print (  \" Cantidad de tokens en el corpus :  \"  ,  len ( tokens )  )   #  C Ã  ³ digo para hacer el desfazaje de las palabras  #  seg Ã  º n el train _ len text _ sequences  =   [  ]  for i in range ( train _ len ,  len ( tokens )  )  :  seq  =  tokens [ i - train _ len : i ]  text _ sequences . append ( seq )   #  Demos un vistazo a nuestros vectores para entrenar el modelo text _ sequences [  :  2  0  ]   #  Proceso de tokenizacion tok  =  Tokenizer (  )  tok . fit _ on _ texts ( text _ sequences )   #  Convertimos las palabras a n Ã  º meros  #  entran palabras  -  >  salen n Ã  º meros sequences  =  tok . texts _ to _ sequences ( text _ sequences )   #  Damos un vistazo sequences [  :  2  0  ]  print (  \" Cantidad de rows del dataset :  \"  ,  len ( sequences )  )   \"  \"  \"  #  #  #   3   -  Input y target \"  \"  \"   #  Con numpy es muy f Ã  ¡ cil realizar el slicing de vectores ex  =  np . array (  [  [  1  ,  2  ,  3  ,  4  ]  ,  [  5  ,  6  ,  7  ,  8  ]  ]  )  ex  #  Con numpy es muy f Ã  ¡ cil realizar el slicing de vectores print (  \" Dimension :  \"  ,  ex . shape )  print (  \" Todos los elementos :  \"  ,  ex )  print (  \" Todos los elementos menos el  Ã  º ltimo :  \"  ,  ex [  :  ,   :  -  1  ]  )  input  =  ex [  :  ,  :  -  1  ]   #  todos los rows ,  menos la ultima col target  =  ex [  :  ,   -  1  ]   #   Ã  º ltima col de cada row print (  \" Input :  \"  ,  input )  print (  \" Target :  \"  ,  target )  arr _ sequences  =  np . array ( sequences )  x _ data  =  arr _ sequences [  :  ,  :  -  1  ]  y _ data _ int  =  arr _ sequences [  :  ,  -  1  ]   #  a Ã  º n falta el oneHotEncoder print ( x _ data . shape )  print ( y _ data _ int . shape )   #  Palabras del vocabulario tok . index _ word  #  Cantidad de palabras en el vocabulario vocab _ size  =  len ( tok . word _ counts )  vocab _ size  #   Â  ¡ Ojo !  y _ data _ int comienza en  \"  1  \"  en vez de  \"  0  \"   #  valor minimo :  min ( y _ data _ int )   #  Hay que restar  1  y _ data _ int _ offset  =  y _ data _ int  -   1  min ( y _ data _ int _ offset )  class Data ( Dataset )  :  def  _  _ init _  _  ( self ,  x ,  y )  :   #  Convertir los arrays de numpy a tensores .   #  pytorch espera en general entradas  3  2 bits self . x  =  torch . from _ numpy ( x . astype ( np . int 3  2  )  )   #  Transformar los datos a oneHotEncoding  #  la loss function esperan la salida float self . y  =  F . one _ hot ( torch . from _ numpy ( y )  ,  num _ classes = vocab _ size )  . float (  )  self . len  =  self . y . shape [  0  ]  def  _  _ getitem _  _  ( self , index )  :  return self . x [ index ]  ,  self . y [ index ]  def  _  _ len _  _  ( self )  :  return self . len data _ set  =  Data ( x _ data ,  y _ data _ int _ offset )  input _ size  =  data _ set . x . shape [  1  ]  print (  \" input _ size :  \"  ,  input _ size )  output _ dim  =  data _ set . y . shape [  1  ]  print (  \" Output dim \"  ,  output _ dim )  torch . manual _ seed (  4  2  )  valid _ set _ size  =  int ( data _ set . len  *   0  .  2  )  train _ set _ size  =  data _ set . len  -  valid _ set _ size train _ set  =  torch . utils . data . Subset ( data _ set ,  range ( train _ set _ size )  )  valid _ set  =  torch . utils . data . Subset ( data _ set ,  range ( train _ set _ size ,  data _ set . len )  )  print (  \" Tama Ã  ± o del conjunto de entrenamiento :  \"  ,  len ( train _ set )  )  print (  \" Tama Ã  ± o del conjunto de validacion :  \"  ,  len ( valid _ set )  )  train _ batch _ size  =   3  2  train _ loader  =  torch . utils . data . DataLoader ( train _ set ,  batch _ size = train _ batch _ size ,  shuffle = True )  valid _ loader  =  torch . utils . data . DataLoader ( valid _ set ,  batch _ size = len ( valid _ set )  ,  shuffle = False )   \"  \"  \"  #  #  #   4   -  Entrenar el modelo \"  \"  \"  class Model 1  ( nn . Module )  :  def  _  _ init _  _  ( self ,  vocab _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )   #  num _ embeddings  =  vocab _ size  -  -  >   1  6  2  8  palabras distintas  +   1  para padding o UNK  #  embedding _ dim  =   5   -  -  >  crear embeddings de tama Ã  ± o  5   ( tama Ã  ± o variable y ajustable )  self . lstm _ size  =   6  4  self . num _ layers  =   2  self . embedding  =  nn . Embedding ( num _ embeddings = vocab _ size +  1  ,  embedding _ dim =  5  ,  padding _ idx =  0  )  self . lstm 1   =  nn . LSTM ( input _ size =  5  ,  hidden _ size = self . lstm _ size ,  batch _ first = True ,  num _ layers = self . num _ layers ,  dropout =  0  .  2  )   #  LSTM layer self . fc 1   =  nn . Linear ( in _ features = self . lstm _ size ,  out _ features =  3  2  )   #  Fully connected layer self . fc 2   =  nn . Linear ( in _ features =  3  2  ,  out _ features = output _ dim )   #  Fully connected layer self . relu  =  nn . ReLU (  )  self . softmax  =  nn . Softmax ( dim =  1  )   #  normalize in dim  1  def forward ( self ,  x ,  prev _ state = None )  :  if prev _ state is None :   #  En cada nueva inferencia reinicio el hidden state  #  de la LSTM al menos que sea pasado por par Ã  ¡ metro el  #  elstado de previo  #  Esta acci Ã  ³ n se realiza especialmente para que  #  el hidden _ state de la  Ã  º ltima inferencia no afecte  #  a la siguiente batch _ size  =  x . shape [  0  ]   #  ( batch ,  seq _ size )  prev _ state  =  self . init _ hidden ( batch _ size )  out  =  self . embedding ( x )  lstm _ output ,   ( ht ,  ct )   =  self . lstm 1  ( out ,  prev _ state )  out  =  self . relu ( self . fc 1  ( lstm _ output [  :  ,  -  1  ,  :  ]  )  )   #  take last output  ( last seq )  out  =  self . softmax ( self . fc 2  ( out )  )  return out def init _ hidden ( self ,  batch _ size )  :  return  ( torch . zeros ( self . num _ layers ,  batch _ size ,  self . lstm _ size )  . to ( device )  ,  torch . zeros ( self . num _ layers ,  batch _ size ,  self . lstm _ size )  . to ( device )  )  model 1   =  Model 1  ( vocab _ size = vocab _ size ,  output _ dim = output _ dim )  if cuda :  model 1  . cuda (  )   #  Crear el optimizador la una funci Ã  ³ n de error model 1  _ optimizer  =  torch . optim . Adam ( model 1  . parameters (  )  ,  lr =  0  .  0  0  1  )  model 1  _ criterion  =  torch . nn . CrossEntropyLoss (  )    #  Para clasificaci Ã  ³ n multi categ Ã  ³ rica  #  Por defecto torchinfo testea el modelo con torch . FloatTensor  # summary ( model 1  ,  input _ size =  (  1  ,  input _ size )  ,  dtypes =  [  ' torch . IntTensor '  ]  ,  device = torch . device ( device )  )   #  otra posibilidad summary ( model 1  ,  input _ data = data _ set [  0  ]  [  0  ]  . unsqueeze (  0  )  )  history 1   =  train ( model 1  ,  train _ loader ,  valid _ loader ,  model 1  _ optimizer ,  model 1  _ criterion ,  epochs =  1  0  0   )  epoch _ count  =  range (  1  ,  len ( history 1  [  ' accuracy '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 1  [  ' accuracy '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 1  [  ' val _ accuracy '  ]  ,  label =  ' valid '  )  plt . show (  )   \"  \"  \"  _  _ Es importante _  _  destacar que en este ejemplo estamos entrenando nuestro propios Embeddings y para ello se requiere mucha data .  En los ejemplos que realizaremos de aqu Ã  ­  en m Ã  ¡ s utilizaremos m Ã  ¡ s datos ,  embeddings pre - enternados o modelos pre - entrenados .   #  #  #   5   -  Predicci Ã  ³ n de pr Ã  ³ xima palabra  \"  \"  \"   #  pad _ sequences  #  Si la secuencia de entrada supera al input _ seq _ len  (  3  )  se trunca  #  Si la secuencia es m Ã  ¡ s corta se agregna ceros al comienzo  #  Se utilizar Ã  ¡  gradio para ensayar el modelo  #  Herramienta poderosa para crear interfaces r Ã  ¡ pidas para ensayar modelos  #  https :  /  / gradio . app /  import sys  !  { sys . executable }   - m pip install gradio  -  - quiet import gradio as gr def model _ response ( human _ text )  :   #  Encodeamos encoded  =  tok . texts _ to _ sequences (  [ human _ text ]  )  [  0  ]   #  Si tienen distinto largo encoded  =  pad _ sequences (  [ encoded ]  ,  maxlen =  3  ,  padding =  ' pre '  )   #  Transformo a tensor tensor  =  torch . from _ numpy ( encoded . astype ( np . int 3  2  )  )   #  Predicci Ã  ³ n softmax y _ hat  =  model 1  ( tensor )  . argmax ( axis =  -  1  )   #  Debemos buscar en el vocabulario la palabra  #  que corresopnde al indice  ( y _ hat )  predicho por le modelo out _ word  =   '  '  for word ,  index in tok . word _ index . items (  )  :  if index  =  =  y _ hat :  out _ word  =  word break  #  Agrego la palabra a la frase predicha return human _ text  +   '   '   +  out _ word iface  =  gr . Interface (  fn = model _ response ,  inputs =  [  \" textbox \"  ]  ,  outputs =  \" text \"  ,  layout =  \" vertical \"  )  iface . launch ( debug = True )   \"  \"  \"  #  #  #   6   -  Generaci Ã  ³ n de secuencias nuevas \"  \"  \"  def generate _ seq ( model ,  tokenizer ,  seed _ text ,  max _ length ,  n _ words )  :   \"  \"  \"  Exec model sequence prediction Args :  model  ( keras )  :  modelo entrenado tokenizer  ( keras tokenizer )  :  tonenizer utilizado en el preprocesamiento seed _ text  ( string )  :  texto de entrada  ( input _ seq )  max _ length  ( int )  :  m Ã  ¡ xima longitud de la sequencia de entrada n _ words  ( int )  :  n Ã  º meros de palabras a agregar a la sequencia de entrada returns :  output _ text  ( string )  :  sentencia con las  \" n _ words \"  agregadas  \"  \"  \"  output _ text  =  seed _ text  #  generate a fixed number of words for  _  in range ( n _ words )  :   #  Encodeamos encoded  =  tokenizer . texts _ to _ sequences (  [ output _ text ]  )  [  0  ]   #  Si tienen distinto largo encoded  =  pad _ sequences (  [ encoded ]  ,  maxlen = max _ length ,  padding =  ' pre '  )   #  Transformo a tensor tensor  =  torch . from _ numpy ( encoded . astype ( np . int 3  2  )  )   #  Predicci Ã  ³ n softmax y _ hat  =  model 1  ( tensor )  . argmax ( axis =  -  1  )   #  Vamos concatenando las predicciones out _ word  =   '  '   #  Debemos buscar en el vocabulario la palabra  #  que corresopnde al indice  ( y _ hat )  predicho por le modelo for word ,  index in tokenizer . word _ index . items (  )  :  if index  =  =  y _ hat :  out _ word  =  word break  #  Agrego las palabras a la frase predicha output _ text  +  =   '   '   +  out _ word return output _ text input _ text =  ' hey jude don \\  ' t '  generate _ seq ( model 1  ,  tok ,  input _ text ,  max _ length =  3  ,  n _ words =  2  )   \"  \"  \"  #  #  #   7   -  Conclusiones El modelo entrenado tuvo un muy mail desempe Ã  ± o en el entrenamiento adem Ã  ¡ s de overfitting .  Cuestiones que podr Ã  ­ an mejorarse :   -  Agregar m Ã  ¡ s capas o neuronaes  -  Incrementar la cantidad de  Ã  © pocas  -  Agregar BRNN Es importante destacar que en este ejemplo estamos entrenando nuestro propios Embeddings ,  y para ello se requiere mucha data .  En los ejemplos que realizaremos de aqu Ã  ­  en m Ã  ¡ s utilizaremos m Ã  ¡ s datos ,  embeddings pre - enternados o modelos pre - entrenados .   \"  \"  \"  import random import io import pickle import numpy as np import pandas as pd import matplotlib . pyplot as plt import seaborn as sns import torch import torch . nn . functional as F import torch . nn as nn from torch . utils . data import Dataset ,  DataLoader  #  torchsummar actualmente tiene un problema con las LSTM ,  por eso  #  se utiliza torchinfo ,  un fork del proyecto original con el bug solucionado  ! pip 3  install torchinfo from torchinfo import summary  #  import os import platform if not os . path . exists (  ' torch _ helpers . py '  )  :  if platform . system (  )   =  =   ' Windows '  :  os . system (  ' curl  - o torch _ helpers . py https :  /  / raw . githubusercontent . com / FIUBA - Posgrado - Inteligencia - Artificial / procesamiento _ lenguaje _ natural / main / scripts / torch _ helpers . py '  )  else :  os . system (  ' curl  - o torch _ helpers . py https :  /  / raw . githubusercontent . com / FIUBA - Posgrado - Inteligencia - Artificial / procesamiento _ lenguaje _ natural / main / scripts / torch _ helpers . py '  )  from torch _ helpers import categorical _ acc def train ( model ,  train _ loader ,  valid _ loader ,  optimizer ,  criterion ,  epochs =  1  0  0  )  :   #  Defino listas para realizar graficas de los resultados train _ loss  =   [  ]  train _ accuracy  =   [  ]  valid _ loss  =   [  ]  valid _ accuracy  =   [  ]   #  Defino mi loop de entrenamiento for epoch in range ( epochs )  :  epoch _ train _ loss  =   0  .  0  epoch _ train _ accuracy  =   0  .  0  for train _ data ,  train _ target in train _ loader :   #  Seteo los gradientes en cero ya que ,  por defecto ,  PyTorch  #  los va acumulando optimizer . zero _ grad (  )  output  =  model ( train _ data )   #  Computo el error de la salida comparando contra las etiquetas loss  =  criterion ( output ,  train _ target )   #  Almaceno el error del batch para luego tener el error promedio de la epoca epoch _ train _ loss  +  =  loss . item (  )   #  Computo el nuevo set de gradientes a lo largo de toda la red loss . backward (  )   #  Realizo el paso de optimizacion actualizando los parametros de toda la red optimizer . step (  )   #  Calculo el accuracy del batch accuracy  =  categorical _ acc ( output ,  train _ target )   #  Almaceno el accuracy del batch para luego tener el accuracy promedio de la epoca epoch _ train _ accuracy  +  =  accuracy . item (  )   #  Calculo la media de error para la epoca de entrenamiento .   #  La longitud de train _ loader es igual a la cantidad de batches dentro de una epoca .  epoch _ train _ loss  =  epoch _ train _ loss  /  len ( train _ loader )  train _ loss . append ( epoch _ train _ loss )  epoch _ train _ accuracy  =  epoch _ train _ accuracy  /  len ( train _ loader )  train _ accuracy . append ( epoch _ train _ accuracy )   #  Realizo el paso de validaci Ã  ³ n computando error y accuracy ,  y  #  almacenando los valores para imprimirlos y graficarlos valid _ data ,  valid _ target  =  iter ( valid _ loader )  .  _  _ next _  _  (  )  output  =  model ( valid _ data )  epoch _ valid _ loss  =  criterion ( output ,  valid _ target )  . item (  )  valid _ loss . append ( epoch _ valid _ loss )   #  Calculo el accuracy de la epoch epoch _ valid _ accuracy  =  categorical _ acc ( output ,  valid _ target )  . item (  )  valid _ accuracy . append ( epoch _ valid _ accuracy )  print ( f \" Epoch :   { epoch +  1  }  /  { epochs }   -  Train loss  { epoch _ train _ loss :  .  3 f }   -  Train accuracy  { epoch _ train _ accuracy :  .  3 f }   -  Valid Loss  { epoch _ valid _ loss :  .  3 f }   -  Valid accuracy  { epoch _ valid _ accuracy :  .  3 f }  \"  )  history  =   {   \" loss \"  :  train _ loss ,   \" accuracy \"  :  train _ accuracy ,   \" val _ loss \"  :  valid _ loss ,   \" val _ accuracy \"  :  valid _ accuracy ,   }  return history  \"  \"  \"  #  #  #  Datos Utilizaremos como dataset canciones de bandas de habla ingl Ã  © s .   \"  \"  \"  import os import platform if not os . path . exists (  '  .  / songs _ dataset '  )  :  if not os . path . exists (  ' songs _ dataset . zip '  )  :  if platform . system (  )   =  =   ' Windows '  :  os . system (  ' curl  - o songs _ dataset . zip https :  /  / raw . githubusercontent . com / FIUBA - Posgrado - Inteligencia - Artificial / procesamiento _ lenguaje _ natural / main / datasets / songs _ dataset . zip '  )  else :  os . system (  ' curl  - o songs _ dataset . zip https :  /  / github . com / FIUBA - Posgrado - Inteligencia - Artificial / procesamiento _ lenguaje _ natural / raw / main / datasets / songs _ dataset . zip '  )  os . system (  ' unzip  - q songs _ dataset . zip '  )  else :  print (  \" El dataset ya se encuentra descargado \"  )   #  Posibles bandas os . listdir (  \"  .  / songs _ dataset /  \"  )   #  Armar el dataset utilizando salto de l Ã  ­ nea para separar las oraciones / docs df  =  pd . read _ csv (  ' songs _ dataset / beatles . txt '  ,  sep =  '  / n '  ,  header = None )  df . head (  )  print (  \" Cantidad de documentos :  \"  ,  df . shape [  0  ]  )   \"  \"  \"  #  #  #   1   -  Ejemplo de Preprocesamiento  -  Hay que transformar las oraciones en tokens .   -  Dichas oraciones hay que ajustarlas al tama Ã  ± o fijo de nuestra sentencia de entrada al modelo .   -  Hay que separar las palabras objetivos  ( target )  que el modelo debe predecir en cada sentencia armada .   \"  \"  \"  from torch _ helpers import Tokenizer  #  tool de keras equivalente a ltokenizer de nltk from torch _ helpers import text _ to _ word _ sequence  #  tool de keras equivalente a word _ teokenize de nltk from torch _ helpers import pad _ sequences  #  tool de keras qye se utilizar Ã  ¡  para padding  #  largo de la secuencia ,  incluye seq input  +  word output train _ len  =   4   #  Ejemplo de como transformar una oraci Ã  ³ n a tokens usando keras text  =  df . loc [  0  ,  0  ]  text tokens  =  text _ to _ word _ sequence ( text )   #  entran oraciones  -  >  salen vectores de N posiciones  ( tokens )  tokens  \"  \"  \"  1  .  1   -  Transformar las oraciones en secuencias  ( tokens )  de palabras \"  \"  \"   #  Recorrer todas las filas y transformar las oraciones  #  en secuencias de palabras sentence _ tokens  =   [  ]  for  _  ,  row in df [  : None ]  . iterrows (  )  :  sentence _ tokens . append ( text _ to _ word _ sequence ( row [  0  ]  )  )   #  Demos un vistazo sentence _ tokens [  :  2  ]   #  C Ã  ³ digo para hacer el desfazaje de las palabras  #  seg Ã  º n el train _ len text _ sequences  =   [  ]  for i in range ( train _ len ,  len ( tokens )  )  :  seq  =  tokens [ i - train _ len : i ]  text _ sequences . append ( seq )   #  Demos un vistazo a nuestros vectores para entrenar el modelo  #  seq _ input  +  output text _ sequences  \"  \"  \"  1  .  2   -  Crear los vectores de palabras  ( word 2 vec )  Ahora necesitamos pasarlos a n Ã  º meros para que lo entienda la red y separar input de output .   -  El Input seran integers  ( word 2 vec )   -  Mientras que el output ser Ã  ¡  one hot encodeado  ( labels )  del tama Ã  ± o del vocabulario  \"  \"  \"  tok  =  Tokenizer (  )   #  El tokeinzer  \" aprende \"  las palabras que se usaran  #  Se construye  ( fit )  una vez por proyecto ,  se aplica N veces  ( tal cual un encoder )  tok . fit _ on _ texts ( text _ sequences )   #  Convertimos las palabras a n Ã  º meros  #  entran palabras  -  >  salen n Ã  º meros sequences  =  tok . texts _ to _ sequences ( text _ sequences )   #  Ahora sequences tiene los n Ã  º meros  \" ID \"  ,  largo  4  sequences  #  Cantidad de casos  ( doc )  de entrada print ( tok . document _ count )   #  Cantidad de palabras distintas print ( len ( tok . word _ counts )  )   #  El  Ã  ­ ndice para cada palabra  #  El sistema las ordena de las m Ã  ¡ s populares a las menos populares print ( tok . word _ index )   #  Cantidad de veces quea aparece cada palabra en cada  \" documento \"   #   (  1  documento  =   1  caso de entrada )  print ( tok . word _ docs )   \"  \"  \"  #  #  #   2   -  Preprocesamiento completo Debemos realizar los mismos pasos que en el ejemplo anterior ,  pero antes de eso debemos transformar ese dataset de filas de oraciones en un texto completo continuo para poder extraer el vocabulario .   \"  \"  \"   #  Vistazo a las primeras filas df . loc [  :  1  5  ,  0  ]   #  Concatenamos todos los rows en un solo valor corpus  =  df . apply ( lambda row :   '   '  . join ( row . values . astype ( str )  )  ,  axis =  0  )  [  0  ]  corpus  #  Transformar el corpus a tokens tokens = text _ to _ word _ sequence ( corpus )   #  Vistazo general de los primeros tokens tokens [  :  2  0  ]  print (  \" Cantidad de tokens en el corpus :  \"  ,  len ( tokens )  )   #  C Ã  ³ digo para hacer el desfazaje de las palabras  #  seg Ã  º n el train _ len text _ sequences  =   [  ]  for i in range ( train _ len ,  len ( tokens )  )  :  seq  =  tokens [ i - train _ len : i ]  text _ sequences . append ( seq )   #  Demos un vistazo a nuestros vectores para entrenar el modelo text _ sequences [  :  2  0  ]   #  Proceso de tokenizacion tok  =  Tokenizer (  )  tok . fit _ on _ texts ( text _ sequences )   #  Convertimos las palabras a n Ã  º meros  #  entran palabras  -  >  salen n Ã  º meros sequences  =  tok . texts _ to _ sequences ( text _ sequences )   #  Damos un vistazo sequences [  :  2  0  ]  print (  \" Cantidad de rows del dataset :  \"  ,  len ( sequences )  )   \"  \"  \"  #  #  #   3   -  Input y target \"  \"  \"   #  Con numpy es muy f Ã  ¡ cil realizar el slicing de vectores ex  =  np . array (  [  [  1  ,  2  ,  3  ,  4  ]  ,  [  5  ,  6  ,  7  ,  8  ]  ]  )  ex  #  Con numpy es muy f Ã  ¡ cil realizar el slicing de vectores print (  \" Dimension :  \"  ,  ex . shape )  print (  \" Todos los elementos :  \"  ,  ex )  print (  \" Todos los elementos menos el  Ã  º ltimo :  \"  ,  ex [  :  ,   :  -  1  ]  )  input  =  ex [  :  ,  :  -  1  ]   #  todos los rows ,  menos la ultima col target  =  ex [  :  ,   -  1  ]   #   Ã  º ltima col de cada row print (  \" Input :  \"  ,  input )  print (  \" Target :  \"  ,  target )  arr _ sequences  =  np . array ( sequences )  x _ data  =  arr _ sequences [  :  ,  :  -  1  ]  y _ data _ int  =  arr _ sequences [  :  ,  -  1  ]   #  a Ã  º n falta el oneHotEncoder print ( x _ data . shape )  print ( y _ data _ int . shape )   #  Palabras del vocabulario tok . index _ word  #  Cantidad de palabras en el vocabulario vocab _ size  =  len ( tok . word _ counts )  vocab _ size  #   Â  ¡ Ojo !  y _ data _ int comienza en  \"  1  \"  en vez de  \"  0  \"   #  valor minimo :  min ( y _ data _ int )   #  Hay que restar  1  y _ data _ int _ offset  =  y _ data _ int  -   1  min ( y _ data _ int _ offset )  class Data ( Dataset )  :  def  _  _ init _  _  ( self ,  x ,  y )  :   #  Convertir los arrays de numpy a tensores .   #  pytorch espera en general entradas  3  2 bits self . x  =  torch . from _ numpy ( x . astype ( np . int 3  2  )  )   #  Transformar los datos a oneHotEncoding  #  la loss function esperan la salida float self . y  =  F . one _ hot ( torch . from _ numpy ( y )  ,  num _ classes = vocab _ size )  . float (  )  self . len  =  self . y . shape [  0  ]  def  _  _ getitem _  _  ( self , index )  :  return self . x [ index ]  ,  self . y [ index ]  def  _  _ len _  _  ( self )  :  return self . len data _ set  =  Data ( x _ data ,  y _ data _ int _ offset )  input _ size  =  data _ set . x . shape [  1  ]  print (  \" input _ size :  \"  ,  input _ size )  output _ dim  =  data _ set . y . shape [  1  ]  print (  \" Output dim \"  ,  output _ dim )  torch . manual _ seed (  4  2  )  valid _ set _ size  =  int ( data _ set . len  *   0  .  2  )  train _ set _ size  =  data _ set . len  -  valid _ set _ size train _ set  =  torch . utils . data . Subset ( data _ set ,  range ( train _ set _ size )  )  valid _ set  =  torch . utils . data . Subset ( data _ set ,  range ( train _ set _ size ,  data _ set . len )  )  print (  \" Tama Ã  ± o del conjunto de entrenamiento :  \"  ,  len ( train _ set )  )  print (  \" Tama Ã  ± o del conjunto de validacion :  \"  ,  len ( valid _ set )  )  train _ batch _ size  =   3  2  train _ loader  =  torch . utils . data . DataLoader ( train _ set ,  batch _ size = train _ batch _ size ,  shuffle = True )  valid _ loader  =  torch . utils . data . DataLoader ( valid _ set ,  batch _ size = len ( valid _ set )  ,  shuffle = False )   \"  \"  \"  #  #  #   4   -  Entrenar el modelo \"  \"  \"  class Model 1  ( nn . Module )  :  def  _  _ init _  _  ( self ,  vocab _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )   #  num _ embeddings  =  vocab _ size  -  -  >   1  6  2  8  palabras distintas  +   1  para padding o UNK  #  embedding _ dim  =   5   -  -  >  crear embeddings de tama Ã  ± o  5   ( tama Ã  ± o variable y ajustable )  self . lstm _ size  =   6  4  self . num _ layers  =   2  self . embedding  =  nn . Embedding ( num _ embeddings = vocab _ size +  1  ,  embedding _ dim =  5  ,  padding _ idx =  0  )  self . lstm 1   =  nn . LSTM ( input _ size =  5  ,  hidden _ size = self . lstm _ size ,  batch _ first = True ,  num _ layers = self . num _ layers ,  dropout =  0  .  2  )   #  LSTM layer self . fc 1   =  nn . Linear ( in _ features = self . lstm _ size ,  out _ features =  3  2  )   #  Fully connected layer self . fc 2   =  nn . Linear ( in _ features =  3  2  ,  out _ features = output _ dim )   #  Fully connected layer self . relu  =  nn . ReLU (  )  self . softmax  =  nn . Softmax ( dim =  1  )   #  normalize in dim  1  def forward ( self ,  x ,  prev _ state = None )  :  if prev _ state is None :   #  En cada nueva inferencia reinicio el hidden state  #  de la LSTM al menos que sea pasado por par Ã  ¡ metro el  #  elstado de previo  #  Esta acci Ã  ³ n se realiza especialmente para que  #  el hidden _ state de la  Ã  º ltima inferencia no afecte  #  a la siguiente batch _ size  =  x . shape [  0  ]   #  ( batch ,  seq _ size )  prev _ state  =  self . init _ hidden ( batch _ size )  out  =  self . embedding ( x )  lstm _ output ,   ( ht ,  ct )   =  self . lstm 1  ( out ,  prev _ state )  out  =  self . relu ( self . fc 1  ( lstm _ output [  :  ,  -  1  ,  :  ]  )  )   #  take last output  ( last seq )  out  =  self . softmax ( self . fc 2  ( out )  )  return out def init _ hidden ( self ,  batch _ size )  :  return  ( torch . zeros ( self . num _ layers ,  batch _ size ,  self . lstm _ size )  ,  torch . zeros ( self . num _ layers ,  batch _ size ,  self . lstm _ size )  )  model 1   =  Model 1  ( vocab _ size = vocab _ size ,  output _ dim = output _ dim )   #  Crear el optimizador la una funci Ã  ³ n de error model 1  _ optimizer  =  torch . optim . Adam ( model 1  . parameters (  )  ,  lr =  0  .  0  0  1  )  model 1  _ criterion  =  torch . nn . CrossEntropyLoss (  )    #  Para clasificaci Ã  ³ n multi categ Ã  ³ rica  #  Por defecto torchinfo testea el modelo con torch . FloatTensor summary ( model 1  ,  input _ size =  (  1  ,  input _ size )  ,  dtypes =  [  ' torch . IntTensor '  ]  ,  device = torch . device (  ' cpu '  )  )   #  otra posibilidad  # summary ( model 1  ,  input _ data = data _ set [  0  ]  [  0  ]  . unsqueeze (  0  )  )  history 1   =  train ( model 1  ,  train _ loader ,  valid _ loader ,  model 1  _ optimizer ,  model 1  _ criterion ,  epochs =  1  0  0   )  epoch _ count  =  range (  1  ,  len ( history 1  [  ' accuracy '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 1  [  ' accuracy '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 1  [  ' val _ accuracy '  ]  ,  label =  ' valid '  )  plt . show (  )   \"  \"  \"  _  _ Es importante _  _  destacar que en este ejemplo estamos entrenando nuestro propios Embeddings y para ello se requiere mucha data .  En los ejemplos que realizaremos de aqu Ã  ­  en m Ã  ¡ s utilizaremos m Ã  ¡ s datos ,  embeddings pre - enternados o modelos pre - entrenados .   #  #  #   5   -  Predicci Ã  ³ n de pr Ã  ³ xima palabra  \"  \"  \"   #  pad _ sequences  #  Si la secuencia de entrada supera al input _ seq _ len  (  3  )  se trunca  #  Si la secuencia es m Ã  ¡ s corta se agregna ceros al comienzo  #  Se utilizar Ã  ¡  gradio para ensayar el modelo  #  Herramienta poderosa para crear interfaces r Ã  ¡ pidas para ensayar modelos  #  https :  /  / gradio . app /  import sys  !  { sys . executable }   - m pip install gradio  -  - quiet import gradio as gr def model _ response ( human _ text )  :   #  Encodeamos encoded  =  tok . texts _ to _ sequences (  [ human _ text ]  )  [  0  ]   #  Si tienen distinto largo encoded  =  pad _ sequences (  [ encoded ]  ,  maxlen =  3  ,  padding =  ' pre '  )   #  Transformo a tensor tensor  =  torch . from _ numpy ( encoded . astype ( np . int 3  2  )  )   #  Predicci Ã  ³ n softmax y _ hat  =  model 1  ( tensor )  . argmax ( axis =  -  1  )   #  Debemos buscar en el vocabulario la palabra  #  que corresopnde al indice  ( y _ hat )  predicho por le modelo out _ word  =   '  '  for word ,  index in tok . word _ index . items (  )  :  if index  =  =  y _ hat :  out _ word  =  word break  #  Agrego la palabra a la frase predicha return human _ text  +   '   '   +  out _ word iface  =  gr . Interface (  fn = model _ response ,  inputs =  [  \" textbox \"  ]  ,  outputs =  \" text \"  ,  layout =  \" vertical \"  )  iface . launch ( debug = True )   \"  \"  \"  #  #  #   6   -  Generaci Ã  ³ n de secuencias nuevas \"  \"  \"  def generate _ seq ( model ,  tokenizer ,  seed _ text ,  max _ length ,  n _ words )  :   \"  \"  \"  Exec model sequence prediction Args :  model  ( keras )  :  modelo entrenado tokenizer  ( keras tokenizer )  :  tonenizer utilizado en el preprocesamiento seed _ text  ( string )  :  texto de entrada  ( input _ seq )  max _ length  ( int )  :  m Ã  ¡ xima longitud de la sequencia de entrada n _ words  ( int )  :  n Ã  º meros de palabras a agregar a la sequencia de entrada returns :  output _ text  ( string )  :  sentencia con las  \" n _ words \"  agregadas  \"  \"  \"  output _ text  =  seed _ text  #  generate a fixed number of words for  _  in range ( n _ words )  :   #  Encodeamos encoded  =  tokenizer . texts _ to _ sequences (  [ output _ text ]  )  [  0  ]   #  Si tienen distinto largo encoded  =  pad _ sequences (  [ encoded ]  ,  maxlen = max _ length ,  padding =  ' pre '  )   #  Transformo a tensor tensor  =  torch . from _ numpy ( encoded . astype ( np . int 3  2  )  )   #  Predicci Ã  ³ n softmax y _ hat  =  model 1  ( tensor )  . argmax ( axis =  -  1  )   #  Vamos concatenando las predicciones out _ word  =   '  '   #  Debemos buscar en el vocabulario la palabra  #  que corresopnde al indice  ( y _ hat )  predicho por le modelo for word ,  index in tokenizer . word _ index . items (  )  :  if index  =  =  y _ hat :  out _ word  =  word break  #  Agrego las palabras a la frase predicha output _ text  +  =   '   '   +  out _ word return output _ text input _ text =  ' hey jude don \\  ' t '  generate _ seq ( model 1  ,  tok ,  input _ text ,  max _ length =  3  ,  n _ words =  2  )   \"  \"  \"  #  #  #   7   -  Conclusiones El modelo entrenado tuvo un muy mail desempe Ã  ± o en el entrenamiento adem Ã  ¡ s de overfitting .  Cuestiones que podr Ã  ­ an mejorarse :   -  Agregar m Ã  ¡ s capas o neuronaes  -  Incrementar la cantidad de  Ã  © pocas  -  Agregar BRNN Es importante destacar que en este ejemplo estamos entrenando nuestro propios Embeddings ,  y para ello se requiere mucha data .  En los ejemplos que realizaremos de aqu Ã  ­  en m Ã  ¡ s utilizaremos m Ã  ¡ s datos ,  embeddings pre - enternados o modelos pre - entrenados .   \"  \"  \"  import re import numpy as np import matplotlib . pyplot as plt import seaborn as sns import torch import torch . nn . functional as F import torch . nn as nn from torch . utils . data import Dataset ,  DataLoader  #  torchsummar actualmente tiene un problema con las LSTM ,  por eso  #  se utiliza torchinfo ,  un fork del proyecto original con el bug solucionado  ! pip 3  install torchinfo from torchinfo import summary import os import platform if os . access (  ' torch _ helpers . py '  ,  os . F _ OK )  is False :  if platform . system (  )   =  =   ' Windows '  :   ! curl  ! wget https :  /  / raw . githubusercontent . com / FIUBA - Posgrado - Inteligencia - Artificial / procesamiento _ lenguaje _ natural / main / scripts / torch _ helpers . py  >  torch _ helpers . py else :   ! wget torch _ helpers . py https :  /  / raw . githubusercontent . com / FIUBA - Posgrado - Inteligencia - Artificial / procesamiento _ lenguaje _ natural / main / scripts / torch _ helpers . py def train ( model ,  train _ loader ,  valid _ loader ,  optimizer ,  criterion ,  epochs =  1  0  0  )  :   #  Defino listas para realizar graficas de los resultados train _ loss  =   [  ]  valid _ loss  =   [  ]   #  Defino mi loop de entrenamiento for epoch in range ( epochs )  :  epoch _ train _ loss  =   0  .  0  epoch _ train _ accuracy  =   0  .  0  for train _ data ,  train _ target in train _ loader :   #  Seteo los gradientes en cero ya que ,  por defecto ,  PyTorch  #  los va acumulando optimizer . zero _ grad (  )  output  =  model ( train _ data )   #  Computo el error de la salida comparando contra las etiquetas loss  =  criterion ( output ,  train _ target )   #  Almaceno el error del batch para luego tener el error promedio de la epoca epoch _ train _ loss  +  =  loss . item (  )   #  Computo el nuevo set de gradientes a lo largo de toda la red loss . backward (  )   #  Realizo el paso de optimizacion actualizando los parametros de toda la red optimizer . step (  )   #  Calculo la media de error para la epoca de entrenamiento .   #  La longitud de train _ loader es igual a la cantidad de batches dentro de una epoca .  epoch _ train _ loss  =  epoch _ train _ loss  /  len ( train _ loader )  train _ loss . append ( epoch _ train _ loss )   #  Realizo el paso de validaci Ã  ³ n computando error y accuracy ,  y  #  almacenando los valores para imprimirlos y graficarlos valid _ data ,  valid _ target  =  iter ( valid _ loader )  . next (  )  output  =  model ( valid _ data )  epoch _ valid _ loss  =  criterion ( output ,  valid _ target )  . item (  )  valid _ loss . append ( epoch _ valid _ loss )  print ( f \" Epoch :   { epoch +  1  }  /  { epochs }   -  Train loss  { epoch _ train _ loss :  .  3 f }   -  Valid Loss  { epoch _ valid _ loss :  .  3 f }  \"  )  history  =   {   \" loss \"  :  train _ loss ,   \" val _ loss \"  :  valid _ loss ,   }  return history  #  Generar datos sint Ã  © ticos X  =  list (  )  y  =  list (  )  X  =   [ x +  1  for x in range (  2  0  )  ]   #   \" y \"   ( target )  se obtiene como cada dato de entrada multiplicado por  1  5  y  =   [ x  *   1  5  for x in X ]  print (  \" datos X :  \"  ,  X )  print (  \" datos y :  \"  ,  y )   #  Cada dato X lo transformarmos en una matriz de  1  fila  1  columna  (  1 x 1  )  X  =  np . array ( X )  . reshape ( len ( X )  ,   1  ,   1  )  print (  \" datos X :  \"  ,  X )   #   ( batch size ,  seq _ len ,  input _ size )  X . shape y  =  np . asanyarray ( y )  y . shape class Data ( Dataset )  :  def  _  _ init _  _  ( self ,  x ,  y )  :   #  Convertir los arrays de numpy a tensores .   #  pytorch espera en general entradas  3  2 bits self . x  =  torch . from _ numpy ( x . astype ( np . float 3  2  )  )   #  las loss unfction esperan la salida float self . y  =  torch . from _ numpy ( y . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  self . len  =  self . y . shape [  0  ]  def  _  _ getitem _  _  ( self , index )  :  return self . x [ index ]  ,  self . y [ index ]  def  _  _ len _  _  ( self )  :  return self . len data _ set  =  Data ( X ,  y )  input _ dim  =  data _ set . x . shape [  1  :  ]  seq _ length  =  input _ dim [  0  ]  input _ size  =  input _ dim [  1  ]  print (  \" Input dim \"  ,  input _ dim )  print (  \" seq _ length :  \"  ,  seq _ length )  print (  \" input _ size :  \"  ,  input _ size )  output _ dim  =  data _ set . y . shape [  1  ]  print (  \" Output dim \"  ,  output _ dim )  data _ set . x . shape data _ set . y . shape torch . manual _ seed (  4  2  )  valid _ set _ size  =  int ( data _ set . len  *   0  .  2  )  train _ set _ size  =  data _ set . len  -  valid _ set _ size  #  Cuando trabajmos con una serie temporal no mezclamos  ( shuffle )  los datos train _ set  =  torch . utils . data . Subset ( data _ set ,  range ( train _ set _ size )  )  valid _ set  =  torch . utils . data . Subset ( data _ set ,  range ( train _ set _ size ,  data _ set . len )  )  print (  \" Tama Ã  ± o del conjunto de entrenamiento :  \"  ,  len ( train _ set )  )  print (  \" Tama Ã  ± o del conjunto de validacion :  \"  ,  len ( valid _ set )  )  train _ loader  =  torch . utils . data . DataLoader ( train _ set ,  batch _ size = len ( train _ set )  ,  shuffle = False )  valid _ loader  =  torch . utils . data . DataLoader ( valid _ set ,  batch _ size = len ( valid _ set )  ,  shuffle = False )   \"  \"  \"  #  #  #   2   -  Entrenar el modelo  ( RNN y LSTM )  \"  \"  \"  from torch _ helpers import CustomRNN class Model 1  ( nn . Module )  :  def  _  _ init _  _  ( self ,  input _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )   # self . rnn 1   =  nn . RNN ( input _ size = input _ size ,  hidden _ size =  6  4  ,  batch _ first = True )   #  RNN layer  #  Utilizamos la CustomRNN ya que para series temporales suele funcionar mejor  #  la activacion  \" relu \"  en las RNN en vez de la  \" tanh \"  ,  pero por defecto la  #  layer de Pytorch RNN no permite modificar la funcion de activacion  # self . rnn 1   =  CustomRNN ( input _ size = input _ size ,  hidden _ size =  6  4  )   #  RNN layer self . rnn 1   =  CustomRNN ( input _ size = input _ size ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  RNN layer self . fc  =  nn . Linear ( in _ features =  6  4  ,  out _ features = output _ dim )   #    #  Fully connected layer def forward ( self ,  x )  :  lstm _ output ,   _   =  self . rnn 1  ( x )  out  =  self . fc ( lstm _ output [  :  ,  -  1  ,  :  ]  )   #  take last output  ( last seq )  return out model 1   =  Model 1  ( input _ size = input _ size ,  output _ dim = output _ dim )   #  Crear el optimizador la una funci Ã  ³ n de error model 1  _ optimizer  =  torch . optim . Adam ( model 1  . parameters (  )  ,  lr =  0  .  0  1  )  model 1  _ criterion  =  nn . MSELoss (  )    #  mean squared error summary ( model 1  ,  input _ size =  (  1  ,  seq _ length ,  input _ size )  )  history 1   =  train ( model 1  ,  train _ loader ,  valid _ loader ,  model 1  _ optimizer ,  model 1  _ criterion ,  epochs =  5  0  0   )  epoch _ count  =  range (  1  ,  len ( history 1  [  ' loss '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 1  [  ' loss '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 1  [  ' val _ loss '  ]  ,  label =  ' valid '  )  plt . show (  )   #  Ensayo  #  x  =   3  0   #  y _ test  =  x  *   1  5  x _ test  =   3  0  y _ test  =  x _ test  *   1  5  test _ input  =  np . array (  [ x _ test ]  )  test _ input  =  test _ input . reshape (  (  1  ,  seq _ length ,  input _ size )  )  test _ input  =  torch . from _ numpy ( test _ input . astype ( np . float 3  2  )  )  test _ target  =  torch . from _ numpy ( np . array ( y _ test )  . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  y _ hat  =  model 1  ( test _ input )  print (  \" y _ test :  \"  ,  y _ test )  print (  \" y _ hat :  \"  ,  y _ hat )  loss  =  model 1  _ criterion ( y _ hat ,  test _ target )  . item (  )  print (  \" loss :  \"  ,  loss )  from torch _ helpers import CustomLSTM class Model 2  ( nn . Module )  :  def  _  _ init _  _  ( self ,  input _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )   # self . lstm 1   =  nn . LSTM ( input _ size = input _ size ,  hidden _ size =  6  4  ,  batch _ first = True )   #  LSTM layer  #  Utilizamos la CustomLSTM ya que para series temporales suele funcionar mejor  #  la activacion  \" relu \"  en las LSTM en vez de la  \" tanh \"  ,  pero por defecto la  #  layer de Pytorch LSTM no permite modificar la funcion de activacion self . lstm 1   =  CustomLSTM ( input _ size = input _ size ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  LSTM layer self . fc  =  nn . Linear ( in _ features =  6  4  ,  out _ features = output _ dim )   #    #  Fully connected layer def forward ( self ,  x )  :  lstm _ output ,   _   =  self . lstm 1  ( x )  out  =  self . fc ( lstm _ output [  :  ,  -  1  ,  :  ]  )   #  take last output  ( last seq )  return out model 2   =  Model 2  ( input _ size = input _ size ,  output _ dim = output _ dim )   #  Crear el optimizador la una funci Ã  ³ n de error model 2  _ optimizer  =  torch . optim . Adam ( model 2  . parameters (  )  ,  lr =  0  .  0  1  )  model 2  _ criterion  =  nn . MSELoss (  )    #  mean squared error summary ( model 2  ,  input _ size =  (  1  ,  seq _ length ,  input _ size )  )  history 2   =  train ( model 2  ,  train _ loader ,  valid _ loader ,  model 2  _ optimizer ,  model 2  _ criterion ,  epochs =  5  0  0   )  epoch _ count  =  range (  1  ,  len ( history 2  [  ' loss '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 2  [  ' loss '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 2  [  ' val _ loss '  ]  ,  label =  ' valid '  )  plt . show (  )   #  Ensayo  #  x  =   3  0   #  y _ test  =  x  *   1  5  x _ test  =   3  0  y _ test  =  x _ test  *   1  5  test _ input  =  np . array (  [ x _ test ]  )  test _ input  =  test _ input . reshape (  (  1  ,  seq _ length ,  input _ size )  )  test _ input  =  torch . from _ numpy ( test _ input . astype ( np . float 3  2  )  )  test _ target  =  torch . from _ numpy ( np . array ( y _ test )  . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  y _ hat  =  model 2  ( test _ input )  print (  \" y _ test :  \"  ,  y _ test )  print (  \" y _ hat :  \"  ,  y _ hat )  loss  =  model 2  _ criterion ( y _ hat ,  test _ target )  . item (  )  print (  \" loss :  \"  ,  loss )   \"  \"  \" Se puede observar que para un problema tan simple como este no hay mucha diferencia entre utilizar una RNN o LSTM .  La LSTM tiene muchos m Ã  ¡ s par Ã  ¡ metros que la RNN .   #  #  #   3   -  Multi - layer LSTM  \"  \"  \"  from torch _ helpers import CustomLSTM  #  En esta oportunidad se utilizar Ã  ¡ n dos layer LSTM class Model 3  ( nn . Module )  :  def  _  _ init _  _  ( self ,  input _ size ,  output _ dim )  :  super (  )  .  _  _ init _  _  (  )  self . lstm 1   =  CustomLSTM ( input _ size = input _ size ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  LSTM layer self . lstm 2   =  CustomLSTM ( input _ size =  6  4  ,  hidden _ size =  6  4  ,  activation = nn . ReLU (  )  )   #  LSTM layer self . fc  =  nn . Linear ( in _ features =  6  4  ,  out _ features = output _ dim )   #    #  Fully connected layer def forward ( self ,  x )  :  lstm _ output ,   _   =  self . lstm 1  ( x )  lstm _ output ,   _   =  self . lstm 2  ( lstm _ output )  out  =  self . fc ( lstm _ output [  :  ,  -  1  ,  :  ]  )   #  take last output  ( last seq )  return out model 3   =  Model 3  ( input _ size = input _ size ,  output _ dim = output _ dim )   #  Crear el optimizador la una funci Ã  ³ n de error model 3  _ optimizer  =  torch . optim . Adam ( model 3  . parameters (  )  ,  lr =  0  .  0  1  )  model 3  _ criterion  =  nn . MSELoss (  )    #  mean squared error summary ( model 3  ,  input _ size =  (  1  ,  seq _ length ,  input _ size )  )  history 3   =  train ( model 3  ,  train _ loader ,  valid _ loader ,  model 3  _ optimizer ,  model 3  _ criterion ,  epochs =  5  0  0   )  epoch _ count  =  range (  1  ,  len ( history 3  [  ' loss '  ]  )   +   1  )  sns . lineplot ( x = epoch _ count ,   y = history 3  [  ' loss '  ]  ,  label =  ' train '  )  sns . lineplot ( x = epoch _ count ,   y = history 3  [  ' val _ loss '  ]  ,  label =  ' valid '  )  plt . show (  )   #  Ensayo  #  x  =   3  0   #  y _ test  =  x  *   1  5  x _ test  =   3  0  y _ test  =  x _ test  *   1  5  test _ input  =  np . array (  [ x _ test ]  )  test _ input  =  test _ input . reshape (  (  1  ,  seq _ length ,  input _ size )  )  test _ input  =  torch . from _ numpy ( test _ input . astype ( np . float 3  2  )  )  test _ target  =  torch . from _ numpy ( np . array ( y _ test )  . astype ( np . int 3  2  )  )  . float (  )  . view (  -  1  ,   1  )  y _ hat  =  model 3  ( test _ input )  print (  \" y _ test :  \"  ,  y _ test )  print (  \" y _ hat :  \"  ,  y _ hat )  loss  =  model 3  _ criterion ( y _ hat ,  test _ target )  . item (  )  print (  \" loss :  \"  ,  loss )   \"  \"  \"  #  #  #   4   -  Conclusi Ã  ³ n El resultado alcanzado es bueno pero podr Ã  ­ a mejorarse agregando m Ã  ¡ s layer LSTM o m Ã  ¡ s layer fully connected .   \"  \"  \" \n"
     ]
    }
   ],
   "source": [
    "def add_spaces(script):\n",
    "    pattern = r'([^a-zA-Z\\s])'  # Matches any character that is not a letter or space\n",
    "    spaced_script = re.sub(pattern, r' \\1 ', script)\n",
    "    return spaced_script\n",
    "\n",
    "corpus = add_spaces(temp_corpus)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x',\n",
       " '2',\n",
       " 'y',\n",
       " 'x',\n",
       " '1',\n",
       " 'x',\n",
       " '2',\n",
       " 'for',\n",
       " 'x',\n",
       " 'in',\n",
       " 'x',\n",
       " 'print',\n",
       " 'datos',\n",
       " 'x',\n",
       " 'x',\n",
       " 'print',\n",
       " 'datos',\n",
       " 'y',\n",
       " 'y',\n",
       " 'cada',\n",
       " 'dato',\n",
       " 'x',\n",
       " 'lo',\n",
       " 'transformarmos',\n",
       " 'en',\n",
       " 'una',\n",
       " 'matriz',\n",
       " 'de',\n",
       " '1',\n",
       " 'fila',\n",
       " '1',\n",
       " 'columna',\n",
       " '1',\n",
       " 'x',\n",
       " '1',\n",
       " 'x',\n",
       " 'np',\n",
       " 'array',\n",
       " 'x',\n",
       " 'reshape',\n",
       " 'len',\n",
       " 'x',\n",
       " '1',\n",
       " '1',\n",
       " 'print',\n",
       " 'datos',\n",
       " 'x',\n",
       " 'x',\n",
       " 'batch',\n",
       " 'size',\n",
       " 'seq',\n",
       " 'len',\n",
       " 'input',\n",
       " 'size',\n",
       " 'x',\n",
       " 'shape',\n",
       " 'y',\n",
       " 'np',\n",
       " 'asanyarray',\n",
       " 'y',\n",
       " 'y',\n",
       " 'shape',\n",
       " 'class',\n",
       " 'data',\n",
       " 'dataset',\n",
       " 'def',\n",
       " 'init',\n",
       " 'self',\n",
       " 'x',\n",
       " 'y',\n",
       " 'convertir',\n",
       " 'los',\n",
       " 'arrays',\n",
       " 'de',\n",
       " 'numpy',\n",
       " 'a',\n",
       " 'tensores',\n",
       " 'pytorch',\n",
       " 'espera',\n",
       " 'en',\n",
       " 'general',\n",
       " 'entradas',\n",
       " '3',\n",
       " '2',\n",
       " 'bits',\n",
       " 'self',\n",
       " 'x',\n",
       " 'torch',\n",
       " 'from',\n",
       " 'numpy',\n",
       " 'x',\n",
       " 'astype',\n",
       " 'np',\n",
       " 'float',\n",
       " '3',\n",
       " '2',\n",
       " 'las',\n",
       " 'loss',\n",
       " 'unfction',\n",
       " 'esperan',\n",
       " 'la',\n",
       " 'salida',\n",
       " 'float',\n",
       " 'self',\n",
       " 'y',\n",
       " 'torch',\n",
       " 'from',\n",
       " 'numpy',\n",
       " 'y',\n",
       " 'astype',\n",
       " 'np',\n",
       " 'float',\n",
       " '3',\n",
       " '2',\n",
       " 'self',\n",
       " 'len',\n",
       " 'self',\n",
       " 'y',\n",
       " 'shape',\n",
       " '0',\n",
       " 'def',\n",
       " 'getitem',\n",
       " 'self',\n",
       " 'index',\n",
       " 'return',\n",
       " 'self',\n",
       " 'x',\n",
       " 'index',\n",
       " 'self',\n",
       " 'y',\n",
       " 'index',\n",
       " 'def',\n",
       " 'len',\n",
       " 'self',\n",
       " 'return',\n",
       " 'self',\n",
       " 'len',\n",
       " 'data',\n",
       " 'set',\n",
       " 'data',\n",
       " 'x',\n",
       " 'y',\n",
       " 'input',\n",
       " 'dim',\n",
       " 'data',\n",
       " 'set',\n",
       " 'x',\n",
       " 'shape',\n",
       " '1',\n",
       " 'seq',\n",
       " 'length',\n",
       " 'input',\n",
       " 'dim',\n",
       " '0',\n",
       " 'input',\n",
       " 'size',\n",
       " 'input',\n",
       " 'dim',\n",
       " '1',\n",
       " 'print',\n",
       " 'input',\n",
       " 'dim',\n",
       " 'input',\n",
       " 'dim',\n",
       " 'print',\n",
       " 'seq',\n",
       " 'length',\n",
       " 'seq',\n",
       " 'length',\n",
       " 'print',\n",
       " 'input',\n",
       " 'size',\n",
       " 'input',\n",
       " 'size',\n",
       " 'output',\n",
       " 'dim',\n",
       " 'data',\n",
       " 'set',\n",
       " 'y',\n",
       " 'shape',\n",
       " '1',\n",
       " 'print',\n",
       " 'output',\n",
       " 'dim',\n",
       " 'output',\n",
       " 'dim',\n",
       " 'data',\n",
       " 'set',\n",
       " 'x',\n",
       " 'shape',\n",
       " 'data',\n",
       " 'set',\n",
       " 'y',\n",
       " 'shape',\n",
       " 'torch',\n",
       " 'manual',\n",
       " 'seed',\n",
       " '4',\n",
       " '2',\n",
       " 'valid',\n",
       " 'set',\n",
       " 'size',\n",
       " 'int',\n",
       " 'data',\n",
       " 'set',\n",
       " 'len',\n",
       " '0',\n",
       " '2',\n",
       " 'train',\n",
       " 'set',\n",
       " 'size',\n",
       " 'data',\n",
       " 'set',\n",
       " 'len',\n",
       " 'valid',\n",
       " 'set',\n",
       " 'size',\n",
       " 'cuando',\n",
       " 'trabajmos',\n",
       " 'con',\n",
       " 'una',\n",
       " 'serie',\n",
       " 'temporal',\n",
       " 'no',\n",
       " 'mezclamos',\n",
       " 'shuffle',\n",
       " 'los',\n",
       " 'datos',\n",
       " 'train',\n",
       " 'set',\n",
       " 'torch',\n",
       " 'utils',\n",
       " 'data',\n",
       " 'subset',\n",
       " 'data',\n",
       " 'set',\n",
       " 'range',\n",
       " 'train',\n",
       " 'set',\n",
       " 'size',\n",
       " 'valid',\n",
       " 'set',\n",
       " 'torch',\n",
       " 'utils',\n",
       " 'data',\n",
       " 'subset',\n",
       " 'data',\n",
       " 'set',\n",
       " 'range',\n",
       " 'train',\n",
       " 'set',\n",
       " 'size',\n",
       " 'data',\n",
       " 'set',\n",
       " 'len',\n",
       " 'print',\n",
       " 'tama',\n",
       " 'ã',\n",
       " '±',\n",
       " 'o',\n",
       " 'del',\n",
       " 'conjunto',\n",
       " 'de',\n",
       " 'entrenamiento',\n",
       " 'len',\n",
       " 'train',\n",
       " 'set',\n",
       " 'print',\n",
       " 'tama',\n",
       " 'ã',\n",
       " '±',\n",
       " 'o',\n",
       " 'del',\n",
       " 'conjunto',\n",
       " 'de',\n",
       " 'validacion',\n",
       " 'len',\n",
       " 'valid',\n",
       " 'set',\n",
       " 'train',\n",
       " 'loader',\n",
       " 'torch',\n",
       " 'utils',\n",
       " 'data',\n",
       " 'dataloader',\n",
       " 'train',\n",
       " 'set',\n",
       " 'batch',\n",
       " 'size',\n",
       " 'len',\n",
       " 'train',\n",
       " 'set',\n",
       " 'shuffle',\n",
       " 'false',\n",
       " 'valid',\n",
       " 'loader',\n",
       " 'torch',\n",
       " 'utils',\n",
       " 'data',\n",
       " 'dataloader',\n",
       " 'valid',\n",
       " 'set',\n",
       " 'batch',\n",
       " 'size',\n",
       " 'len',\n",
       " 'valid',\n",
       " 'set',\n",
       " 'shuffle',\n",
       " 'false',\n",
       " '2',\n",
       " 'entrenar',\n",
       " 'el',\n",
       " 'modelo',\n",
       " 'from',\n",
       " 'torch',\n",
       " 'helpers',\n",
       " 'import',\n",
       " 'customlstm',\n",
       " 'class',\n",
       " 'model',\n",
       " '1',\n",
       " 'nn',\n",
       " 'module',\n",
       " 'def',\n",
       " 'init',\n",
       " 'self',\n",
       " 'input',\n",
       " 'size',\n",
       " 'output',\n",
       " 'dim',\n",
       " 'super',\n",
       " 'init',\n",
       " 'self',\n",
       " 'rnn',\n",
       " '1',\n",
       " 'nn',\n",
       " 'rnn',\n",
       " 'input',\n",
       " 'size',\n",
       " 'input',\n",
       " 'size',\n",
       " 'hidden',\n",
       " 'size',\n",
       " '6',\n",
       " '4',\n",
       " 'batch',\n",
       " 'first',\n",
       " 'true',\n",
       " 'rnn',\n",
       " 'layer',\n",
       " 'utilizamos',\n",
       " 'la',\n",
       " 'customrnn',\n",
       " 'ya',\n",
       " 'que',\n",
       " 'para',\n",
       " 'series',\n",
       " 'temporales',\n",
       " 'suele',\n",
       " 'funcionar',\n",
       " 'mejor',\n",
       " 'la',\n",
       " 'activacion',\n",
       " 'relu',\n",
       " 'en',\n",
       " 'las',\n",
       " 'rnn',\n",
       " 'en',\n",
       " 'vez',\n",
       " 'de',\n",
       " 'la',\n",
       " 'tanh',\n",
       " 'pero',\n",
       " 'por',\n",
       " 'defecto',\n",
       " 'la',\n",
       " 'layer',\n",
       " 'de',\n",
       " 'pytorch',\n",
       " 'rnn',\n",
       " 'no',\n",
       " 'permite',\n",
       " 'modificar',\n",
       " 'la',\n",
       " 'funcion',\n",
       " 'de',\n",
       " 'activacion',\n",
       " 'self',\n",
       " 'rnn',\n",
       " '1',\n",
       " 'customrnn',\n",
       " 'input',\n",
       " 'size',\n",
       " 'input',\n",
       " 'size',\n",
       " 'hidden',\n",
       " 'size',\n",
       " '6',\n",
       " '4',\n",
       " 'rnn',\n",
       " 'layer',\n",
       " 'self',\n",
       " 'rnn',\n",
       " '1',\n",
       " 'customlstm',\n",
       " 'input',\n",
       " 'size',\n",
       " 'input',\n",
       " 'size',\n",
       " 'hidden',\n",
       " 'size',\n",
       " '6',\n",
       " '4',\n",
       " 'activation',\n",
       " 'nn',\n",
       " 'relu',\n",
       " 'rnn',\n",
       " 'layer',\n",
       " 'self',\n",
       " 'fc',\n",
       " 'nn',\n",
       " 'linear',\n",
       " 'in',\n",
       " 'features',\n",
       " '6',\n",
       " '4',\n",
       " 'out',\n",
       " 'features',\n",
       " 'output',\n",
       " 'dim',\n",
       " 'fully',\n",
       " 'connected',\n",
       " 'layer',\n",
       " 'def',\n",
       " 'forward',\n",
       " 'self',\n",
       " 'x',\n",
       " 'lstm',\n",
       " 'output',\n",
       " 'self',\n",
       " 'rnn',\n",
       " '1',\n",
       " 'x',\n",
       " 'out',\n",
       " 'self',\n",
       " 'fc',\n",
       " 'lstm',\n",
       " 'output',\n",
       " '1',\n",
       " 'take',\n",
       " 'last',\n",
       " 'output',\n",
       " 'last',\n",
       " 'seq',\n",
       " 'return',\n",
       " 'out',\n",
       " 'model',\n",
       " '1',\n",
       " 'model',\n",
       " '1',\n",
       " 'input',\n",
       " 'size',\n",
       " 'input',\n",
       " 'size',\n",
       " 'output',\n",
       " 'dim',\n",
       " 'output',\n",
       " 'dim',\n",
       " 'crear',\n",
       " 'el',\n",
       " 'optimizador',\n",
       " 'la',\n",
       " 'una',\n",
       " 'funci',\n",
       " 'ã',\n",
       " '³',\n",
       " 'n',\n",
       " 'de',\n",
       " 'error',\n",
       " 'model',\n",
       " '1',\n",
       " 'optimizer',\n",
       " 'torch',\n",
       " 'optim',\n",
       " 'adam',\n",
       " 'model',\n",
       " '1',\n",
       " 'parameters',\n",
       " 'lr',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " 'model',\n",
       " '1',\n",
       " 'criterion',\n",
       " 'nn',\n",
       " 'mseloss',\n",
       " 'mean',\n",
       " 'squared',\n",
       " 'error',\n",
       " 'summary',\n",
       " 'model',\n",
       " '1',\n",
       " 'input',\n",
       " 'size',\n",
       " '1',\n",
       " 'seq',\n",
       " 'length',\n",
       " 'input',\n",
       " 'size',\n",
       " 'history',\n",
       " '1',\n",
       " 'train',\n",
       " 'model',\n",
       " '1',\n",
       " 'train',\n",
       " 'loader',\n",
       " 'valid',\n",
       " 'loader',\n",
       " 'model',\n",
       " '1',\n",
       " 'optimizer',\n",
       " 'model',\n",
       " '1',\n",
       " 'criterion',\n",
       " 'epochs',\n",
       " '5',\n",
       " '0',\n",
       " '0',\n",
       " 'epoch',\n",
       " 'count',\n",
       " 'range',\n",
       " '1',\n",
       " 'len',\n",
       " 'history',\n",
       " '1',\n",
       " \"'\",\n",
       " 'loss',\n",
       " \"'\",\n",
       " '1',\n",
       " 'sns',\n",
       " 'lineplot',\n",
       " 'x',\n",
       " 'epoch',\n",
       " 'count',\n",
       " 'y',\n",
       " 'history',\n",
       " '1',\n",
       " \"'\",\n",
       " 'loss',\n",
       " \"'\",\n",
       " 'label',\n",
       " \"'\",\n",
       " 'train',\n",
       " \"'\",\n",
       " 'sns',\n",
       " 'lineplot',\n",
       " 'x',\n",
       " 'epoch',\n",
       " 'count',\n",
       " 'y',\n",
       " 'history',\n",
       " '1',\n",
       " \"'\",\n",
       " 'val',\n",
       " 'loss',\n",
       " \"'\",\n",
       " 'label',\n",
       " \"'\",\n",
       " 'valid',\n",
       " \"'\",\n",
       " 'plt',\n",
       " 'show',\n",
       " 'ensayo',\n",
       " 'x',\n",
       " 'test',\n",
       " '1',\n",
       " '0',\n",
       " 'y',\n",
       " 'test',\n",
       " 'x',\n",
       " 'test',\n",
       " '1',\n",
       " 'x',\n",
       " 'test',\n",
       " '2',\n",
       " 'test',\n",
       " 'input',\n",
       " 'np',\n",
       " 'array',\n",
       " 'x',\n",
       " 'test',\n",
       " 'test',\n",
       " 'input',\n",
       " 'test',\n",
       " 'input',\n",
       " 'reshape',\n",
       " '1',\n",
       " 'seq',\n",
       " 'length',\n",
       " 'input',\n",
       " 'size',\n",
       " 'test',\n",
       " 'input',\n",
       " 'torch',\n",
       " 'from',\n",
       " 'numpy',\n",
       " 'test',\n",
       " 'input',\n",
       " 'astype',\n",
       " 'np',\n",
       " 'float',\n",
       " '3',\n",
       " '2',\n",
       " 'test',\n",
       " 'target',\n",
       " 'torch',\n",
       " 'from',\n",
       " 'numpy',\n",
       " 'np',\n",
       " 'array',\n",
       " 'y',\n",
       " 'test',\n",
       " 'astype',\n",
       " 'np',\n",
       " 'int',\n",
       " '3',\n",
       " '2',\n",
       " 'float',\n",
       " 'view',\n",
       " '1',\n",
       " '1',\n",
       " 'y',\n",
       " 'hat',\n",
       " 'model',\n",
       " '1',\n",
       " 'test',\n",
       " 'input',\n",
       " 'print',\n",
       " 'y',\n",
       " 'test',\n",
       " 'y',\n",
       " 'test',\n",
       " 'print',\n",
       " 'y',\n",
       " 'hat',\n",
       " 'y',\n",
       " 'hat',\n",
       " 'loss',\n",
       " 'model',\n",
       " '1',\n",
       " 'criterion',\n",
       " 'y',\n",
       " 'hat',\n",
       " 'test',\n",
       " 'target',\n",
       " 'item',\n",
       " 'print',\n",
       " 'loss',\n",
       " 'loss',\n",
       " '3',\n",
       " 'multi',\n",
       " 'layer',\n",
       " 'lstm',\n",
       " 'from',\n",
       " 'torch',\n",
       " 'helpers',\n",
       " 'import',\n",
       " 'customlstm',\n",
       " 'en',\n",
       " 'esta',\n",
       " 'oportunidad',\n",
       " 'se',\n",
       " 'utilizar',\n",
       " 'ã',\n",
       " '¡',\n",
       " 'n',\n",
       " 'dos',\n",
       " 'layer',\n",
       " 'lstm',\n",
       " 'class',\n",
       " 'model',\n",
       " '2',\n",
       " 'nn',\n",
       " 'module',\n",
       " 'def',\n",
       " 'init',\n",
       " 'self',\n",
       " 'input',\n",
       " 'size',\n",
       " 'output',\n",
       " 'dim',\n",
       " 'super',\n",
       " 'init',\n",
       " 'self',\n",
       " 'lstm',\n",
       " '1',\n",
       " 'customlstm',\n",
       " 'input',\n",
       " 'size',\n",
       " 'input',\n",
       " 'size',\n",
       " 'hidden',\n",
       " 'size',\n",
       " '6',\n",
       " '4',\n",
       " 'activation',\n",
       " 'nn',\n",
       " 'relu',\n",
       " 'lstm',\n",
       " 'layer',\n",
       " 'self',\n",
       " 'lstm',\n",
       " '2',\n",
       " 'customlstm',\n",
       " 'input',\n",
       " 'size',\n",
       " '6',\n",
       " '4',\n",
       " 'hidden',\n",
       " 'size',\n",
       " '6',\n",
       " '4',\n",
       " 'activation',\n",
       " 'nn',\n",
       " 'relu',\n",
       " 'lstm',\n",
       " 'layer',\n",
       " 'self',\n",
       " 'fc',\n",
       " 'nn',\n",
       " 'linear',\n",
       " 'in',\n",
       " 'features',\n",
       " '6',\n",
       " '4',\n",
       " 'out',\n",
       " 'features',\n",
       " 'output',\n",
       " 'dim',\n",
       " 'fully',\n",
       " 'connected',\n",
       " 'layer',\n",
       " 'def',\n",
       " 'forward',\n",
       " 'self',\n",
       " 'x',\n",
       " 'lstm',\n",
       " 'output',\n",
       " 'self',\n",
       " 'lstm',\n",
       " '1',\n",
       " 'x',\n",
       " 'lstm',\n",
       " 'output',\n",
       " 'self',\n",
       " 'lstm',\n",
       " '2',\n",
       " 'lstm',\n",
       " 'output',\n",
       " 'out',\n",
       " 'self',\n",
       " 'fc',\n",
       " 'lstm',\n",
       " 'output',\n",
       " '1',\n",
       " 'take',\n",
       " 'last',\n",
       " 'output',\n",
       " 'last',\n",
       " 'seq',\n",
       " 'return',\n",
       " 'out',\n",
       " 'model',\n",
       " '2',\n",
       " 'model',\n",
       " '2',\n",
       " 'input',\n",
       " 'size',\n",
       " 'input',\n",
       " 'size',\n",
       " 'output',\n",
       " 'dim',\n",
       " 'output',\n",
       " 'dim',\n",
       " 'crear',\n",
       " 'el',\n",
       " 'optimizador',\n",
       " 'la',\n",
       " 'una',\n",
       " 'funci',\n",
       " 'ã',\n",
       " '³',\n",
       " 'n',\n",
       " 'de',\n",
       " 'error',\n",
       " 'model',\n",
       " '2',\n",
       " 'optimizer',\n",
       " 'torch',\n",
       " 'optim',\n",
       " 'adam',\n",
       " 'model',\n",
       " '2',\n",
       " 'parameters',\n",
       " 'lr',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " 'model',\n",
       " '2',\n",
       " 'criterion',\n",
       " 'nn',\n",
       " 'mseloss',\n",
       " 'mean',\n",
       " 'squared',\n",
       " 'error',\n",
       " 'summary',\n",
       " 'model',\n",
       " '2',\n",
       " 'input',\n",
       " 'size',\n",
       " '1',\n",
       " 'seq',\n",
       " 'length',\n",
       " 'input',\n",
       " 'size',\n",
       " 'history',\n",
       " '2',\n",
       " 'train',\n",
       " 'model',\n",
       " '2',\n",
       " 'train',\n",
       " 'loader',\n",
       " 'valid',\n",
       " 'loader',\n",
       " 'model',\n",
       " '2',\n",
       " 'optimizer',\n",
       " 'model',\n",
       " '2',\n",
       " 'criterion',\n",
       " 'epochs',\n",
       " '5',\n",
       " '0',\n",
       " '0',\n",
       " 'epoch',\n",
       " 'count',\n",
       " 'range',\n",
       " '1',\n",
       " 'len',\n",
       " 'history',\n",
       " '2',\n",
       " \"'\",\n",
       " 'loss',\n",
       " \"'\",\n",
       " '1',\n",
       " 'sns',\n",
       " 'lineplot',\n",
       " 'x',\n",
       " 'epoch',\n",
       " 'count',\n",
       " 'y',\n",
       " 'history',\n",
       " '2',\n",
       " \"'\",\n",
       " 'loss',\n",
       " \"'\",\n",
       " 'label',\n",
       " \"'\",\n",
       " 'train',\n",
       " \"'\",\n",
       " 'sns',\n",
       " 'lineplot',\n",
       " 'x',\n",
       " 'epoch',\n",
       " 'count',\n",
       " 'y',\n",
       " 'history',\n",
       " '2',\n",
       " \"'\",\n",
       " 'val',\n",
       " 'loss',\n",
       " \"'\",\n",
       " 'label',\n",
       " \"'\",\n",
       " 'valid',\n",
       " \"'\",\n",
       " 'plt',\n",
       " 'show',\n",
       " 'ensayo',\n",
       " 'x',\n",
       " 'test',\n",
       " '1',\n",
       " '0',\n",
       " 'y',\n",
       " 'test',\n",
       " 'x',\n",
       " 'test',\n",
       " '1',\n",
       " 'x',\n",
       " 'test',\n",
       " '2',\n",
       " 'test',\n",
       " 'input',\n",
       " 'np',\n",
       " 'array',\n",
       " 'x',\n",
       " 'test',\n",
       " 'test',\n",
       " 'input',\n",
       " 'test',\n",
       " 'input',\n",
       " 'reshape',\n",
       " '1',\n",
       " 'seq',\n",
       " 'length',\n",
       " 'input',\n",
       " 'size',\n",
       " 'test',\n",
       " 'input',\n",
       " 'torch',\n",
       " 'from',\n",
       " 'numpy',\n",
       " 'test',\n",
       " 'input',\n",
       " 'astype',\n",
       " 'np',\n",
       " 'float',\n",
       " '3',\n",
       " '2',\n",
       " 'test',\n",
       " 'target',\n",
       " 'torch',\n",
       " 'from',\n",
       " 'numpy',\n",
       " 'np',\n",
       " 'array',\n",
       " 'y',\n",
       " 'test',\n",
       " 'astype',\n",
       " 'np',\n",
       " 'int',\n",
       " '3',\n",
       " '2',\n",
       " 'float',\n",
       " 'view',\n",
       " '1',\n",
       " '1',\n",
       " 'y',\n",
       " 'hat',\n",
       " 'model',\n",
       " '2',\n",
       " 'test',\n",
       " 'input',\n",
       " 'print',\n",
       " 'y',\n",
       " 'test',\n",
       " 'y',\n",
       " 'test',\n",
       " 'print',\n",
       " 'y',\n",
       " 'hat',\n",
       " 'y',\n",
       " 'hat',\n",
       " 'loss',\n",
       " 'model',\n",
       " '2',\n",
       " 'criterion',\n",
       " 'y',\n",
       " 'hat',\n",
       " 'test',\n",
       " 'target',\n",
       " 'item',\n",
       " 'print',\n",
       " 'loss',\n",
       " 'loss',\n",
       " '4',\n",
       " 'conclusi',\n",
       " 'ã',\n",
       " '³',\n",
       " 'n',\n",
       " 'la',\n",
       " 'unica',\n",
       " 'diferencia',\n",
       " 'que',\n",
       " 'se',\n",
       " 'debe',\n",
       " 'tener',\n",
       " 'en',\n",
       " 'cuenta',\n",
       " 'cuando',\n",
       " 'hay',\n",
       " 'm',\n",
       " 'ã',\n",
       " '¡',\n",
       " 's',\n",
       " 'de',\n",
       " 'una',\n",
       " 'salida',\n",
       " ...]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformar el corpus a tokens\n",
    "tokens=text_to_word_sequence(corpus)\n",
    "# Vistazo general de los primeros tokens\n",
    "tokens[7700:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de tokens en el corpus: 22891\n"
     ]
    }
   ],
   "source": [
    "print(\"Cantidad de tokens en el corpus:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código para hacer el desfazaje de las palabras\n",
    "# según el train_len\n",
    "text_sequences = []\n",
    "for i in range(train_len, len(tokens)):\n",
    "  seq = tokens[i-train_len:i]\n",
    "  text_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['import', 're', 'import'],\n",
       " ['re', 'import', 'numpy'],\n",
       " ['import', 'numpy', 'as'],\n",
       " ['numpy', 'as', 'np'],\n",
       " ['as', 'np', 'import'],\n",
       " ['np', 'import', 'matplotlib'],\n",
       " ['import', 'matplotlib', 'pyplot'],\n",
       " ['matplotlib', 'pyplot', 'as'],\n",
       " ['pyplot', 'as', 'plt'],\n",
       " ['as', 'plt', 'import'],\n",
       " ['plt', 'import', 'seaborn'],\n",
       " ['import', 'seaborn', 'as'],\n",
       " ['seaborn', 'as', 'sns'],\n",
       " ['as', 'sns', 'import'],\n",
       " ['sns', 'import', 'torch'],\n",
       " ['import', 'torch', 'import'],\n",
       " ['torch', 'import', 'torch'],\n",
       " ['import', 'torch', 'nn'],\n",
       " ['torch', 'nn', 'functional'],\n",
       " ['nn', 'functional', 'as']]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demos un vistazo a nuestros vectores para entrenar el modelo\n",
    "text_sequences[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[30, 340, 30],\n",
       " [340, 30, 47],\n",
       " [30, 47, 79],\n",
       " [47, 79, 31],\n",
       " [79, 31, 30],\n",
       " [31, 30, 241],\n",
       " [30, 241, 242],\n",
       " [241, 242, 79],\n",
       " [242, 79, 107],\n",
       " [79, 107, 30],\n",
       " [107, 30, 243],\n",
       " [30, 243, 79],\n",
       " [243, 79, 72],\n",
       " [79, 72, 30],\n",
       " [72, 30, 16],\n",
       " [30, 16, 30],\n",
       " [16, 30, 16],\n",
       " [30, 16, 29],\n",
       " [16, 29, 244],\n",
       " [29, 244, 79]]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proceso de tokenizacion\n",
    "tok = Tokenizer() \n",
    "tok.fit_on_texts(text_sequences) \n",
    "\n",
    "# Convertimos las palabras a números\n",
    "# entran palabras -> salen números\n",
    "sequences = tok.texts_to_sequences(text_sequences)\n",
    "\n",
    "# Damos un vistazo\n",
    "sequences[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de rows del dataset: 22888\n"
     ]
    }
   ],
   "source": [
    "print(\"Cantidad de rows del dataset:\", len(sequences))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input y target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4],\n",
       "       [5, 6, 7, 8]])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Con numpy es muy fácil realizar el slicing de vectores\n",
    "ex = np.array([[1,2,3,4],[5,6,7,8]])\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[1 2 3]\n",
      " [5 6 7]]\n",
      "Target: [4 8]\n"
     ]
    }
   ],
   "source": [
    "input = ex[:,:-1] # todos los rows, menos la ultima col\n",
    "target = ex[:, -1] # última col de cada row\n",
    "\n",
    "print(\"Input:\", input)\n",
    "print(\"Target:\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22888, 2)\n",
      "(22888,)\n"
     ]
    }
   ],
   "source": [
    "arr_sequences = np.array(sequences)\n",
    "x_data = arr_sequences[:,:-1]\n",
    "y_data_int = arr_sequences[:,-1] # aún falta el oneHotEncoder\n",
    "\n",
    "print(x_data.shape)\n",
    "print(y_data_int.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '1',\n",
       " 2: 'input',\n",
       " 3: 'y',\n",
       " 4: 'size',\n",
       " 5: 'x',\n",
       " 6: 'train',\n",
       " 7: 'test',\n",
       " 8: 'loss',\n",
       " 9: 'de',\n",
       " 10: \"'\",\n",
       " 11: 'model',\n",
       " 12: 'self',\n",
       " 13: '2',\n",
       " 14: '3',\n",
       " 15: 'output',\n",
       " 16: 'torch',\n",
       " 17: '0',\n",
       " 18: 'valid',\n",
       " 19: 'la',\n",
       " 20: 'lstm',\n",
       " 21: 'set',\n",
       " 22: 'data',\n",
       " 23: 'epoch',\n",
       " 24: 'ã',\n",
       " 25: 'print',\n",
       " 26: 'el',\n",
       " 27: 'dim',\n",
       " 28: 'len',\n",
       " 29: 'nn',\n",
       " 30: 'import',\n",
       " 31: 'np',\n",
       " 32: 'loader',\n",
       " 33: 'seq',\n",
       " 34: 'layer',\n",
       " 35: 'from',\n",
       " 36: 'history',\n",
       " 37: '4',\n",
       " 38: 'en',\n",
       " 39: 'target',\n",
       " 40: 'criterion',\n",
       " 41: 'hat',\n",
       " 42: 'error',\n",
       " 43: 'n',\n",
       " 44: 'rnn',\n",
       " 45: 'def',\n",
       " 46: '5',\n",
       " 47: 'numpy',\n",
       " 48: 'out',\n",
       " 49: '6',\n",
       " 50: 'para',\n",
       " 51: 'los',\n",
       " 52: 'helpers',\n",
       " 53: 'optimizer',\n",
       " 54: 'length',\n",
       " 55: 'count',\n",
       " 56: 'in',\n",
       " 57: 'que',\n",
       " 58: 'float',\n",
       " 59: 'a',\n",
       " 60: 'las',\n",
       " 61: 'batch',\n",
       " 62: 'shape',\n",
       " 63: 'astype',\n",
       " 64: '¡',\n",
       " 65: 'range',\n",
       " 66: 'una',\n",
       " 67: '³',\n",
       " 68: 'accuracy',\n",
       " 69: 'init',\n",
       " 70: 'int',\n",
       " 71: 'hidden',\n",
       " 72: 'sns',\n",
       " 73: 'return',\n",
       " 74: 'f',\n",
       " 75: 'array',\n",
       " 76: 'se',\n",
       " 77: 'epochs',\n",
       " 78: 'o',\n",
       " 79: 'as',\n",
       " 80: 'fc',\n",
       " 81: 'features',\n",
       " 82: 'por',\n",
       " 83: 'text',\n",
       " 84: 'utils',\n",
       " 85: 'del',\n",
       " 86: 'py',\n",
       " 87: 'relu',\n",
       " 88: 'last',\n",
       " 89: 'lineplot',\n",
       " 90: 'label',\n",
       " 91: 'datos',\n",
       " 92: 'customlstm',\n",
       " 93: 'for',\n",
       " 94: 'item',\n",
       " 95: 'sequences',\n",
       " 96: 'dataset',\n",
       " 97: 'index',\n",
       " 98: 'os',\n",
       " 99: 'word',\n",
       " 100: 'un',\n",
       " 101: 'con',\n",
       " 102: 'summary',\n",
       " 103: 'val',\n",
       " 104: 'm',\n",
       " 105: 's',\n",
       " 106: 'pytorch',\n",
       " 107: 'plt',\n",
       " 108: 'class',\n",
       " 109: 'no',\n",
       " 110: 'fully',\n",
       " 111: 'connected',\n",
       " 112: 'torchinfo',\n",
       " 113: 'if',\n",
       " 114: 'entrenamiento',\n",
       " 115: 'epoca',\n",
       " 116: 'reshape',\n",
       " 117: 'tokens',\n",
       " 118: 'es',\n",
       " 119: 'view',\n",
       " 120: 'dataloader',\n",
       " 121: 'activation',\n",
       " 122: 'crear',\n",
       " 123: 'false',\n",
       " 124: '±',\n",
       " 125: 'append',\n",
       " 126: 'shuffle',\n",
       " 127: 'customrnn',\n",
       " 128: 'activacion',\n",
       " 129: 'palabras',\n",
       " 130: 'tama',\n",
       " 131: 'modelo',\n",
       " 132: 'defecto',\n",
       " 133: 'cada',\n",
       " 134: 'linear',\n",
       " 135: 'ya',\n",
       " 136: 'cantidad',\n",
       " 137: 'funci',\n",
       " 138: 'º',\n",
       " 139: 'https',\n",
       " 140: 'module',\n",
       " 141: 'super',\n",
       " 142: 'forward',\n",
       " 143: 'take',\n",
       " 144: 'optimizador',\n",
       " 145: 'optim',\n",
       " 146: 'adam',\n",
       " 147: 'parameters',\n",
       " 148: 'lr',\n",
       " 149: 'show',\n",
       " 150: 'tok',\n",
       " 151: 'platform',\n",
       " 152: 'raw',\n",
       " 153: 'com',\n",
       " 154: 'fiuba',\n",
       " 155: 'posgrado',\n",
       " 156: 'inteligencia',\n",
       " 157: 'artificial',\n",
       " 158: 'procesamiento',\n",
       " 159: 'lenguaje',\n",
       " 160: 'natural',\n",
       " 161: 'main',\n",
       " 162: 'pero',\n",
       " 163: 'mseloss',\n",
       " 164: 'mean',\n",
       " 165: 'squared',\n",
       " 166: 'ensayo',\n",
       " 167: 'salida',\n",
       " 168: 'como',\n",
       " 169: 'githubusercontent',\n",
       " 170: 'lo',\n",
       " 171: 'red',\n",
       " 172: 'true',\n",
       " 173: 'scripts',\n",
       " 174: 'defino',\n",
       " 175: 'gradientes',\n",
       " 176: 'computo',\n",
       " 177: 'toda',\n",
       " 178: 'realizo',\n",
       " 179: 'paso',\n",
       " 180: 'entrada',\n",
       " 181: 'subset',\n",
       " 182: 'conjunto',\n",
       " 183: 'utilizar',\n",
       " 184: 'wget',\n",
       " 185: 'tiene',\n",
       " 186: 'problema',\n",
       " 187: 'vez',\n",
       " 188: 'largo',\n",
       " 189: 'list',\n",
       " 190: 'first',\n",
       " 191: 'to',\n",
       " 192: 'system',\n",
       " 193: 'dato',\n",
       " 194: 'realizar',\n",
       " 195: 'seed',\n",
       " 196: '\\xad',\n",
       " 197: 'songs',\n",
       " 198: 'is',\n",
       " 199: 'else',\n",
       " 200: 'utilizamos',\n",
       " 201: 'series',\n",
       " 202: 'temporales',\n",
       " 203: 'suele',\n",
       " 204: 'funcionar',\n",
       " 205: 'mejor',\n",
       " 206: 'tanh',\n",
       " 207: 'permite',\n",
       " 208: 'modificar',\n",
       " 209: 'funcion',\n",
       " 210: 'hay',\n",
       " 211: 'meros',\n",
       " 212: 'curl',\n",
       " 213: 'calculo',\n",
       " 214: 'entrenar',\n",
       " 215: 'este',\n",
       " 216: 'al',\n",
       " 217: 'tokenizer',\n",
       " 218: 'vocab',\n",
       " 219: 'num',\n",
       " 220: 'encoded',\n",
       " 221: 'eso',\n",
       " 222: 'tener',\n",
       " 223: 'proyecto',\n",
       " 224: 'pip',\n",
       " 225: 'install',\n",
       " 226: 'windows',\n",
       " 227: 'almaceno',\n",
       " 228: 'luego',\n",
       " 229: 'promedio',\n",
       " 230: 'longitud',\n",
       " 231: '©',\n",
       " 232: 'general',\n",
       " 233: 'oraciones',\n",
       " 234: 'df',\n",
       " 235: 'transformar',\n",
       " 236: 'ex',\n",
       " 237: 'embeddings',\n",
       " 238: 'access',\n",
       " 239: 'ok',\n",
       " 240: 'dentro',\n",
       " 241: 'matplotlib',\n",
       " 242: 'pyplot',\n",
       " 243: 'seaborn',\n",
       " 244: 'functional',\n",
       " 245: 'torchsummar',\n",
       " 246: 'actualmente',\n",
       " 247: 'utiliza',\n",
       " 248: 'fork',\n",
       " 249: 'original',\n",
       " 250: 'bug',\n",
       " 251: 'solucionado',\n",
       " 252: 'listas',\n",
       " 253: 'graficas',\n",
       " 254: 'resultados',\n",
       " 255: 'mi',\n",
       " 256: 'loop',\n",
       " 257: 'seteo',\n",
       " 258: 'cero',\n",
       " 259: 'va',\n",
       " 260: 'acumulando',\n",
       " 261: 'zero',\n",
       " 262: 'grad',\n",
       " 263: 'comparando',\n",
       " 264: 'contra',\n",
       " 265: 'etiquetas',\n",
       " 266: 'nuevo',\n",
       " 267: 'backward',\n",
       " 268: 'optimizacion',\n",
       " 269: 'actualizando',\n",
       " 270: 'parametros',\n",
       " 271: 'step',\n",
       " 272: 'media',\n",
       " 273: 'igual',\n",
       " 274: 'batches',\n",
       " 275: 'validaci',\n",
       " 276: 'computando',\n",
       " 277: 'almacenando',\n",
       " 278: 'valores',\n",
       " 279: 'imprimirlos',\n",
       " 280: 'graficarlos',\n",
       " 281: 'iter',\n",
       " 282: 'next',\n",
       " 283: 'convertir',\n",
       " 284: 'arrays',\n",
       " 285: 'tensores',\n",
       " 286: 'espera',\n",
       " 287: 'entradas',\n",
       " 288: 'bits',\n",
       " 289: 'esperan',\n",
       " 290: 'getitem',\n",
       " 291: 'manual',\n",
       " 292: 'validacion',\n",
       " 293: 'mucha',\n",
       " 294: 'par',\n",
       " 295: 'esta',\n",
       " 296: 'ejemplo',\n",
       " 297: 'device',\n",
       " 298: 'zip',\n",
       " 299: 'keras',\n",
       " 300: 'vectores',\n",
       " 301: 'vistazo',\n",
       " 302: 'i',\n",
       " 303: 'vocabulario',\n",
       " 304: 'texts',\n",
       " 305: 'palabra',\n",
       " 306: 'state',\n",
       " 307: 'pre',\n",
       " 308: 'tensor',\n",
       " 309: 'words',\n",
       " 310: 'cuando',\n",
       " 311: 'serie',\n",
       " 312: 'temporal',\n",
       " 313: 'multi',\n",
       " 314: 'generar',\n",
       " 315: 'sint',\n",
       " 316: 'ticos',\n",
       " 317: 'obtiene',\n",
       " 318: 'transformarmos',\n",
       " 319: 'matriz',\n",
       " 320: 'fila',\n",
       " 321: 'columna',\n",
       " 322: 'asanyarray',\n",
       " 323: 'unfction',\n",
       " 324: 'trabajmos',\n",
       " 325: 'mezclamos',\n",
       " 326: 'diferencia',\n",
       " 327: 'metros',\n",
       " 328: 'oportunidad',\n",
       " 329: 'dos',\n",
       " 330: 'conclusi',\n",
       " 331: 'podr',\n",
       " 332: 'mejorarse',\n",
       " 333: 'sequence',\n",
       " 334: 'padding',\n",
       " 335: 'row',\n",
       " 336: 'corpus',\n",
       " 337: 'embedding',\n",
       " 338: 'layers',\n",
       " 339: 'softmax',\n",
       " 340: 're',\n",
       " 341: 'puede',\n",
       " 342: 'observar',\n",
       " 343: 'tan',\n",
       " 344: 'entre',\n",
       " 345: 'resultado',\n",
       " 346: 'alcanzado',\n",
       " 347: 'bueno',\n",
       " 348: 'multiplicado',\n",
       " 349: 'simple',\n",
       " 350: 'muchos',\n",
       " 351: 'agregando',\n",
       " 352: 'none',\n",
       " 353: 'pad',\n",
       " 354: 'menos',\n",
       " 355: 'debemos',\n",
       " 356: 'todos',\n",
       " 357: 'prev',\n",
       " 358: 'si',\n",
       " 359: 'gradio',\n",
       " 360: 'max',\n",
       " 361: 'secuencia',\n",
       " 362: 'utilizaremos',\n",
       " 363: 'muy',\n",
       " 364: 'cuda',\n",
       " 365: 'categorical',\n",
       " 366: 'acc',\n",
       " 367: 'separar',\n",
       " 368: 'preprocesamiento',\n",
       " 369: 'sentencia',\n",
       " 370: 'tool',\n",
       " 371: 'entran',\n",
       " 372: 'salen',\n",
       " 373: 'secuencias',\n",
       " 374: 'filas',\n",
       " 375: 'sentence',\n",
       " 376: 'demos',\n",
       " 377: 'fit',\n",
       " 378: 'rows',\n",
       " 379: 'axis',\n",
       " 380: 'arr',\n",
       " 381: 'offset',\n",
       " 382: 'modelos',\n",
       " 383: 'predicci',\n",
       " 384: 'human',\n",
       " 385: 'generate',\n",
       " 386: 'agregar',\n",
       " 387: 'ltima',\n",
       " 388: 'capa',\n",
       " 389: 'hacer',\n",
       " 390: 'debe',\n",
       " 391: 'ser',\n",
       " 392: 'bidirectional',\n",
       " 393: 'brnn',\n",
       " 394: 'bidireccional',\n",
       " 395: 'caso',\n",
       " 396: 'pd',\n",
       " 397: 'cpu',\n",
       " 398: 'bandas',\n",
       " 399: 'datasets',\n",
       " 400: 'docs',\n",
       " 401: 'equivalente',\n",
       " 402: 'nltk',\n",
       " 403: 'loc',\n",
       " 404: 'c',\n",
       " 405: 'digo',\n",
       " 406: 'desfazaje',\n",
       " 407: 'seg',\n",
       " 408: 'nuestros',\n",
       " 409: 'vec',\n",
       " 410: 'ahora',\n",
       " 411: 'one',\n",
       " 412: 'hot',\n",
       " 413: 'veces',\n",
       " 414: 'on',\n",
       " 415: 'convertimos',\n",
       " 416: 'distintas',\n",
       " 417: 'counts',\n",
       " 418: 'populares',\n",
       " 419: 'documento',\n",
       " 420: 'completo',\n",
       " 421: 'texto',\n",
       " 422: 'valor',\n",
       " 423: 'cil',\n",
       " 424: 'slicing',\n",
       " 425: '7',\n",
       " 426: '8',\n",
       " 427: 'elementos',\n",
       " 428: 'col',\n",
       " 429: 'min',\n",
       " 430: 'inferencia',\n",
       " 431: 'zeros',\n",
       " 432: 'importante',\n",
       " 433: 'destacar',\n",
       " 434: 'estamos',\n",
       " 435: 'entrenando',\n",
       " 436: 'nuestro',\n",
       " 437: 'propios',\n",
       " 438: 'ello',\n",
       " 439: 'requiere',\n",
       " 440: 'ejemplos',\n",
       " 441: 'realizaremos',\n",
       " 442: 'aqu',\n",
       " 443: 'enternados',\n",
       " 444: 'entrenados',\n",
       " 445: 'xima',\n",
       " 446: 'ensayar',\n",
       " 447: 'sys',\n",
       " 448: 'gr',\n",
       " 449: 'response',\n",
       " 450: 'encodeamos',\n",
       " 451: 'tienen',\n",
       " 452: 'distinto',\n",
       " 453: 'maxlen',\n",
       " 454: 'transformo',\n",
       " 455: 'argmax',\n",
       " 456: 'buscar',\n",
       " 457: 'corresopnde',\n",
       " 458: 'indice',\n",
       " 459: 'predicho',\n",
       " 460: 'le',\n",
       " 461: 'items',\n",
       " 462: 'break',\n",
       " 463: 'agrego',\n",
       " 464: 'frase',\n",
       " 465: 'predicha',\n",
       " 466: 'iface',\n",
       " 467: 'entrenado',\n",
       " 468: 'string',\n",
       " 469: 'sequencia',\n",
       " 470: 'sum',\n",
       " 471: 'nueva',\n",
       " 472: 'anterior',\n",
       " 473: 'not',\n",
       " 474: 'path',\n",
       " 475: 'exists',\n",
       " 476: 'lista',\n",
       " 477: 'doble',\n",
       " 478: 'implementar',\n",
       " 479: 'random',\n",
       " 480: 'io',\n",
       " 481: 'pickle',\n",
       " 482: 'pandas',\n",
       " 483: 'available',\n",
       " 484: 'canciones',\n",
       " 485: 'habla',\n",
       " 486: 'ingl',\n",
       " 487: 'github',\n",
       " 488: 'unzip',\n",
       " 489: 'q',\n",
       " 490: 'encuentra',\n",
       " 491: 'descargado',\n",
       " 492: 'posibles',\n",
       " 493: 'listdir',\n",
       " 494: 'armar',\n",
       " 495: 'utilizando',\n",
       " 496: 'salto',\n",
       " 497: 'l',\n",
       " 498: 'nea',\n",
       " 499: 'read',\n",
       " 500: 'csv',\n",
       " 501: 'beatles',\n",
       " 502: 'txt',\n",
       " 503: 'sep',\n",
       " 504: 'header',\n",
       " 505: 'head',\n",
       " 506: 'documentos',\n",
       " 507: 'dichas',\n",
       " 508: 'ajustarlas',\n",
       " 509: 'fijo',\n",
       " 510: 'nuestra',\n",
       " 511: 'objetivos',\n",
       " 512: 'predecir',\n",
       " 513: 'armada',\n",
       " 514: 'ltokenizer',\n",
       " 515: 'teokenize',\n",
       " 516: 'qye',\n",
       " 517: 'incluye',\n",
       " 518: 'oraci',\n",
       " 519: 'usando',\n",
       " 520: 'posiciones',\n",
       " 521: 'recorrer',\n",
       " 522: 'todas',\n",
       " 523: 'iterrows',\n",
       " 524: 'necesitamos',\n",
       " 525: 'pasarlos',\n",
       " 526: 'entienda',\n",
       " 527: 'seran',\n",
       " 528: 'integers',\n",
       " 529: 'mientras',\n",
       " 530: 'encodeado',\n",
       " 531: 'labels',\n",
       " 532: 'tokeinzer',\n",
       " 533: 'aprende',\n",
       " 534: 'usaran',\n",
       " 535: 'construye',\n",
       " 536: 'aplica',\n",
       " 537: 'tal',\n",
       " 538: 'cual',\n",
       " 539: 'encoder',\n",
       " 540: 'id',\n",
       " 541: 'casos',\n",
       " 542: 'doc',\n",
       " 543: 'document',\n",
       " 544: 'ndice',\n",
       " 545: 'sistema',\n",
       " 546: 'ordena',\n",
       " 547: 'quea',\n",
       " 548: 'aparece',\n",
       " 549: 'mismos',\n",
       " 550: 'pasos',\n",
       " 551: 'antes',\n",
       " 552: 'ese',\n",
       " 553: 'continuo',\n",
       " 554: 'poder',\n",
       " 555: 'extraer',\n",
       " 556: 'primeras',\n",
       " 557: 'concatenamos',\n",
       " 558: 'solo',\n",
       " 559: 'apply',\n",
       " 560: 'lambda',\n",
       " 561: 'join',\n",
       " 562: 'values',\n",
       " 563: 'str',\n",
       " 564: 'primeros',\n",
       " 565: 'proceso',\n",
       " 566: 'tokenizacion',\n",
       " 567: 'damos',\n",
       " 568: 'dimension',\n",
       " 569: 'ltimo',\n",
       " 570: 'ultima',\n",
       " 571: 'falta',\n",
       " 572: 'onehotencoder',\n",
       " 573: 'â',\n",
       " 574: 'ojo',\n",
       " 575: 'comienza',\n",
       " 576: 'minimo',\n",
       " 577: 'restar',\n",
       " 578: 'onehotencoding',\n",
       " 579: 'function',\n",
       " 580: 'classes',\n",
       " 581: 'unk',\n",
       " 582: 'variable',\n",
       " 583: 'ajustable',\n",
       " 584: 'idx',\n",
       " 585: 'dropout',\n",
       " 586: 'normalize',\n",
       " 587: 'reinicio',\n",
       " 588: 'sea',\n",
       " 589: 'pasado',\n",
       " 590: 'metro',\n",
       " 591: 'elstado',\n",
       " 592: 'previo',\n",
       " 593: 'acci',\n",
       " 594: 'realiza',\n",
       " 595: 'especialmente',\n",
       " 596: 'afecte',\n",
       " 597: 'siguiente',\n",
       " 598: 'ht',\n",
       " 599: 'ct',\n",
       " 600: 'crossentropyloss',\n",
       " 601: 'clasificaci',\n",
       " 602: 'categ',\n",
       " 603: 'rica',\n",
       " 604: 'testea',\n",
       " 605: 'floattensor',\n",
       " 606: 'dtypes',\n",
       " 607: 'inttensor',\n",
       " 608: 'otra',\n",
       " 609: 'posibilidad',\n",
       " 610: 'unsqueeze',\n",
       " 611: 'pr',\n",
       " 612: 'supera',\n",
       " 613: 'trunca',\n",
       " 614: 'corta',\n",
       " 615: 'agregna',\n",
       " 616: 'ceros',\n",
       " 617: 'comienzo',\n",
       " 618: 'herramienta',\n",
       " 619: 'poderosa',\n",
       " 620: 'interfaces',\n",
       " 621: 'r',\n",
       " 622: 'pidas',\n",
       " 623: 'app',\n",
       " 624: 'executable',\n",
       " 625: 'quiet',\n",
       " 626: 'interface',\n",
       " 627: 'fn',\n",
       " 628: 'inputs',\n",
       " 629: 'textbox',\n",
       " 630: 'outputs',\n",
       " 631: 'layout',\n",
       " 632: 'vertical',\n",
       " 633: 'launch',\n",
       " 634: 'debug',\n",
       " 635: 'generaci',\n",
       " 636: 'nuevas',\n",
       " 637: 'exec',\n",
       " 638: 'prediction',\n",
       " 639: 'args',\n",
       " 640: 'tonenizer',\n",
       " 641: 'utilizado',\n",
       " 642: 'returns',\n",
       " 643: 'agregadas',\n",
       " 644: 'fixed',\n",
       " 645: 'number',\n",
       " 646: 'of',\n",
       " 647: 'vamos',\n",
       " 648: 'concatenando',\n",
       " 649: 'predicciones',\n",
       " 650: 'hey',\n",
       " 651: 'jude',\n",
       " 652: 'don',\n",
       " 653: 't',\n",
       " 654: 'conclusiones',\n",
       " 655: 'tuvo',\n",
       " 656: 'mail',\n",
       " 657: 'desempe',\n",
       " 658: 'adem',\n",
       " 659: 'overfitting',\n",
       " 660: 'cuestiones',\n",
       " 661: 'an',\n",
       " 662: 'capas',\n",
       " 663: 'neuronaes',\n",
       " 664: 'incrementar',\n",
       " 665: 'pocas',\n",
       " 666: 'avanzan',\n",
       " 667: 'obtienen',\n",
       " 668: 'unica',\n",
       " 669: 'cuenta',\n",
       " 670: 'neuronas',\n",
       " 671: 'coincidir',\n",
       " 672: 'donde',\n",
       " 673: 'complejo',\n",
       " 674: 'hubo',\n",
       " 675: 'apreciable',\n",
       " 676: 'sola',\n",
       " 677: 'varias',\n",
       " 678: 'agrupado',\n",
       " 679: 'consecutivos',\n",
       " 680: 'suma',\n",
       " 681: 'grupo',\n",
       " 682: 'implementaci',\n",
       " 683: 'soporta',\n",
       " 684: 'flag',\n",
       " 685: 'trae',\n",
       " 686: 'notar',\n",
       " 687: 'exactamente',\n",
       " 688: 'porque',\n",
       " 689: 'agrega',\n",
       " 690: 'otro',\n",
       " 691: 'bias',\n",
       " 692: 'ecuaci',\n",
       " 693: 'tradicional',\n",
       " 694: 'nica',\n",
       " 695: 'desventaja',\n",
       " 696: 'tratarse',\n",
       " 697: 'podremos',\n",
       " 698: 'activaci',\n",
       " 699: 'tanto',\n",
       " 700: 'especifica',\n",
       " 701: 'desea',\n",
       " 702: 'nuestor',\n",
       " 703: 'basado',\n",
       " 704: 'sensillo',\n",
       " 705: 'explot',\n",
       " 706: 'su',\n",
       " 707: 'potencialidad',\n",
       " 708: 'queda',\n",
       " 709: 'nota',\n",
       " 710: 'descargar',\n",
       " 711: 'carpeta'}"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Palabras del vocabulario\n",
    "tok.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "711"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cantidad de palabras en el vocabulario\n",
    "vocab_size = len(tok.word_counts)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ¡Ojo! y_data_int comienza en \"1\" en vez de \"0\"\n",
    "# valor minimo:\n",
    "min(y_data_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hay que restar 1\n",
    "y_data_int_offset = y_data_int - 1\n",
    "min(y_data_int_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size: 2\n",
      "Output dim 711\n"
     ]
    }
   ],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        # Convertir los arrays de numpy a tensores. \n",
    "        # pytorch espera en general entradas 32bits\n",
    "        self.x = torch.from_numpy(x.astype(np.int32))\n",
    "        # Transformar los datos a oneHotEncoding\n",
    "        # la loss function esperan la salida float\n",
    "        self.y = F.one_hot(torch.from_numpy(y), num_classes=vocab_size).float()\n",
    "\n",
    "        self.len = self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "data_set = Data(x_data, y_data_int_offset)\n",
    "\n",
    "input_size = data_set.x.shape[1]\n",
    "print(\"input_size:\", input_size)\n",
    "\n",
    "output_dim = data_set.y.shape[1]\n",
    "print(\"Output dim\", output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento: 18311\n",
      "Tamaño del conjunto de validacion: 4577\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(50)\n",
    "valid_set_size = int(data_set.len * 0.2)\n",
    "train_set_size = data_set.len - valid_set_size\n",
    "\n",
    "train_set = torch.utils.data.Subset(data_set, range(train_set_size))\n",
    "valid_set = torch.utils.data.Subset(data_set, range(train_set_size, data_set.len))\n",
    "\n",
    "print(\"Tamaño del conjunto de entrenamiento:\", len(train_set))\n",
    "print(\"Tamaño del conjunto de validacion:\", len(valid_set))\n",
    "\n",
    "train_batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=train_batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=len(valid_set), shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entreno el modelo, donde ajuste el tamaño de los embeddings a 50 ya que las palabras son cortas y asumo parecidas, al tratarse de codigo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Model1                                   [1, 711]                  --\n",
       "├─Embedding: 1-1                         [1, 2, 50]                35,600\n",
       "├─LSTM: 1-2                              [1, 2, 64]                62,976\n",
       "├─Linear: 1-3                            [1, 32]                   2,080\n",
       "├─ReLU: 1-4                              [1, 32]                   --\n",
       "├─Linear: 1-5                            [1, 711]                  23,463\n",
       "├─Softmax: 1-6                           [1, 711]                  --\n",
       "==========================================================================================\n",
       "Total params: 124,119\n",
       "Trainable params: 124,119\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.19\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 0.50\n",
       "Estimated Total Size (MB): 0.50\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model1(nn.Module):\n",
    "    def __init__(self, vocab_size, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # embedding_dim = 5 --> crear embeddings de tamaño 5 (tamaño variable y ajustable)\n",
    "        self.lstm_size = 64\n",
    "        self.num_layers = 2\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size+1, embedding_dim=50, padding_idx=0)\n",
    "        self.lstm1 = nn.LSTM(input_size=50, hidden_size=self.lstm_size, batch_first=True,\n",
    "                            num_layers=self.num_layers, dropout=0.2) # LSTM layer\n",
    "        self.fc1 = nn.Linear(in_features=self.lstm_size, out_features=32) # Fully connected layer\n",
    "        self.fc2 = nn.Linear(in_features=32, out_features=output_dim) # Fully connected layer\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1) # normalize in dim 1\n",
    "\n",
    "    def forward(self, x, prev_state=None):\n",
    "        if prev_state is None:\n",
    "            # En cada nueva inferencia reinicio el hidden state\n",
    "            # de la LSTM al menos que sea pasado por parámetro el\n",
    "            # elstado de previo\n",
    "            # Esta acción se realiza especialmente para que\n",
    "            # el hidden_state de la última inferencia no afecte\n",
    "            # a la siguiente\n",
    "            batch_size = x.shape[0] #(batch, seq_size)\n",
    "            prev_state = self.init_hidden(batch_size)\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        lstm_output, (ht, ct) = self.lstm1(out, prev_state)\n",
    "        out = self.relu(self.fc1(lstm_output[:,-1,:])) # take last output (last seq)\n",
    "        out = self.softmax(self.fc2(out))\n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.lstm_size),\n",
    "                torch.zeros(self.num_layers, batch_size, self.lstm_size))\n",
    "\n",
    "model1 = Model1(vocab_size=vocab_size, output_dim=output_dim)\n",
    "\n",
    "# Crear el optimizador la una función de error\n",
    "model1_optimizer = torch.optim.Adam(model1.parameters(), lr=0.001)\n",
    "model1_criterion = torch.nn.CrossEntropyLoss()  # Para clasificación multi categórica\n",
    "\n",
    "# Por defecto torchinfo testea el modelo con torch.FloatTensor\n",
    "summary(model1, input_size=(1, input_size), dtypes=['torch.IntTensor'], device=torch.device('cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200 - Train loss 6.521 - Train accuracy 0.051 - Valid Loss 6.510 - Valid accuracy 0.063\n",
      "Epoch: 2/200 - Train loss 6.445 - Train accuracy 0.128 - Valid Loss 6.458 - Valid accuracy 0.112\n",
      "Epoch: 3/200 - Train loss 6.406 - Train accuracy 0.166 - Valid Loss 6.439 - Valid accuracy 0.134\n",
      "Epoch: 4/200 - Train loss 6.376 - Train accuracy 0.197 - Valid Loss 6.414 - Valid accuracy 0.157\n",
      "Epoch: 5/200 - Train loss 6.358 - Train accuracy 0.214 - Valid Loss 6.403 - Valid accuracy 0.168\n",
      "Epoch: 6/200 - Train loss 6.349 - Train accuracy 0.221 - Valid Loss 6.398 - Valid accuracy 0.171\n",
      "Epoch: 7/200 - Train loss 6.348 - Train accuracy 0.222 - Valid Loss 6.397 - Valid accuracy 0.173\n",
      "Epoch: 8/200 - Train loss 6.348 - Train accuracy 0.222 - Valid Loss 6.397 - Valid accuracy 0.172\n",
      "Epoch: 9/200 - Train loss 6.347 - Train accuracy 0.223 - Valid Loss 6.398 - Valid accuracy 0.172\n",
      "Epoch: 10/200 - Train loss 6.346 - Train accuracy 0.224 - Valid Loss 6.395 - Valid accuracy 0.175\n",
      "Epoch: 11/200 - Train loss 6.343 - Train accuracy 0.227 - Valid Loss 6.392 - Valid accuracy 0.177\n",
      "Epoch: 12/200 - Train loss 6.342 - Train accuracy 0.228 - Valid Loss 6.391 - Valid accuracy 0.178\n",
      "Epoch: 13/200 - Train loss 6.341 - Train accuracy 0.228 - Valid Loss 6.390 - Valid accuracy 0.179\n",
      "Epoch: 14/200 - Train loss 6.340 - Train accuracy 0.229 - Valid Loss 6.388 - Valid accuracy 0.181\n",
      "Epoch: 15/200 - Train loss 6.339 - Train accuracy 0.230 - Valid Loss 6.388 - Valid accuracy 0.181\n",
      "Epoch: 16/200 - Train loss 6.339 - Train accuracy 0.231 - Valid Loss 6.385 - Valid accuracy 0.184\n",
      "Epoch: 17/200 - Train loss 6.335 - Train accuracy 0.234 - Valid Loss 6.383 - Valid accuracy 0.186\n",
      "Epoch: 18/200 - Train loss 6.334 - Train accuracy 0.235 - Valid Loss 6.384 - Valid accuracy 0.185\n",
      "Epoch: 19/200 - Train loss 6.334 - Train accuracy 0.235 - Valid Loss 6.383 - Valid accuracy 0.186\n",
      "Epoch: 20/200 - Train loss 6.333 - Train accuracy 0.236 - Valid Loss 6.383 - Valid accuracy 0.186\n",
      "Epoch: 21/200 - Train loss 6.333 - Train accuracy 0.236 - Valid Loss 6.383 - Valid accuracy 0.186\n",
      "Epoch: 22/200 - Train loss 6.332 - Train accuracy 0.237 - Valid Loss 6.381 - Valid accuracy 0.187\n",
      "Epoch: 23/200 - Train loss 6.332 - Train accuracy 0.237 - Valid Loss 6.380 - Valid accuracy 0.189\n",
      "Epoch: 24/200 - Train loss 6.331 - Train accuracy 0.238 - Valid Loss 6.380 - Valid accuracy 0.189\n",
      "Epoch: 25/200 - Train loss 6.331 - Train accuracy 0.238 - Valid Loss 6.380 - Valid accuracy 0.189\n",
      "Epoch: 26/200 - Train loss 6.330 - Train accuracy 0.239 - Valid Loss 6.378 - Valid accuracy 0.192\n",
      "Epoch: 27/200 - Train loss 6.328 - Train accuracy 0.241 - Valid Loss 6.378 - Valid accuracy 0.191\n",
      "Epoch: 28/200 - Train loss 6.329 - Train accuracy 0.241 - Valid Loss 6.376 - Valid accuracy 0.193\n",
      "Epoch: 29/200 - Train loss 6.328 - Train accuracy 0.241 - Valid Loss 6.377 - Valid accuracy 0.193\n",
      "Epoch: 30/200 - Train loss 6.328 - Train accuracy 0.242 - Valid Loss 6.377 - Valid accuracy 0.192\n",
      "Epoch: 31/200 - Train loss 6.328 - Train accuracy 0.242 - Valid Loss 6.376 - Valid accuracy 0.193\n",
      "Epoch: 32/200 - Train loss 6.328 - Train accuracy 0.241 - Valid Loss 6.375 - Valid accuracy 0.194\n",
      "Epoch: 33/200 - Train loss 6.328 - Train accuracy 0.241 - Valid Loss 6.375 - Valid accuracy 0.194\n",
      "Epoch: 34/200 - Train loss 6.327 - Train accuracy 0.242 - Valid Loss 6.375 - Valid accuracy 0.194\n",
      "Epoch: 35/200 - Train loss 6.327 - Train accuracy 0.242 - Valid Loss 6.373 - Valid accuracy 0.196\n",
      "Epoch: 36/200 - Train loss 6.323 - Train accuracy 0.247 - Valid Loss 6.367 - Valid accuracy 0.202\n",
      "Epoch: 37/200 - Train loss 6.318 - Train accuracy 0.251 - Valid Loss 6.364 - Valid accuracy 0.206\n",
      "Epoch: 38/200 - Train loss 6.316 - Train accuracy 0.253 - Valid Loss 6.363 - Valid accuracy 0.206\n",
      "Epoch: 39/200 - Train loss 6.314 - Train accuracy 0.255 - Valid Loss 6.361 - Valid accuracy 0.209\n",
      "Epoch: 40/200 - Train loss 6.313 - Train accuracy 0.256 - Valid Loss 6.361 - Valid accuracy 0.208\n",
      "Epoch: 41/200 - Train loss 6.313 - Train accuracy 0.256 - Valid Loss 6.361 - Valid accuracy 0.209\n",
      "Epoch: 42/200 - Train loss 6.313 - Train accuracy 0.256 - Valid Loss 6.360 - Valid accuracy 0.210\n",
      "Epoch: 43/200 - Train loss 6.313 - Train accuracy 0.256 - Valid Loss 6.361 - Valid accuracy 0.208\n",
      "Epoch: 44/200 - Train loss 6.312 - Train accuracy 0.258 - Valid Loss 6.359 - Valid accuracy 0.211\n",
      "Epoch: 45/200 - Train loss 6.311 - Train accuracy 0.259 - Valid Loss 6.358 - Valid accuracy 0.211\n",
      "Epoch: 46/200 - Train loss 6.311 - Train accuracy 0.258 - Valid Loss 6.358 - Valid accuracy 0.211\n",
      "Epoch: 47/200 - Train loss 6.310 - Train accuracy 0.259 - Valid Loss 6.359 - Valid accuracy 0.211\n",
      "Epoch: 48/200 - Train loss 6.311 - Train accuracy 0.258 - Valid Loss 6.358 - Valid accuracy 0.211\n",
      "Epoch: 49/200 - Train loss 6.310 - Train accuracy 0.260 - Valid Loss 6.358 - Valid accuracy 0.211\n",
      "Epoch: 50/200 - Train loss 6.310 - Train accuracy 0.259 - Valid Loss 6.358 - Valid accuracy 0.211\n",
      "Epoch: 51/200 - Train loss 6.309 - Train accuracy 0.260 - Valid Loss 6.357 - Valid accuracy 0.212\n",
      "Epoch: 52/200 - Train loss 6.309 - Train accuracy 0.260 - Valid Loss 6.357 - Valid accuracy 0.212\n",
      "Epoch: 53/200 - Train loss 6.310 - Train accuracy 0.260 - Valid Loss 6.357 - Valid accuracy 0.212\n",
      "Epoch: 54/200 - Train loss 6.309 - Train accuracy 0.260 - Valid Loss 6.358 - Valid accuracy 0.211\n",
      "Epoch: 55/200 - Train loss 6.309 - Train accuracy 0.260 - Valid Loss 6.357 - Valid accuracy 0.213\n",
      "Epoch: 56/200 - Train loss 6.309 - Train accuracy 0.260 - Valid Loss 6.356 - Valid accuracy 0.213\n",
      "Epoch: 57/200 - Train loss 6.309 - Train accuracy 0.260 - Valid Loss 6.356 - Valid accuracy 0.213\n",
      "Epoch: 58/200 - Train loss 6.309 - Train accuracy 0.260 - Valid Loss 6.356 - Valid accuracy 0.213\n",
      "Epoch: 59/200 - Train loss 6.308 - Train accuracy 0.261 - Valid Loss 6.356 - Valid accuracy 0.213\n",
      "Epoch: 60/200 - Train loss 6.308 - Train accuracy 0.261 - Valid Loss 6.356 - Valid accuracy 0.213\n",
      "Epoch: 61/200 - Train loss 6.306 - Train accuracy 0.263 - Valid Loss 6.353 - Valid accuracy 0.217\n",
      "Epoch: 62/200 - Train loss 6.302 - Train accuracy 0.268 - Valid Loss 6.351 - Valid accuracy 0.218\n",
      "Epoch: 63/200 - Train loss 6.301 - Train accuracy 0.268 - Valid Loss 6.351 - Valid accuracy 0.218\n",
      "Epoch: 64/200 - Train loss 6.301 - Train accuracy 0.268 - Valid Loss 6.350 - Valid accuracy 0.219\n",
      "Epoch: 65/200 - Train loss 6.300 - Train accuracy 0.269 - Valid Loss 6.350 - Valid accuracy 0.219\n",
      "Epoch: 66/200 - Train loss 6.301 - Train accuracy 0.268 - Valid Loss 6.351 - Valid accuracy 0.218\n",
      "Epoch: 67/200 - Train loss 6.300 - Train accuracy 0.269 - Valid Loss 6.350 - Valid accuracy 0.219\n",
      "Epoch: 68/200 - Train loss 6.300 - Train accuracy 0.269 - Valid Loss 6.350 - Valid accuracy 0.219\n",
      "Epoch: 69/200 - Train loss 6.300 - Train accuracy 0.269 - Valid Loss 6.350 - Valid accuracy 0.219\n",
      "Epoch: 70/200 - Train loss 6.300 - Train accuracy 0.269 - Valid Loss 6.351 - Valid accuracy 0.218\n",
      "Epoch: 71/200 - Train loss 6.300 - Train accuracy 0.269 - Valid Loss 6.350 - Valid accuracy 0.219\n",
      "Epoch: 72/200 - Train loss 6.300 - Train accuracy 0.270 - Valid Loss 6.350 - Valid accuracy 0.219\n",
      "Epoch: 73/200 - Train loss 6.300 - Train accuracy 0.269 - Valid Loss 6.350 - Valid accuracy 0.218\n",
      "Epoch: 74/200 - Train loss 6.299 - Train accuracy 0.270 - Valid Loss 6.350 - Valid accuracy 0.220\n",
      "Epoch: 75/200 - Train loss 6.299 - Train accuracy 0.270 - Valid Loss 6.349 - Valid accuracy 0.220\n",
      "Epoch: 76/200 - Train loss 6.299 - Train accuracy 0.270 - Valid Loss 6.350 - Valid accuracy 0.219\n",
      "Epoch: 77/200 - Train loss 6.299 - Train accuracy 0.270 - Valid Loss 6.350 - Valid accuracy 0.219\n",
      "Epoch: 78/200 - Train loss 6.299 - Train accuracy 0.270 - Valid Loss 6.350 - Valid accuracy 0.219\n",
      "Epoch: 79/200 - Train loss 6.299 - Train accuracy 0.270 - Valid Loss 6.350 - Valid accuracy 0.219\n",
      "Epoch: 80/200 - Train loss 6.298 - Train accuracy 0.271 - Valid Loss 6.348 - Valid accuracy 0.221\n",
      "Epoch: 81/200 - Train loss 6.297 - Train accuracy 0.273 - Valid Loss 6.347 - Valid accuracy 0.222\n",
      "Epoch: 82/200 - Train loss 6.296 - Train accuracy 0.274 - Valid Loss 6.346 - Valid accuracy 0.223\n",
      "Epoch: 83/200 - Train loss 6.295 - Train accuracy 0.274 - Valid Loss 6.346 - Valid accuracy 0.223\n",
      "Epoch: 84/200 - Train loss 6.295 - Train accuracy 0.274 - Valid Loss 6.347 - Valid accuracy 0.222\n",
      "Epoch: 85/200 - Train loss 6.295 - Train accuracy 0.274 - Valid Loss 6.346 - Valid accuracy 0.223\n",
      "Epoch: 86/200 - Train loss 6.294 - Train accuracy 0.275 - Valid Loss 6.346 - Valid accuracy 0.223\n",
      "Epoch: 87/200 - Train loss 6.295 - Train accuracy 0.274 - Valid Loss 6.347 - Valid accuracy 0.222\n",
      "Epoch: 88/200 - Train loss 6.295 - Train accuracy 0.275 - Valid Loss 6.347 - Valid accuracy 0.223\n",
      "Epoch: 89/200 - Train loss 6.295 - Train accuracy 0.275 - Valid Loss 6.346 - Valid accuracy 0.224\n",
      "Epoch: 90/200 - Train loss 6.294 - Train accuracy 0.275 - Valid Loss 6.347 - Valid accuracy 0.223\n",
      "Epoch: 91/200 - Train loss 6.295 - Train accuracy 0.274 - Valid Loss 6.346 - Valid accuracy 0.224\n",
      "Epoch: 92/200 - Train loss 6.294 - Train accuracy 0.275 - Valid Loss 6.345 - Valid accuracy 0.225\n",
      "Epoch: 93/200 - Train loss 6.292 - Train accuracy 0.277 - Valid Loss 6.343 - Valid accuracy 0.226\n",
      "Epoch: 94/200 - Train loss 6.290 - Train accuracy 0.279 - Valid Loss 6.342 - Valid accuracy 0.228\n",
      "Epoch: 95/200 - Train loss 6.289 - Train accuracy 0.280 - Valid Loss 6.341 - Valid accuracy 0.228\n",
      "Epoch: 96/200 - Train loss 6.289 - Train accuracy 0.280 - Valid Loss 6.342 - Valid accuracy 0.227\n",
      "Epoch: 97/200 - Train loss 6.289 - Train accuracy 0.280 - Valid Loss 6.342 - Valid accuracy 0.227\n",
      "Epoch: 98/200 - Train loss 6.289 - Train accuracy 0.280 - Valid Loss 6.342 - Valid accuracy 0.227\n",
      "Epoch: 99/200 - Train loss 6.290 - Train accuracy 0.279 - Valid Loss 6.343 - Valid accuracy 0.226\n",
      "Epoch: 100/200 - Train loss 6.289 - Train accuracy 0.280 - Valid Loss 6.342 - Valid accuracy 0.227\n",
      "Epoch: 101/200 - Train loss 6.289 - Train accuracy 0.280 - Valid Loss 6.342 - Valid accuracy 0.227\n",
      "Epoch: 102/200 - Train loss 6.287 - Train accuracy 0.282 - Valid Loss 6.341 - Valid accuracy 0.228\n",
      "Epoch: 103/200 - Train loss 6.287 - Train accuracy 0.282 - Valid Loss 6.341 - Valid accuracy 0.229\n",
      "Epoch: 104/200 - Train loss 6.287 - Train accuracy 0.282 - Valid Loss 6.341 - Valid accuracy 0.228\n",
      "Epoch: 105/200 - Train loss 6.284 - Train accuracy 0.285 - Valid Loss 6.338 - Valid accuracy 0.231\n",
      "Epoch: 106/200 - Train loss 6.282 - Train accuracy 0.287 - Valid Loss 6.339 - Valid accuracy 0.231\n",
      "Epoch: 107/200 - Train loss 6.281 - Train accuracy 0.288 - Valid Loss 6.338 - Valid accuracy 0.231\n",
      "Epoch: 108/200 - Train loss 6.281 - Train accuracy 0.289 - Valid Loss 6.338 - Valid accuracy 0.231\n",
      "Epoch: 109/200 - Train loss 6.281 - Train accuracy 0.288 - Valid Loss 6.337 - Valid accuracy 0.232\n",
      "Epoch: 110/200 - Train loss 6.281 - Train accuracy 0.288 - Valid Loss 6.338 - Valid accuracy 0.231\n",
      "Epoch: 111/200 - Train loss 6.281 - Train accuracy 0.289 - Valid Loss 6.338 - Valid accuracy 0.231\n",
      "Epoch: 112/200 - Train loss 6.280 - Train accuracy 0.289 - Valid Loss 6.337 - Valid accuracy 0.232\n",
      "Epoch: 113/200 - Train loss 6.280 - Train accuracy 0.289 - Valid Loss 6.338 - Valid accuracy 0.231\n",
      "Epoch: 114/200 - Train loss 6.281 - Train accuracy 0.288 - Valid Loss 6.337 - Valid accuracy 0.232\n",
      "Epoch: 115/200 - Train loss 6.281 - Train accuracy 0.289 - Valid Loss 6.337 - Valid accuracy 0.232\n",
      "Epoch: 116/200 - Train loss 6.281 - Train accuracy 0.288 - Valid Loss 6.337 - Valid accuracy 0.232\n",
      "Epoch: 117/200 - Train loss 6.281 - Train accuracy 0.288 - Valid Loss 6.337 - Valid accuracy 0.232\n",
      "Epoch: 118/200 - Train loss 6.281 - Train accuracy 0.288 - Valid Loss 6.337 - Valid accuracy 0.232\n",
      "Epoch: 119/200 - Train loss 6.280 - Train accuracy 0.289 - Valid Loss 6.337 - Valid accuracy 0.232\n",
      "Epoch: 120/200 - Train loss 6.280 - Train accuracy 0.289 - Valid Loss 6.337 - Valid accuracy 0.232\n",
      "Epoch: 121/200 - Train loss 6.280 - Train accuracy 0.289 - Valid Loss 6.337 - Valid accuracy 0.232\n",
      "Epoch: 122/200 - Train loss 6.280 - Train accuracy 0.289 - Valid Loss 6.337 - Valid accuracy 0.232\n",
      "Epoch: 123/200 - Train loss 6.280 - Train accuracy 0.289 - Valid Loss 6.337 - Valid accuracy 0.232\n",
      "Epoch: 124/200 - Train loss 6.280 - Train accuracy 0.289 - Valid Loss 6.336 - Valid accuracy 0.233\n",
      "Epoch: 125/200 - Train loss 6.280 - Train accuracy 0.289 - Valid Loss 6.336 - Valid accuracy 0.233\n",
      "Epoch: 126/200 - Train loss 6.280 - Train accuracy 0.289 - Valid Loss 6.336 - Valid accuracy 0.233\n",
      "Epoch: 127/200 - Train loss 6.280 - Train accuracy 0.289 - Valid Loss 6.336 - Valid accuracy 0.233\n",
      "Epoch: 128/200 - Train loss 6.280 - Train accuracy 0.289 - Valid Loss 6.336 - Valid accuracy 0.233\n",
      "Epoch: 129/200 - Train loss 6.280 - Train accuracy 0.289 - Valid Loss 6.336 - Valid accuracy 0.233\n",
      "Epoch: 130/200 - Train loss 6.280 - Train accuracy 0.289 - Valid Loss 6.336 - Valid accuracy 0.234\n",
      "Epoch: 131/200 - Train loss 6.280 - Train accuracy 0.289 - Valid Loss 6.336 - Valid accuracy 0.233\n",
      "Epoch: 132/200 - Train loss 6.279 - Train accuracy 0.290 - Valid Loss 6.336 - Valid accuracy 0.233\n",
      "Epoch: 133/200 - Train loss 6.280 - Train accuracy 0.289 - Valid Loss 6.335 - Valid accuracy 0.234\n",
      "Epoch: 134/200 - Train loss 6.280 - Train accuracy 0.289 - Valid Loss 6.335 - Valid accuracy 0.234\n",
      "Epoch: 135/200 - Train loss 6.278 - Train accuracy 0.291 - Valid Loss 6.335 - Valid accuracy 0.235\n",
      "Epoch: 136/200 - Train loss 6.278 - Train accuracy 0.291 - Valid Loss 6.335 - Valid accuracy 0.234\n",
      "Epoch: 137/200 - Train loss 6.278 - Train accuracy 0.292 - Valid Loss 6.335 - Valid accuracy 0.234\n",
      "Epoch: 138/200 - Train loss 6.277 - Train accuracy 0.292 - Valid Loss 6.334 - Valid accuracy 0.235\n",
      "Epoch: 139/200 - Train loss 6.277 - Train accuracy 0.293 - Valid Loss 6.333 - Valid accuracy 0.236\n",
      "Epoch: 140/200 - Train loss 6.277 - Train accuracy 0.292 - Valid Loss 6.334 - Valid accuracy 0.235\n",
      "Epoch: 141/200 - Train loss 6.277 - Train accuracy 0.292 - Valid Loss 6.333 - Valid accuracy 0.236\n",
      "Epoch: 142/200 - Train loss 6.277 - Train accuracy 0.293 - Valid Loss 6.333 - Valid accuracy 0.236\n",
      "Epoch: 143/200 - Train loss 6.276 - Train accuracy 0.293 - Valid Loss 6.333 - Valid accuracy 0.236\n",
      "Epoch: 144/200 - Train loss 6.276 - Train accuracy 0.293 - Valid Loss 6.333 - Valid accuracy 0.236\n",
      "Epoch: 145/200 - Train loss 6.277 - Train accuracy 0.292 - Valid Loss 6.334 - Valid accuracy 0.236\n",
      "Epoch: 146/200 - Train loss 6.276 - Train accuracy 0.293 - Valid Loss 6.334 - Valid accuracy 0.235\n",
      "Epoch: 147/200 - Train loss 6.276 - Train accuracy 0.293 - Valid Loss 6.333 - Valid accuracy 0.236\n",
      "Epoch: 148/200 - Train loss 6.276 - Train accuracy 0.293 - Valid Loss 6.333 - Valid accuracy 0.236\n",
      "Epoch: 149/200 - Train loss 6.276 - Train accuracy 0.293 - Valid Loss 6.332 - Valid accuracy 0.237\n",
      "Epoch: 150/200 - Train loss 6.276 - Train accuracy 0.293 - Valid Loss 6.333 - Valid accuracy 0.236\n",
      "Epoch: 151/200 - Train loss 6.276 - Train accuracy 0.293 - Valid Loss 6.333 - Valid accuracy 0.236\n",
      "Epoch: 152/200 - Train loss 6.276 - Train accuracy 0.293 - Valid Loss 6.333 - Valid accuracy 0.237\n",
      "Epoch: 153/200 - Train loss 6.276 - Train accuracy 0.293 - Valid Loss 6.332 - Valid accuracy 0.237\n",
      "Epoch: 154/200 - Train loss 6.276 - Train accuracy 0.293 - Valid Loss 6.333 - Valid accuracy 0.236\n",
      "Epoch: 155/200 - Train loss 6.276 - Train accuracy 0.293 - Valid Loss 6.332 - Valid accuracy 0.237\n",
      "Epoch: 156/200 - Train loss 6.275 - Train accuracy 0.294 - Valid Loss 6.332 - Valid accuracy 0.237\n",
      "Epoch: 157/200 - Train loss 6.275 - Train accuracy 0.294 - Valid Loss 6.332 - Valid accuracy 0.237\n",
      "Epoch: 158/200 - Train loss 6.274 - Train accuracy 0.295 - Valid Loss 6.331 - Valid accuracy 0.238\n",
      "Epoch: 159/200 - Train loss 6.275 - Train accuracy 0.294 - Valid Loss 6.332 - Valid accuracy 0.237\n",
      "Epoch: 160/200 - Train loss 6.275 - Train accuracy 0.294 - Valid Loss 6.331 - Valid accuracy 0.238\n",
      "Epoch: 161/200 - Train loss 6.274 - Train accuracy 0.295 - Valid Loss 6.332 - Valid accuracy 0.237\n",
      "Epoch: 162/200 - Train loss 6.275 - Train accuracy 0.294 - Valid Loss 6.331 - Valid accuracy 0.238\n",
      "Epoch: 163/200 - Train loss 6.274 - Train accuracy 0.295 - Valid Loss 6.332 - Valid accuracy 0.237\n",
      "Epoch: 164/200 - Train loss 6.274 - Train accuracy 0.295 - Valid Loss 6.332 - Valid accuracy 0.237\n",
      "Epoch: 165/200 - Train loss 6.274 - Train accuracy 0.295 - Valid Loss 6.331 - Valid accuracy 0.238\n",
      "Epoch: 166/200 - Train loss 6.275 - Train accuracy 0.294 - Valid Loss 6.332 - Valid accuracy 0.238\n",
      "Epoch: 167/200 - Train loss 6.275 - Train accuracy 0.295 - Valid Loss 6.332 - Valid accuracy 0.237\n",
      "Epoch: 168/200 - Train loss 6.275 - Train accuracy 0.294 - Valid Loss 6.332 - Valid accuracy 0.237\n",
      "Epoch: 169/200 - Train loss 6.275 - Train accuracy 0.294 - Valid Loss 6.333 - Valid accuracy 0.236\n",
      "Epoch: 170/200 - Train loss 6.275 - Train accuracy 0.294 - Valid Loss 6.331 - Valid accuracy 0.238\n",
      "Epoch: 171/200 - Train loss 6.273 - Train accuracy 0.296 - Valid Loss 6.331 - Valid accuracy 0.238\n",
      "Epoch: 172/200 - Train loss 6.273 - Train accuracy 0.296 - Valid Loss 6.331 - Valid accuracy 0.238\n",
      "Epoch: 173/200 - Train loss 6.274 - Train accuracy 0.295 - Valid Loss 6.330 - Valid accuracy 0.239\n",
      "Epoch: 174/200 - Train loss 6.274 - Train accuracy 0.295 - Valid Loss 6.331 - Valid accuracy 0.238\n",
      "Epoch: 175/200 - Train loss 6.274 - Train accuracy 0.295 - Valid Loss 6.330 - Valid accuracy 0.239\n",
      "Epoch: 176/200 - Train loss 6.274 - Train accuracy 0.294 - Valid Loss 6.330 - Valid accuracy 0.239\n",
      "Epoch: 177/200 - Train loss 6.274 - Train accuracy 0.295 - Valid Loss 6.331 - Valid accuracy 0.238\n",
      "Epoch: 178/200 - Train loss 6.273 - Train accuracy 0.296 - Valid Loss 6.331 - Valid accuracy 0.238\n",
      "Epoch: 179/200 - Train loss 6.274 - Train accuracy 0.295 - Valid Loss 6.330 - Valid accuracy 0.239\n",
      "Epoch: 180/200 - Train loss 6.273 - Train accuracy 0.296 - Valid Loss 6.331 - Valid accuracy 0.238\n",
      "Epoch: 181/200 - Train loss 6.273 - Train accuracy 0.296 - Valid Loss 6.330 - Valid accuracy 0.239\n",
      "Epoch: 182/200 - Train loss 6.273 - Train accuracy 0.296 - Valid Loss 6.330 - Valid accuracy 0.239\n",
      "Epoch: 183/200 - Train loss 6.272 - Train accuracy 0.297 - Valid Loss 6.330 - Valid accuracy 0.239\n",
      "Epoch: 184/200 - Train loss 6.272 - Train accuracy 0.297 - Valid Loss 6.330 - Valid accuracy 0.239\n",
      "Epoch: 185/200 - Train loss 6.272 - Train accuracy 0.297 - Valid Loss 6.330 - Valid accuracy 0.239\n",
      "Epoch: 186/200 - Train loss 6.272 - Train accuracy 0.297 - Valid Loss 6.329 - Valid accuracy 0.240\n",
      "Epoch: 187/200 - Train loss 6.272 - Train accuracy 0.297 - Valid Loss 6.329 - Valid accuracy 0.240\n",
      "Epoch: 188/200 - Train loss 6.272 - Train accuracy 0.297 - Valid Loss 6.329 - Valid accuracy 0.240\n",
      "Epoch: 189/200 - Train loss 6.272 - Train accuracy 0.297 - Valid Loss 6.330 - Valid accuracy 0.239\n",
      "Epoch: 190/200 - Train loss 6.272 - Train accuracy 0.297 - Valid Loss 6.330 - Valid accuracy 0.239\n",
      "Epoch: 191/200 - Train loss 6.272 - Train accuracy 0.297 - Valid Loss 6.329 - Valid accuracy 0.240\n",
      "Epoch: 192/200 - Train loss 6.271 - Train accuracy 0.298 - Valid Loss 6.329 - Valid accuracy 0.240\n",
      "Epoch: 193/200 - Train loss 6.272 - Train accuracy 0.297 - Valid Loss 6.329 - Valid accuracy 0.240\n",
      "Epoch: 194/200 - Train loss 6.271 - Train accuracy 0.298 - Valid Loss 6.329 - Valid accuracy 0.240\n",
      "Epoch: 195/200 - Train loss 6.271 - Train accuracy 0.298 - Valid Loss 6.329 - Valid accuracy 0.240\n",
      "Epoch: 196/200 - Train loss 6.271 - Train accuracy 0.298 - Valid Loss 6.329 - Valid accuracy 0.240\n",
      "Epoch: 197/200 - Train loss 6.271 - Train accuracy 0.298 - Valid Loss 6.329 - Valid accuracy 0.240\n",
      "Epoch: 198/200 - Train loss 6.271 - Train accuracy 0.298 - Valid Loss 6.328 - Valid accuracy 0.241\n",
      "Epoch: 199/200 - Train loss 6.271 - Train accuracy 0.298 - Valid Loss 6.328 - Valid accuracy 0.241\n",
      "Epoch: 200/200 - Train loss 6.271 - Train accuracy 0.298 - Valid Loss 6.328 - Valid accuracy 0.241\n"
     ]
    }
   ],
   "source": [
    "history1 = train(model1,\n",
    "                train_loader,\n",
    "                valid_loader,\n",
    "                model1_optimizer,\n",
    "                model1_criterion,\n",
    "                epochs=200\n",
    "                )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analizo resultados train vs valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "epoch_count = range(1, len(history1['accuracy']) + 1)\n",
    "sns.lineplot(x=epoch_count,  y=history1['accuracy'], label='train')\n",
    "sns.lineplot(x=epoch_count,  y=history1['val_accuracy'], label='valid')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prediccion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generacion sec n uev        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gradio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(model, tokenizer, seed_text, max_length, n_words):\n",
    "    \"\"\"\n",
    "        Exec model sequence prediction\n",
    "\n",
    "        Args:\n",
    "            model (keras): modelo entrenado\n",
    "            tokenizer (keras tokenizer): tonenizer utilizado en el preprocesamiento\n",
    "            seed_text (string): texto de entrada (input_seq)\n",
    "            max_length (int): máxima longitud de la sequencia de entrada\n",
    "            n_words (int): números de palabras a agregar a la sequencia de entrada\n",
    "        returns:\n",
    "            output_text (string): sentencia con las \"n_words\" agregadas\n",
    "    \"\"\"\n",
    "    output_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "\t\t# Encodeamos\n",
    "        encoded = tokenizer.texts_to_sequences([output_text])[0]\n",
    "\t\t# Si tienen distinto largo\n",
    "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "\t\t\n",
    "        # Transformo a tensor\n",
    "        tensor = torch.from_numpy(encoded.astype(np.int32))\n",
    "\n",
    "        # Predicción softmax\n",
    "        y_hat = model1(tensor).argmax(axis=-1)\n",
    "\n",
    "\t\t# Vamos concatenando las predicciones\n",
    "        out_word = ''\n",
    "\n",
    "        # Debemos buscar en el vocabulario la palabra\n",
    "        # que corresopnde al indice (y_hat) predicho por le modelo\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == y_hat:\n",
    "                out_word = word\n",
    "                break\n",
    "\n",
    "\t\t# Agrego las palabras a la frase predicha\n",
    "        output_text += ' ' + out_word\n",
    "    return output_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebo resultados con partes de codigo CON y SIN espacios entre las palabras, usando cantidades diferentes de outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Output_2</th>\n",
       "      <th>Output_3</th>\n",
       "      <th>Output_5</th>\n",
       "      <th>Output_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>import pandas as</td>\n",
       "      <td>import pandas as output output</td>\n",
       "      <td>import pandas as rnn o o</td>\n",
       "      <td>import pandas as py output o o la</td>\n",
       "      <td>import pandas as py output o la la self loss la</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>import numpy asprint(</td>\n",
       "      <td>import numpy asprint( model 3</td>\n",
       "      <td>import numpy asprint( model 3 self</td>\n",
       "      <td>import numpy asprint( output model model x x</td>\n",
       "      <td>import numpy asprint( model 3 self size size s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>for _ in</td>\n",
       "      <td>for _ in size data</td>\n",
       "      <td>for _ in data model</td>\n",
       "      <td>for _ in '</td>\n",
       "      <td>for _ in '</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model1_optimizer =</td>\n",
       "      <td>model1_optimizer =  ' 3</td>\n",
       "      <td>model1_optimizer =  ' 3 la</td>\n",
       "      <td>model1_optimizer =  ' valid hat ' valid</td>\n",
       "      <td>model1_optimizer =  ' 3 la epoch loss ' 2 test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>return</td>\n",
       "      <td>return  model model</td>\n",
       "      <td>return  model model</td>\n",
       "      <td>return  model 3 6 cada sns</td>\n",
       "      <td>return  model model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>self.lstm_size</td>\n",
       "      <td>self.lstm_size 2 2</td>\n",
       "      <td>self.lstm_size import 3 6</td>\n",
       "      <td>self.lstm_size import 3 x import output</td>\n",
       "      <td>self.lstm_size 2 2 2 2 2 2 2 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>class sns.lineplot(x=epoch_count,</td>\n",
       "      <td>class sns.lineplot(x=epoch_count,</td>\n",
       "      <td>class sns.lineplot(x=epoch_count, hat ' '</td>\n",
       "      <td>class sns.lineplot(x=epoch_count, 2 2 2 2 2</td>\n",
       "      <td>class sns.lineplot(x=epoch_count,   2 2 2 2 2 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>if not</td>\n",
       "      <td>if not  model model</td>\n",
       "      <td>if not  ' 3 test</td>\n",
       "      <td>if not  model model</td>\n",
       "      <td>if not  ' 3 hat ' valid de print epoch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>import pandas</td>\n",
       "      <td>import pandas py sns</td>\n",
       "      <td>import pandas py la sns</td>\n",
       "      <td>import pandas py sns 6 import 5</td>\n",
       "      <td>import pandas py py 6 self 6 2 x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>print (</td>\n",
       "      <td>print (  ' 2</td>\n",
       "      <td>print (  ' 2 test</td>\n",
       "      <td>print (  ' 2 6 2 2</td>\n",
       "      <td>print (  ' 2 6 2 2 2 6 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>for _</td>\n",
       "      <td>for _ cada cada</td>\n",
       "      <td>for _ ' 3 6</td>\n",
       "      <td>for _ ' 3 la epoch loss</td>\n",
       "      <td>for _ ' 3 6 ' 2 de de epoch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>model1_optimizer =</td>\n",
       "      <td>model1_optimizer =  model 3</td>\n",
       "      <td>model1_optimizer =  ' 3 hat</td>\n",
       "      <td>model1_optimizer =  ' 3 hat ' valid</td>\n",
       "      <td>model1_optimizer =  ' 3 la la epoch loss la epoch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>return</td>\n",
       "      <td>return  model 3</td>\n",
       "      <td>return  model 3 la</td>\n",
       "      <td>return  model 3 hat ' valid</td>\n",
       "      <td>return  model 2 2 2 2 2 2 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>self. lstm_size</td>\n",
       "      <td>self. lstm_size import 3</td>\n",
       "      <td>self. lstm_size import 3 6</td>\n",
       "      <td>self. lstm_size import 3 6 cada data</td>\n",
       "      <td>self. lstm_size import 3 x import output o o la</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>class sns . lineplot ( x = epoch_count ,</td>\n",
       "      <td>class sns . lineplot ( x = epoch_count , 2 test</td>\n",
       "      <td>class sns . lineplot ( x = epoch_count ,</td>\n",
       "      <td>class sns . lineplot ( x = epoch_count ,  hat ...</td>\n",
       "      <td>class sns . lineplot ( x = epoch_count , 2 tes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>if not</td>\n",
       "      <td>if not model model</td>\n",
       "      <td>if not model 3 size</td>\n",
       "      <td>if not model model</td>\n",
       "      <td>if not model model</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Input   \n",
       "0                           import pandas as  \\\n",
       "1                      import numpy asprint(   \n",
       "2                                   for _ in   \n",
       "3                        model1_optimizer =    \n",
       "4                                    return    \n",
       "5                             self.lstm_size   \n",
       "6          class sns.lineplot(x=epoch_count,   \n",
       "7                                    if not    \n",
       "8                              import pandas   \n",
       "9                                   print (    \n",
       "10                                     for _   \n",
       "11                       model1_optimizer =    \n",
       "12                                   return    \n",
       "13                           self. lstm_size   \n",
       "14  class sns . lineplot ( x = epoch_count ,   \n",
       "15                                    if not   \n",
       "\n",
       "                                           Output_2   \n",
       "0                    import pandas as output output  \\\n",
       "1                     import numpy asprint( model 3   \n",
       "2                                for _ in size data   \n",
       "3                           model1_optimizer =  ' 3   \n",
       "4                               return  model model   \n",
       "5                                self.lstm_size 2 2   \n",
       "6               class sns.lineplot(x=epoch_count,     \n",
       "7                               if not  model model   \n",
       "8                              import pandas py sns   \n",
       "9                                      print (  ' 2   \n",
       "10                                  for _ cada cada   \n",
       "11                      model1_optimizer =  model 3   \n",
       "12                                  return  model 3   \n",
       "13                         self. lstm_size import 3   \n",
       "14  class sns . lineplot ( x = epoch_count , 2 test   \n",
       "15                               if not model model   \n",
       "\n",
       "                                       Output_3   \n",
       "0                      import pandas as rnn o o  \\\n",
       "1            import numpy asprint( model 3 self   \n",
       "2                          for _ in data model    \n",
       "3                    model1_optimizer =  ' 3 la   \n",
       "4                          return  model model    \n",
       "5                     self.lstm_size import 3 6   \n",
       "6     class sns.lineplot(x=epoch_count, hat ' '   \n",
       "7                              if not  ' 3 test   \n",
       "8                       import pandas py la sns   \n",
       "9                             print (  ' 2 test   \n",
       "10                                  for _ ' 3 6   \n",
       "11                  model1_optimizer =  ' 3 hat   \n",
       "12                           return  model 3 la   \n",
       "13                   self. lstm_size import 3 6   \n",
       "14  class sns . lineplot ( x = epoch_count ,      \n",
       "15                          if not model 3 size   \n",
       "\n",
       "                                             Output_5   \n",
       "0                   import pandas as py output o o la  \\\n",
       "1        import numpy asprint( output model model x x   \n",
       "2                                      for _ in '       \n",
       "3             model1_optimizer =  ' valid hat ' valid   \n",
       "4                          return  model 3 6 cada sns   \n",
       "5             self.lstm_size import 3 x import output   \n",
       "6         class sns.lineplot(x=epoch_count, 2 2 2 2 2   \n",
       "7                              if not  model model      \n",
       "8                     import pandas py sns 6 import 5   \n",
       "9                                  print (  ' 2 6 2 2   \n",
       "10                            for _ ' 3 la epoch loss   \n",
       "11                model1_optimizer =  ' 3 hat ' valid   \n",
       "12                        return  model 3 hat ' valid   \n",
       "13               self. lstm_size import 3 6 cada data   \n",
       "14  class sns . lineplot ( x = epoch_count ,  hat ...   \n",
       "15                              if not model model      \n",
       "\n",
       "                                             Output_8  \n",
       "0     import pandas as py output o la la self loss la  \n",
       "1   import numpy asprint( model 3 self size size s...  \n",
       "2                                   for _ in '         \n",
       "3      model1_optimizer =  ' 3 la epoch loss ' 2 test  \n",
       "4                           return  model model        \n",
       "5                      self.lstm_size 2 2 2 2 2 2 2 2  \n",
       "6     class sns.lineplot(x=epoch_count,   2 2 2 2 2 2  \n",
       "7              if not  ' 3 hat ' valid de print epoch  \n",
       "8                   import pandas py py 6 self 6 2 x   \n",
       "9                            print (  ' 2 6 2 2 2 6 2  \n",
       "10                        for _ ' 3 6 ' 2 de de epoch  \n",
       "11  model1_optimizer =  ' 3 la la epoch loss la epoch  \n",
       "12                        return  model 2 2 2 2 2 2 2  \n",
       "13    self. lstm_size import 3 x import output o o la  \n",
       "14  class sns . lineplot ( x = epoch_count , 2 tes...  \n",
       "15                           if not model model        "
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts=[\n",
    "    \"import pandas as\",\n",
    "    \"import numpy as\"\n",
    "    \"print(\",\n",
    "    \"for _ in\",\n",
    "    \"model1_optimizer = \",\n",
    "    \"return \",\n",
    "    \"self.lstm_size\",\n",
    "    \"class \" \n",
    "    \"sns.lineplot(x=epoch_count,\",\n",
    "    \"if not \",\n",
    "    \"import pandas\",\n",
    "    \"print ( \",\n",
    "    \"for _\",\n",
    "    \"model1_optimizer = \",\n",
    "    \"return \",\n",
    "    \"self. lstm_size\",\n",
    "    \"class \" \n",
    "    \"sns . lineplot ( x = epoch_count ,\",\n",
    "    \"if not\",\n",
    "    ]\n",
    "\n",
    "output_list2 = [generate_seq(model1, tok, result, max_length=6, n_words=2) for result in input_texts] \n",
    "output_list3  = [generate_seq(model1, tok, result, max_length=6, n_words=3) for result in input_texts]\n",
    "output_list5  = [generate_seq(model1, tok, result, max_length=6, n_words=5) for result in input_texts]\n",
    "output_list8  = [generate_seq(model1, tok, result, max_length=6, n_words=8) for result in input_texts]\n",
    "df = pd.DataFrame({'Input': input_texts, 'Output_2': output_list2, 'Output_3': output_list3,'Output_5': output_list5,'Output_8': output_list8,})\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "##### Luego de entrenar 100 epocas, observe que la diferencia entre train y test en las métricas distaba bastante, por lo que subi las epochs a 200, de esta manera se nota una mejora en el grafico de la metrica aunque la prediccion aun es bastante pobres, ademas de este cambio tambien modifque los embeddings a un largo de 50, para poder distinguir mas las palabras ya que al tratarde de codigo con palabras bastante cortas y/o similares, es probable que se parezcan bastante. Si bien los resultados en las predicciones probadas son similares a las palabras que pertenecen al codigo, en ningun caso ha acertado a la siguiente palabra careciendo de sentido.\n",
    "\n",
    "##### Para mejorar este resultado, creo que se podria comenzar por mejora la etapa de pre-procesamiento, por ejemplo reformateando el codigo para saber en que caso colocar un espacio y en que casos no, por ejemplo, evitar los espacios cuando hay dos palabras separadas por un punto, dado que es muy probable se trate de un metodo perteneciente a una clase. Ademas, se podría mejorar la arquitectura, agregando capas o neuronales."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
